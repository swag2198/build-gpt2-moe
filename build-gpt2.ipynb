{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f837c-2897-4a0a-855a-d4c76978d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.1.2\n",
      "matplotlib version: 3.9.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.4.1\n",
      "/mnt/lustre/work/macke/mwe102/.conda/sbi/bin/python\n",
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = ['numpy', 'matplotlib', 'tiktoken', 'torch']\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "!which python; python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8066db-c024-43a4-b09a-16943e44ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, time, inspect\n",
    "import urllib.request\n",
    "\n",
    "file_path = 'input.txt'\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text = response.read().decode('utf-8')\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "text = text[:1000] # first 1,000 characters\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3512461d-4a67-4492-8875-be7119fc70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # config for gpt2 124M model\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257 # 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "# TODO\n",
    "@dataclass\n",
    "class TokenformerConfig:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fed47cc-8635-429d-ad1c-424b1d5c96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # note that these matrices also have a bias!\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # Wq, Wk, Wv matrices\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)     # Wo: final projection\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0  # a flag to identify this particular module\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # note: name is misleading, it is actually the causal mask, not bias!\n",
    "        # this is the autoregressive mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x) # B, T, 3*d\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # these 4 lines can be replaced by flash attention\n",
    "        # flash attention never materializes the TxT att matrix in GPU memory\n",
    "        # relies on online softmax calculation\n",
    "        \"\"\"\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        \"\"\"\n",
    "        # replace by this line\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        # gaussian error linear unit, approximation is a historical quirk\n",
    "        # gelu always contributes a local gradient in the tail end of the flat region\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # prefer clean residual stream from outputs to all the way back to inputs\n",
    "        # no normalization in the residual streams\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict( # the main container\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # layers will be indexed by integers (0, 1, ...) instead of names (like wpe, wte)\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight tying scheme\n",
    "        # wte weight redirected to the lm_head weight\n",
    "        # wte weight original gets orphaned and hopefully cleaned up\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                # 2 times num layers as each layer adds 2 times to the residual path\n",
    "                # once by attn layer and another time by the MLP layer\n",
    "                std *= (2 * self.config.n_layer) ** (-0.5)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # zero init bias is not pytorch default\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape [B, T]\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) #    [T, n_embd]\n",
    "        tok_emb = self.transformer.wte(idx) # [B, T, n_embd]\n",
    "        x = tok_emb + pos_emb               # broadcasting hidden\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t()) # inplace copying of a tensor\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k]) # inplace copying of a tensor\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # just pick out params that require grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups -- all 2d params will be weight decayed, biases and layernorms no decay\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        \n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        \n",
    "        return optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad426c2d-b352-4ac3-bfc8-9388422901ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb468db1-f354-4fc0-9092-e7090f028283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "# print(f\"using device: {device}\")\n",
    "# # device = \"cpu\" # OVERRIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad80fc10-72ae-4db6-a7d8-8e006d181283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_return_sequences = 5\n",
    "# max_length = 30\n",
    "\n",
    "# # model = GPT.from_pretrained('gpt2')\n",
    "# model = GPT(GPTConfig()) # randomly initialized model\n",
    "# model.eval()\n",
    "# model.to(device);\n",
    "\n",
    "# enc = tiktoken.get_encoding('gpt2')\n",
    "# tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "# tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "# tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # repeat same row 5 times\n",
    "# x = tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249b1a91-b154-419d-91ca-34d18a50b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "\n",
    "# while x.size(1) < max_length:\n",
    "#     with torch.no_grad():\n",
    "#         logits, _ = model(x) # [B, T, V]\n",
    "#         logits = logits[:, -1, :]\n",
    "#         probs = F.softmax(logits, dim=-1)\n",
    "#         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "#         # select a token from top-50 tokens according to probabilities\n",
    "#         ix = torch.multinomial(topk_probs, 1)\n",
    "#         xcol = torch.gather(topk_indices, -1, ix)\n",
    "#         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# for i in range(num_return_sequences):\n",
    "#     tokens = x[i, :].tolist()\n",
    "#     decoded = enc.decode(tokens)\n",
    "#     print(f\"> {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a845962-7cf0-4847-9767-6bf9eb240054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706f7ea1-8328-4c30-9597-0c5a306e230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B, self.T = B, T\n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "        self.current_position += B*T\n",
    "        if self.current_position + B*T + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d21ae3ad-cff7-4f72-83c2-5e38099dbd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "# device = \"cpu\" # OVERRIDE\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589da6b0-a8d9-4724-9dcb-4338cd5432b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288 tokens\n",
      "=> calculated gradient accumulation steps: 32\n",
      "loaded 338025 tokens\n",
      "1 epoch = 20 batches\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524_288 # 2**19, closest power of 2 to ~0.5M\n",
    "B = 16   # micro batch size: how many rows we are processing in a single forward-backward\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T) == 0, \"total batch size in # tokens is divisible by B*T\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size: {total_batch_size} tokens\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "# initialize the dataloader\n",
    "train_loader = DataLoaderLite(B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03a1fc4-7ee6-442e-8067-67e607f5c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tf32, now matmuls will use tf32 (tensor cores from A100)\n",
    "torch.set_float32_matmul_precision('high') # default is highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b41c5d-1ace-4a85-af2d-1ca291aaeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # prev constant lr that we were using\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 10\n",
    "max_steps=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cad1eba-c4c4-49fe-8096-e2e2373081e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # linear warmup\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # if it > lr decay iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1, goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f09aed-869e-460f-8c82-1887a7db6a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5609ec0dc0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(max_steps)\n",
    "lrs = np.array([get_lr(it) for it in steps])\n",
    "\n",
    "plt.plot(steps, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6445eac-22c0-4277-a5a6-a8f33c8f74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(optimizer.param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47cdc7d2-38a0-4608-ab86-9d9a8b436f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Step    0 | loss: 10.938566 | lr: 6.0000e-05 | norm: 27.0126 | dt: 48163.27ms | tok/sec: 10885.64\n",
      "Step    1 | loss: 9.649344 | lr: 1.2000e-04 | norm: 9.5177 | dt: 3460.72ms | tok/sec: 151496.72\n",
      "Step    2 | loss: 9.225602 | lr: 1.8000e-04 | norm: 5.7290 | dt: 3461.05ms | tok/sec: 151482.52\n",
      "Step    3 | loss: 9.813100 | lr: 2.4000e-04 | norm: 8.2066 | dt: 3462.22ms | tok/sec: 151431.37\n",
      "Step    4 | loss: 9.191634 | lr: 3.0000e-04 | norm: 4.2995 | dt: 3462.32ms | tok/sec: 151426.76\n",
      "Step    5 | loss: 8.678035 | lr: 3.6000e-04 | norm: 3.6285 | dt: 3463.89ms | tok/sec: 151358.17\n",
      "Step    6 | loss: 8.294984 | lr: 4.2000e-04 | norm: 1.9536 | dt: 3466.73ms | tok/sec: 151234.21\n",
      "Step    7 | loss: 8.068030 | lr: 4.8000e-04 | norm: 2.8520 | dt: 3465.58ms | tok/sec: 151284.39\n",
      "Step    8 | loss: 7.714195 | lr: 5.4000e-04 | norm: 1.9110 | dt: 3469.67ms | tok/sec: 151106.06\n",
      "Step    9 | loss: 7.347044 | lr: 6.0000e-04 | norm: 1.8003 | dt: 3471.67ms | tok/sec: 151019.06\n",
      "Step   10 | loss: 7.029725 | lr: 6.0000e-04 | norm: 1.8391 | dt: 3478.23ms | tok/sec: 150734.04\n",
      "Step   11 | loss: 6.741224 | lr: 5.9917e-04 | norm: 1.5058 | dt: 3484.70ms | tok/sec: 150454.21\n",
      "Step   12 | loss: 6.528901 | lr: 5.9668e-04 | norm: 1.1513 | dt: 3484.89ms | tok/sec: 150445.87\n",
      "Step   13 | loss: 6.377277 | lr: 5.9254e-04 | norm: 1.0575 | dt: 3487.19ms | tok/sec: 150347.02\n",
      "Step   14 | loss: 6.341176 | lr: 5.8679e-04 | norm: 2.6022 | dt: 3484.73ms | tok/sec: 150452.91\n",
      "Step   15 | loss: 6.243546 | lr: 5.7945e-04 | norm: 0.9688 | dt: 3494.92ms | tok/sec: 150014.52\n",
      "Step   16 | loss: 6.212714 | lr: 5.7057e-04 | norm: 0.7745 | dt: 3503.73ms | tok/sec: 149637.29\n",
      "Step   17 | loss: 6.210191 | lr: 5.6021e-04 | norm: 1.1366 | dt: 3487.98ms | tok/sec: 150312.91\n",
      "Step   18 | loss: 6.157043 | lr: 5.4843e-04 | norm: 1.0174 | dt: 3488.91ms | tok/sec: 150272.51\n",
      "Step   19 | loss: 6.147971 | lr: 5.3531e-04 | norm: 1.5109 | dt: 3492.12ms | tok/sec: 150134.71\n",
      "Step   20 | loss: 6.119920 | lr: 5.2092e-04 | norm: 0.9282 | dt: 3492.13ms | tok/sec: 150134.08\n",
      "Step   21 | loss: 6.065421 | lr: 5.0535e-04 | norm: 0.7643 | dt: 3495.36ms | tok/sec: 149995.21\n",
      "Step   22 | loss: 6.051904 | lr: 4.8870e-04 | norm: 0.5093 | dt: 3500.51ms | tok/sec: 149774.88\n",
      "Step   23 | loss: 6.005638 | lr: 4.7107e-04 | norm: 0.5360 | dt: 3504.61ms | tok/sec: 149599.63\n",
      "Step   24 | loss: 5.991609 | lr: 4.5258e-04 | norm: 0.3689 | dt: 3505.08ms | tok/sec: 149579.45\n",
      "Step   25 | loss: 5.979499 | lr: 4.3332e-04 | norm: 0.3815 | dt: 3508.40ms | tok/sec: 149437.83\n",
      "Step   26 | loss: 5.957145 | lr: 4.1343e-04 | norm: 0.4057 | dt: 3511.02ms | tok/sec: 149326.35\n",
      "Step   27 | loss: 5.967769 | lr: 3.9303e-04 | norm: 0.4982 | dt: 3513.53ms | tok/sec: 149219.66\n",
      "Step   28 | loss: 5.931130 | lr: 3.7224e-04 | norm: 0.4187 | dt: 3515.04ms | tok/sec: 149155.45\n",
      "Step   29 | loss: 5.928729 | lr: 3.5118e-04 | norm: 0.7060 | dt: 3513.95ms | tok/sec: 149201.82\n",
      "Step   30 | loss: 5.917717 | lr: 3.3000e-04 | norm: 0.3357 | dt: 3519.55ms | tok/sec: 148964.40\n",
      "Step   31 | loss: 5.899310 | lr: 3.0882e-04 | norm: 0.4874 | dt: 3523.42ms | tok/sec: 148800.81\n",
      "Step   32 | loss: 5.910439 | lr: 2.8776e-04 | norm: 0.4503 | dt: 3524.94ms | tok/sec: 148736.64\n",
      "Step   33 | loss: 5.878658 | lr: 2.6697e-04 | norm: 0.2765 | dt: 3527.73ms | tok/sec: 148619.03\n",
      "Step   34 | loss: 5.875084 | lr: 2.4657e-04 | norm: 0.4402 | dt: 3525.34ms | tok/sec: 148720.04\n",
      "Step   35 | loss: 5.869894 | lr: 2.2668e-04 | norm: 0.2854 | dt: 3527.61ms | tok/sec: 148624.25\n",
      "Step   36 | loss: 5.852420 | lr: 2.0742e-04 | norm: 0.2244 | dt: 3530.89ms | tok/sec: 148485.92\n",
      "Step   37 | loss: 5.869555 | lr: 1.8893e-04 | norm: 0.3156 | dt: 3532.65ms | tok/sec: 148412.05\n",
      "Step   38 | loss: 5.841742 | lr: 1.7130e-04 | norm: 0.1864 | dt: 3532.38ms | tok/sec: 148423.29\n",
      "Step   39 | loss: 5.839181 | lr: 1.5465e-04 | norm: 0.2376 | dt: 3542.23ms | tok/sec: 148010.75\n",
      "Step   40 | loss: 5.839615 | lr: 1.3908e-04 | norm: 0.2122 | dt: 3540.00ms | tok/sec: 148103.98\n",
      "Step   41 | loss: 5.824176 | lr: 1.2469e-04 | norm: 0.2215 | dt: 3542.60ms | tok/sec: 147995.46\n",
      "Step   42 | loss: 5.842920 | lr: 1.1157e-04 | norm: 0.2199 | dt: 3537.08ms | tok/sec: 148226.12\n",
      "Step   43 | loss: 5.817076 | lr: 9.9787e-05 | norm: 0.2622 | dt: 3539.21ms | tok/sec: 148137.13\n",
      "Step   44 | loss: 5.814799 | lr: 8.9428e-05 | norm: 0.2219 | dt: 3542.39ms | tok/sec: 148003.88\n",
      "Step   45 | loss: 5.816161 | lr: 8.0553e-05 | norm: 0.1670 | dt: 3541.47ms | tok/sec: 148042.41\n",
      "Step   46 | loss: 5.801759 | lr: 7.3215e-05 | norm: 0.2219 | dt: 3542.31ms | tok/sec: 148007.29\n",
      "Step   47 | loss: 5.821619 | lr: 6.7460e-05 | norm: 0.1941 | dt: 3544.47ms | tok/sec: 147917.25\n",
      "Step   48 | loss: 5.795034 | lr: 6.3324e-05 | norm: 0.1682 | dt: 3545.03ms | tok/sec: 147893.66\n",
      "Step   49 | loss: 5.794344 | lr: 6.0832e-05 | norm: 0.1685 | dt: 3548.51ms | tok/sec: 147748.82\n",
      "logits.shape: torch.Size([16, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# model = GPT(GPTConfig())\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # 6th optimization\n",
    "model.to(device)\n",
    "model = torch.compile(model) #4th optimization\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device) # fused update\n",
    "\n",
    "max_steps = 50\n",
    "for step in range(max_steps):\n",
    "    # start timer\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # gradient-accumulation\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # data loading\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        # forward-backward and step\n",
    "        # amp for 3rd optimization, just surround forward pass and loss calculation, only possible in A100\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward() # deposits gradients, i.e., += on nodes\n",
    "\n",
    "    # clip gradient norms to 1.0, returns total norm of the gradient vector\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # determine lr for this step\n",
    "    lr = get_lr(step)\n",
    "    # pytorch syntax to set the learning rate for the parameters\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # param_group is a dict\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for gpu to finish the compute and measure time\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000 # time difference for one-batch or step in miliseconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tps = tokens_processed / (t1 - t0)\n",
    "    \n",
    "    print(f\"Step {step:4d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tps:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"logits.shape: {logits.shape}, logits dtype: {logits.dtype}, loss.dtype: {loss.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93da17a-a032-4bfe-84da-4274a3be8cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5c44bf-8db6-492b-8d1f-83559d4d6319",
   "metadata": {},
   "source": [
    "### 1. baseline (all computations in float32):\n",
    "```\n",
    "Step 0 loss: 10.988560676574707, dt: 1302.39ms, tok/sec: 12579.96\n",
    "...\n",
    "Step 49 loss: 6.0608367919921875, dt: 1131.17ms, tok/sec: 14484.17\n",
    "```\n",
    "### 2. tensorcore (tf32) matmul, but all computations still in float32:\n",
    "- we added `torch.set_float32_matmul_precision('high')`\n",
    "- okay so throughput increased from $14,000$ to $40,000$\n",
    "```\n",
    "Step 0 loss: 10.935468673706055, dt: 1369.09ms, tok/sec: 11967.11\n",
    "...\n",
    "Step 49 loss: 6.053790092468262, dt: 403.40ms, tok/sec: 40614.37\n",
    "torch.Size([16, 1024, 50257]) torch.float32 torch.float32\n",
    "```\n",
    "- this change is local to the matmul operation, so we see the dtype of everythig is still `fp32`\n",
    "### 3. mixed precision: dropping down everythin to bfloat16 (similar to tf32):\n",
    "- exponent bits for `fp32`, `tf32` and `bf16` are all same (8 bits for exponent)\n",
    "- their range is same, but now it is reduced precision\n",
    "- note that we are not doing `fp16` as that reduced exponent bits (not same range)\n",
    "- what exactly is **mixed** precision? the parameters/weights are still in `fp32`, but the logits are now in `bf16`\n",
    "- why we are using `bf6` instead of `fp16` in `with torch.autocast(device_type=device, dtype=torch.bfloat16)`? because if we used `fp16` we would need to use gradient scalers\n",
    "- this is only possible in ampere ie A100s\n",
    "- similarly fp8 is only possible in H100s\n",
    "```\n",
    "Step 0 loss: 10.982086181640625, dt: 1479.24ms, tok/sec: 11075.94\n",
    "Step 1 loss: 9.621953964233398, dt: 359.68ms, tok/sec: 45551.13\n",
    "Step 2 loss: 9.218406677246094, dt: 342.26ms, tok/sec: 47870.29\n",
    "...\n",
    "Step 49 loss: 5.9743523597717285, dt: 343.68ms, tok/sec: 47672.53\n",
    "torch.Size([16, 1024, 50257]) torch.bfloat16 tensor(5.9744, device='cuda:0', grad_fn=<NllLossBackward0>) torch.float32\n",
    "```\n",
    "\n",
    "\n",
    "### 4. use `model=torch.compile(model)`:\n",
    "- costs compile time\n",
    "- reduces back and forth moving between gpu memory\n",
    "- when you have the entire model computation graph available with you, why do eager execution?\n",
    "- **massive** throughput gain from $47,000$ to $103,000$\n",
    "```\n",
    "Step 0 loss: 10.935879707336426, dt: 119321.65ms, tok/sec: 137.31\n",
    "Step 1 loss: 9.398224830627441, dt: 153.08ms, tok/sec: 107029.85\n",
    "Step 2 loss: 8.942508697509766, dt: 155.39ms, tok/sec: 105436.13\n",
    "...\n",
    "Step 47 loss: 6.1945366859436035, dt: 158.20ms, tok/sec: 103566.34\n",
    "Step 48 loss: 6.1350908279418945, dt: 156.87ms, tok/sec: 104445.94\n",
    "Step 49 loss: 6.056550025939941, dt: 157.70ms, tok/sec: 103893.43\n",
    "torch.Size([16, 1024, 50257]) torch.bfloat16 tensor(6.0566, device='cuda:0', grad_fn=<CompiledFunctionBackward>) torch.float32\n",
    "```\n",
    "\n",
    "\n",
    "### 5. introduce flash attention:\n",
    "- basically another kernel optimization that `torch.compile` cannot find on its own\n",
    "- based on online softmax calculation\n",
    "- \n",
    "```\n",
    "Step 0 loss: 10.9359130859375, dt: 23674.47ms, tok/sec: 692.05\n",
    "Step 1 loss: 9.398143768310547, dt: 113.62ms, tok/sec: 144203.84\n",
    "Step 2 loss: 8.942390441894531, dt: 115.83ms, tok/sec: 141449.32\n",
    "...\n",
    "Step 47 loss: 6.194267272949219, dt: 117.96ms, tok/sec: 138894.85\n",
    "Step 48 loss: 6.1363019943237305, dt: 117.95ms, tok/sec: 138904.39\n",
    "Step 49 loss: 6.058759689331055, dt: 116.97ms, tok/sec: 140074.31\n",
    "torch.Size([16, 1024, 50257]) torch.bfloat16 tensor(6.0588, device='cuda:0', grad_fn=<CompiledFunctionBackward>) torch.float32\n",
    "```\n",
    "\n",
    "\n",
    "### 6. change vocabulary size to nearest power of 2:\n",
    "- nice numbers are good to have\n",
    "- adding fake tokens to the vocabulary, these tokens are never used\n",
    "- predicting probabilities for tokens that were never present in the training set, so the network has to learn to drive the probabilities for those tokens to zero\n",
    "```\n",
    "Step 0 loss: 10.924062728881836, dt: 23784.74ms, tok/sec: 688.85\n",
    "Step 1 loss: 9.522332191467285, dt: 109.65ms, tok/sec: 149423.95\n",
    "Step 2 loss: 9.18037223815918, dt: 113.67ms, tok/sec: 144132.47\n",
    "...\n",
    "Step 47 loss: 6.144783973693848, dt: 113.11ms, tok/sec: 144852.81\n",
    "Step 48 loss: 6.116957187652588, dt: 113.76ms, tok/sec: 144020.40\n",
    "Step 49 loss: 5.957751750946045, dt: 113.51ms, tok/sec: 144333.78\n",
    "torch.Size([16, 1024, 50304]) torch.bfloat16 tensor(5.9578, device='cuda:0', grad_fn=<CompiledFunctionBackward>) torch.float32\n",
    "\n",
    "```\n",
    "- so slight increase in timming with this, not much!\n",
    "\n",
    "### 7. hyperparameters:\n",
    "- introduce AdamW betas\n",
    "- gradient norm clipping to 1\n",
    "- norms should stabilize as happening here\n",
    "```\n",
    "Step    0 | loss: 10.947336 | norm: 28.5686 | dt: 19151.34ms | tok/sec: 855.50\n",
    "Step    1 | loss: 9.388495 | norm: 6.1851 | dt: 153.79ms | tok/sec: 106537.86\n",
    "Step    2 | loss: 8.949816 | norm: 2.4913 | dt: 145.93ms | tok/sec: 112272.97\n",
    "...\n",
    "Step   47 | loss: 6.053903 | norm: 0.9616 | dt: 113.84ms | tok/sec: 143923.88\n",
    "Step   48 | loss: 5.986834 | norm: 0.8159 | dt: 114.86ms | tok/sec: 142641.07\n",
    "Step   49 | loss: 5.901224 | norm: 0.7297 | dt: 115.14ms | tok/sec: 142296.38\n",
    "torch.Size([16, 1024, 50304]) torch.bfloat16 tensor(5.9012, device='cuda:0', grad_fn=<CompiledFunctionBackward>) torch.float32\n",
    "```\n",
    "\n",
    "### 8. learning rate scheduler\n",
    "- warm-up\n",
    "- cosine decay has been popularized by GPT2 and 3, but other schedulers also exist\n",
    "- we do actually go to a lower loss value!\n",
    "```\n",
    "Step    0 | loss: 10.901613 | lr: 5.9999999999999995e-05 | norm: 26.4353 | dt: 17998.45ms | tok/sec: 910.30\n",
    "Step    1 | loss: 9.676805 | lr: 0.00011999999999999999 | norm: 9.6122 | dt: 113.05ms | tok/sec: 144923.99\n",
    "Step    2 | loss: 9.136786 | lr: 0.00017999999999999998 | norm: 3.1799 | dt: 114.70ms | tok/sec: 142848.04\n",
    "Step    3 | loss: 9.215250 | lr: 0.00023999999999999998 | norm: 6.7971 | dt: 113.99ms | tok/sec: 143733.92\n",
    "...\n",
    "Step   47 | loss: 5.876487 | lr: 6.746012149262733e-05 | norm: 0.6398 | dt: 112.87ms | tok/sec: 145163.08\n",
    "Step   48 | loss: 5.854237 | lr: 6.332414803931283e-05 | norm: 0.8459 | dt: 115.68ms | tok/sec: 141630.65\n",
    "Step   49 | loss: 5.710653 | lr: 6.083231989205545e-05 | norm: 1.0062 | dt: 114.64ms | tok/sec: 142922.91\n",
    "torch.Size([16, 1024, 50304]) torch.bfloat16 tensor(5.7107, device='cuda:0', grad_fn=<CompiledFunctionBackward>) torch.float32\n",
    "```\n",
    "\n",
    "### 9. weight decay + fused Adam\n",
    "- adding `configure_optimizer` code to model, instead of separate optimizer\n",
    "- forcing the network to distribute the work among weights\n",
    "- kernel fusion for the AdamW update instead of iterating through the tensors\n",
    "```\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Step    0 | loss: 10.947336 | lr: 5.9999999999999995e-05 | norm: 28.5691 | dt: 18786.88ms | tok/sec: 872.10\n",
    "Step    1 | loss: 9.502428 | lr: 0.00011999999999999999 | norm: 10.6575 | dt: 106.14ms | tok/sec: 154362.66\n",
    "Step    2 | loss: 9.251534 | lr: 0.00017999999999999998 | norm: 7.3723 | dt: 108.48ms | tok/sec: 151030.82\n",
    "Step    3 | loss: 9.762380 | lr: 0.00023999999999999998 | norm: 6.6549 | dt: 108.88ms | tok/sec: 150482.47\n",
    "Step    4 | loss: 9.105087 | lr: 0.0003 | norm: 4.2451 | dt: 109.56ms | tok/sec: 149542.64\n",
    "Step    5 | loss: 8.795059 | lr: 0.00035999999999999997 | norm: 3.2620 | dt: 108.36ms | tok/sec: 151201.63\n",
    "...\n",
    "Step   46 | loss: 6.147086 | lr: 7.321474060030853e-05 | norm: 0.7845 | dt: 109.39ms | tok/sec: 149780.57\n",
    "Step   47 | loss: 6.032320 | lr: 6.746012149262733e-05 | norm: 0.7649 | dt: 110.12ms | tok/sec: 148776.95\n",
    "Step   48 | loss: 5.968857 | lr: 6.332414803931283e-05 | norm: 0.5618 | dt: 108.75ms | tok/sec: 150655.67\n",
    "Step   49 | loss: 5.882954 | lr: 6.083231989205545e-05 | norm: 0.6518 | dt: 110.61ms | tok/sec: 148121.79\n",
    "logits.shape: torch.Size([16, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "```\n",
    "\n",
    "### 10. gradient accummulation steps to \"increase\" effective batch size\n",
    "- fit larger batch sizes on our gpu\n",
    "- but for faithful replication of `learning_rate` (`6e-4`) we also need to match the `batch_size`(0.5M tokens) given in the GPT3 paper\n",
    "- current batch size = `B*T` $= 16 \\times 1024 = 16,384$ tokens\n",
    "- simulate a batch size of 0.5M\n",
    "```\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Step    0 | loss: 10.938566 | lr: 6.0000e-05 | norm: 27.0126 | dt: 20967.69ms | tok/sec: 25004.56\n",
    "Step    1 | loss: 9.649337 | lr: 1.2000e-04 | norm: 9.5177 | dt: 3627.09ms | tok/sec: 144548.00\n",
    "Step    2 | loss: 9.225610 | lr: 1.8000e-04 | norm: 5.7291 | dt: 3445.08ms | tok/sec: 152184.60\n",
    "Step    3 | loss: 9.813091 | lr: 2.4000e-04 | norm: 8.2064 | dt: 3443.46ms | tok/sec: 152256.20\n",
    "...\n",
    "Step   46 | loss: 5.797256 | lr: 7.3215e-05 | norm: 0.2464 | dt: 3515.07ms | tok/sec: 149154.37\n",
    "Step   47 | loss: 5.816778 | lr: 6.7460e-05 | norm: 0.1615 | dt: 3517.56ms | tok/sec: 149048.61\n",
    "Step   48 | loss: 5.790338 | lr: 6.3324e-05 | norm: 0.1888 | dt: 3512.78ms | tok/sec: 149251.64\n",
    "Step   49 | loss: 5.790049 | lr: 6.0832e-05 | norm: 0.1988 | dt: 3518.16ms | tok/sec: 149023.32\n",
    "logits.shape: torch.Size([16, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "\n",
    "```\n",
    "- note that overall `dt` now increased because we are doing multiple forward-backwards for a single update\n",
    "- this is just for simulating larger batch sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e5cd8-5bd8-4983-8100-10799ad2139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15a43d8-88ee-4a38-9838-f3506572c818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be7b3a7-8d72-4b01-94f5-1ed964dbd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del logits\n",
    "del x, y\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a1e57c6-d6df-4dc4-8cb8-19b27ca02045",
   "metadata": {},
   "outputs": [],
   "source": [
    "del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f4575e-bfed-4c8a-bfe1-059fbdfbdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d0a8bd-1334-4a32-b288-592f5e1f3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8add33-7c2e-45db-94f2-f140216ea8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr  6 21:48:48 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              46W / 250W |   9313MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    161416      C   .../macke/mwe102/.conda/sbi/bin/python     9300MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57dbd331-cda0-4062-8c90-9ead7d27fee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77543666-0262-4c3a-82b4-025fdea35b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab615f1-7145-4114-ab07-357b08cb50bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86aaec-0f8c-4fa1-8cec-eb805a5f6356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b396d-7d4d-41f7-8754-3a42a4cbe7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8e0a-e6f0-448a-9781-d53ee9f49d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e94c8-0301-40cd-a925-0e0b4e93fada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6363424-95a2-4563-8232-08c488ce2806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf37e1e-c86a-4d46-993b-012d2304a914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faa924-4201-4547-9017-f899bf36bf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4f19-e6b0-444f-9acb-18bc1bb2ab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aeab8-7d29-481c-a6c5-e55c9c244e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9cf1a-1b7a-460c-b9f7-3d8c60073944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d178f-297a-48fc-a98a-f33263dbce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7c10-a866-4e91-ace4-27493281098b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280e46-1caa-423c-9f35-1c1aa0c115b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0871ae6-7803-48d9-8a31-fddae538c186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
