{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f837c-2897-4a0a-855a-d4c76978d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.1.2\n",
      "matplotlib version: 3.9.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.4.1\n",
      "transformers version: 4.50.3\n",
      "tokenizers version: 0.21.1\n",
      "/mnt/lustre/work/macke/mwe102/.conda/sbi/bin/python\n",
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = ['numpy', 'matplotlib', 'tiktoken', 'torch', 'transformers', 'tokenizers']\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "!which python; python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8066db-c024-43a4-b09a-16943e44ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os, math\n",
    "import time, inspect\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7166e8-aecd-4ac6-9d13-83e1544a527a",
   "metadata": {},
   "source": [
    "Need the following:\n",
    "```python\n",
    "# from norms import get_norm, get_final_norm\n",
    "# from positional_embeddings import RotaryEmbedding, apply_rotary_pos_emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c74b95-2750-4193-9917-1bf29a518907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(config):\n",
    "    eps = config.get('layernorm_epsilon', 1e-5)\n",
    "    if config['norm'] == \"layernorm_nonparam\":\n",
    "        norm = LayerNorm_NonParam\n",
    "    elif config['norm'] == \"layernorm\":\n",
    "        norm = nn.LayerNorm\n",
    "    else:\n",
    "        raise ValueError(f\"norm {config['norm']} not recognized\")\n",
    "    return norm, eps\n",
    "\n",
    "\n",
    "def get_final_norm(config):\n",
    "    eps = config.get('layernorm_epsilon', 1e-5)\n",
    "    if config['final_norm'] == \"layernorm_nonparam\":\n",
    "        norm = LayerNorm_NonParam\n",
    "    elif config['final_norm'] == \"layernorm\":\n",
    "        norm = nn.LayerNorm\n",
    "    else:\n",
    "        raise ValueError(f\"norm {config['final_norm']} not recognized\")\n",
    "    return norm, eps\n",
    "\n",
    "\n",
    "class LayerNorm_NonParam(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_channels = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, normalized_shape=(self.num_channels,), eps=self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a242811-758e-4993-b789-73046e86fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, max_seq_len, base=10000, precision=torch.half, save_inv_freqs=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "        self.precision = precision\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.dim = dim\n",
    "\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        cos_cached, sin_cached, inv_freq = self._prepare_cache(\n",
    "            max_seq_len, precision, base\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n",
    "        self.cos_cached = cos_cached\n",
    "        self.sin_cached = sin_cached\n",
    "\n",
    "    def _prepare_cache(self, seq_len, precision, base):\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "\n",
    "        t = torch.arange(seq_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        cos_cached = emb.cos()[:, None, None, :]\n",
    "        sin_cached = emb.sin()[:, None, None, :]\n",
    "\n",
    "        return (\n",
    "            cos_cached.to(precision),\n",
    "            sin_cached.to(precision),\n",
    "            inv_freq.to(precision),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_dim=0, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[seq_dim]\n",
    "\n",
    "        assert seq_len <= self.max_seq_len\n",
    "\n",
    "        if seq_len != self.max_seq_len:\n",
    "            # y, z, _ = self._prepare_cache(seq_len, self.precision, self.base)\n",
    "            return (\n",
    "                self.cos_cached[:seq_len, ...].to(x.device),\n",
    "                self.sin_cached[:seq_len, ...].to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            return self.cos_cached.to(x.device), self.sin_cached.to(x.device)\n",
    "\n",
    "\n",
    "# rotary pos emb helpers:\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat(\n",
    "        (-x2, x1), dim=x1.ndim - 1\n",
    "    )  # dim=-1 triggers a bug in earlier torch versions\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n",
    "    cos, sin = (\n",
    "        cos[offset : q.shape[0] + offset, ...],\n",
    "        sin[offset : q.shape[0] + offset, ...],\n",
    "    )\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb_torch(\n",
    "    q, k, cos, sin, offset: int = 0\n",
    "):  # jitting fails with bf16\n",
    "    cos, sin = (\n",
    "        cos[offset : q.shape[0] + offset, ...],\n",
    "        sin[offset : q.shape[0] + offset, ...],\n",
    "    )\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a188905-177d-4b6c-8b4a-6e6fd1d91bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db572b9d-3703-4339-9c9e-98ad784ef866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_normalization(inputs, normalization_type, dim=-1):\n",
    "    if normalization_type == 'gelu_l2_norm':\n",
    "        nonlinear_outputs = F.gelu(inputs)\n",
    "        norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "        outputs = norm_outputs\n",
    "    elif normalization_type == 'l2_norm_gelu':\n",
    "        norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "        nonlinear_outputs = F.gelu(norm_outputs)\n",
    "        outputs = nonlinear_outputs\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9eb77a6-157c-4e1d-9ac3-9c7fd59f5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pattention(nn.Module):\n",
    "    \"\"\"Parameter-based attention layer\"\"\"\n",
    "    def __init__(self, input_channels, output_channels, param_token_num, normalization_type):\n",
    "        super().__init__()\n",
    "        self.key_param_tokens = nn.Parameter(torch.empty(param_token_num, input_channels))\n",
    "        self.value_param_tokens = nn.Parameter(torch.empty(param_token_num, output_channels))\n",
    "        self.normalization_type = normalization_type\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute attention weights using dot product between inputs and key parameters\n",
    "        attn_weights = inputs @ self.key_param_tokens.transpose(-2, -1)\n",
    "\n",
    "        # Apply nonlinear normalization\n",
    "        attn_weights = nonlinear_normalization(attn_weights, self.normalization_type)\n",
    "    \n",
    "        # Compute weighted sum with value parameters\n",
    "        output = attn_weights @ self.value_param_tokens\n",
    "        return output\n",
    "\n",
    "\n",
    "class TokenFormerBlock(nn.Module):\n",
    "    \"\"\"Main TokenFormer layer block\"\"\"\n",
    "    def __init__(self, hidden_size, qkv_slot_num, ffn_slot_num, proj_slot_num, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalization_type = config['norm_activation_type']\n",
    "        self.num_attention_heads = config['num_attention_heads']\n",
    "        self.hidden_size_per_attention_head = hidden_size // self.num_attention_heads\n",
    "        \n",
    "        # Get norm layers based on config\n",
    "        norm, eps = get_norm(config)\n",
    "        self.norm1 = norm(hidden_size, eps=eps)\n",
    "        self.norm2 = norm(hidden_size, eps=eps)\n",
    "        \n",
    "        # Self-attention with parameter tokens\n",
    "        self.query = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.key = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.value = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.proj = Pattention(hidden_size, hidden_size, proj_slot_num, self.normalization_type)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = Pattention(hidden_size, hidden_size, ffn_slot_num, self.normalization_type)\n",
    "\n",
    "        # Initialize rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=int(self.hidden_size_per_attention_head * 0.25),  # 25% of dimensions\n",
    "            base=10000,  # default base\n",
    "            max_seq_len=config.get('max_position_embeddings', 2048),\n",
    "            precision=torch.float16 if config.get('fp16', False) else torch.float32\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Get Q,K,V through parameter attention\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Compute self-attention\n",
    "        x = self.attention(q, k, v)\n",
    "        \n",
    "        # Project and add residual connection\n",
    "        x = self.proj(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "\n",
    "        # Reshape for attention heads\n",
    "        b, s, h = q.size()\n",
    "        q = q.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "        k = k.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "        v = v.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "\n",
    "        # At this point, dimensions are:\n",
    "        # q, k: [batch_size, seq_len, num_heads, head_size]\n",
    " \n",
    "        # Apply rotary embeddings to first 25% of dimensions\n",
    "        rotary_ndims = int(self.hidden_size_per_attention_head * 0.25)\n",
    "        \n",
    "        # Split the dimensions for partial rotary\n",
    "        q_rot, q_pass = q[..., :rotary_ndims], q[..., rotary_ndims:]\n",
    "        k_rot, k_pass = k[..., :rotary_ndims], k[..., rotary_ndims:]\n",
    "        \n",
    "        # After split:\n",
    "        # q_rot, k_rot: [batch_size, seq_len, num_heads, rotary_ndims]\n",
    "        # q_pass, k_pass: [batch_size, seq_len, num_heads, (head_size - rotary_ndims)]\n",
    "\n",
    "        # Get and apply rotary embeddings\n",
    "        seq_len = q.size(1)\n",
    "        cos, sin = self.rotary_emb(q, seq_len=seq_len)\n",
    "        q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin, offset=0)\n",
    "        \n",
    "        # Recombine rotary and pass-through dimensions\n",
    "        q = torch.cat((q_rot, q_pass), dim=-1)\n",
    "        k = torch.cat((k_rot, k_pass), dim=-1)\n",
    "        \n",
    "        # Transpose for attention calculation\n",
    "        q = q.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        k = k.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        v = v.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        \n",
    "        # Create causal mask with proper dimensions\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones((1, 1, seq_len, seq_len), dtype=torch.bool, device=q.device), \n",
    "            diagonal=1\n",
    "        )\n",
    "        attn_mask = torch.where(causal_mask, float('-inf'), 0.0)\n",
    "        \n",
    "        # Compute attention scores with proper scaling and normalization\n",
    "        scale = 1 / math.sqrt(self.hidden_size_per_attention_head)\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * scale  # [b, nh, s, s]\n",
    "        attn_weights = attn_weights + attn_mask\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = attn_weights @ v  # [b, nh, s, hs]\n",
    "\n",
    "        # Reshape and return\n",
    "        x = x.transpose(1, 2).contiguous().view(b, s, h)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenFormer(nn.Module):\n",
    "    \"\"\"TokenFormer model for inference\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        qkv_slot_num=768,\n",
    "        ffn_slot_num=3072,\n",
    "        proj_slot_num=768,\n",
    "        max_position_embeddings=2048,\n",
    "        config=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # TokenFormer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TokenFormerBlock(\n",
    "                hidden_size=hidden_size,\n",
    "                qkv_slot_num=qkv_slot_num,\n",
    "                ffn_slot_num=ffn_slot_num,\n",
    "                proj_slot_num=proj_slot_num,\n",
    "                config=config\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Get final norm based on config\n",
    "        final_norm, final_eps = get_final_norm(config)\n",
    "        self.norm_f = final_norm(hidden_size, eps=final_eps)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "        # initialize and report number of parameters\n",
    "        self.initialize_parameters()\n",
    "        print(\"init and number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        since all trainable tokens are in nn.Parameter containers, they need\n",
    "        to be initialized by hand as pytorch won't do it\n",
    "        \"\"\"\n",
    "        std = 0.02\n",
    "        for name, param in self.named_parameters():\n",
    "            if ('weight' in name) or ('param_tokens' in name):\n",
    "                if param.dim() >= 2:\n",
    "                    torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "                else:\n",
    "                    torch.nn.init.zeros_(param) # for 1d bias like params (if any)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        Note that here they are not doing weight tying for some reason, i.e.,\n",
    "        `self.word_embeddings.weight` is not same as `self.output.weight`\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.word_embeddings.weight.numel()\n",
    "        return n_params\n",
    "        \n",
    "    def forward(self, input_ids, targets=None):\n",
    "        # Embeddings\n",
    "        hidden_states = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # Forward through layers\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "            \n",
    "        # Final norm and output projection\n",
    "        hidden_states = self.norm_f(hidden_states)\n",
    "        logits = self.output(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # just pick out params that require grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups -- all 2d params will be weight decayed, biases and layernorms no decay\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        \n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5a38c5-572d-457d-b709-4afccd951974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer...\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from tokenizer import build_tokenizer\n",
    "\n",
    "def load_config(model_type='150M'):\n",
    "    assert model_type in {'150M', '450M', '900M', '1-5B'}\n",
    "    config_path = f'../tokenformer-minimal/config/{model_type}_eval.yml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "model_size = '150M'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = build_tokenizer(\n",
    "    tokenizer_type=config['tokenizer-type'],\n",
    "    vocab_file=config['vocab-file'],\n",
    "    padding_multiple=128\n",
    ")\n",
    "\n",
    "# regular gpt2 tokenizer used in nanoGPT\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc5f363-6b77-4c07-92c2-24ddf8351952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['tokenizer-type'], config['vocab-file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088131e5-b919-4dcd-ae00-9c6f3d2be8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12092, 13, 309, 1353, 247, 3448, 1566, 13], \"Hello, I'm a language model,\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Hello, I'm a language model,\"), tokenizer.detokenize([12092, 13, 309, 1353, 247, 3448, 1566, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e206c7a-f83a-41a6-80d2-8edc748c1f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15496, 11, 314, 1101, 257, 3303, 2746, 11],\n",
       " \"Hello, I'm a language model,\",\n",
       " ' coup. T topï¿½pri until.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hello, I'm a language model,\"), enc.decode([15496, 11, 314, 1101, 257, 3303, 2746, 11]), enc.decode([12092, 13, 309, 1353, 247, 3448, 1566, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dca92f-e407-4b23-bbfe-fac98e8d3b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce94c25-8f4a-48d7-89d7-135bd27143b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50277, 50304)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size, tokenizer.padded_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03bd3d-7b05-459b-b6ee-09e7eba25eb4",
   "metadata": {},
   "source": [
    "## Skip the below cells and move to the pretraining cells to initialize model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "059a5d90-4b87-4225-97ae-76c7a20a13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move model to GPU if available\n",
    "# device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "# print(f\"using device: {device}\")\n",
    "# # device = \"cpu\" # OVERRIDE\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "\n",
    "# # Initialize model\n",
    "# model_size = '150M'\n",
    "# config = load_config(model_type=model_size)\n",
    "\n",
    "# model = TokenFormer(\n",
    "#     vocab_size = tokenizer.padded_vocab_size,\n",
    "#     hidden_size = config['hidden_size'],\n",
    "#     num_layers = config['num_layers'],\n",
    "#     qkv_slot_num = config['qkv_slot_num'],\n",
    "#     ffn_slot_num = config['ffn_slot_num'],\n",
    "#     proj_slot_num = config['proj_slot_num'],\n",
    "#     max_position_embeddings = config['max_position_embeddings'],\n",
    "#     config=config\n",
    "# )\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01300cc0-2319-4e36-b59e-3dd11dcf2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f4c7955-1f4b-48eb-8153-ea6645739733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dict = dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1704f1a-888e-4a02-b2cd-255a90e4be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in param_dict.keys():\n",
    "#     print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bab4868c-8ce9-4933-b356-0863e7650935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80ed56-af06-422f-b42f-41e232d72032",
   "metadata": {},
   "source": [
    "## 1. check fineweb val-loss on randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d60f505-4b1f-453f-a359-63826b6cc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test on val data\n",
    "def load_tokens(filename: str):\n",
    "    # expects .npy file\n",
    "    npt = np.load(filename)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank=0, num_processes=1, split='train', data_root='edu_fineweb10B_tokenformer'):\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filename\n",
    "        data_root = data_root\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f\"with data root: {data_root} found: {len(shards)} shards for: {split} split and num processes: {self.num_processes}\")\n",
    "\n",
    "        # state management\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # useful in val_loader.reset()\n",
    "        # state, initialize at shard 0 or first file\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        # prepare inputs and targets for a single *step* of the optimization\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "        x = buf[:-1].view(B, T) # remove last token\n",
    "        y = buf[1:].view(B, T)  # remove first token\n",
    "        # advance to the next chunk of the array\n",
    "        self.current_position += (B * T * self.num_processes)\n",
    "        # check for next batch loading for all processes\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B*T*self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n",
    "\n",
    "# B = 32   # micro batch size: how many rows we are processing in a single forward-backward step (32 fits in one A100 40GB, was 16 earlier)\n",
    "# T = 1024 # sequence length\n",
    "# # initialize the dataloader\n",
    "# val_loader = DataLoaderLite(B=B, T=T, split='val', data_root='edu_fineweb10B_tokenformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65faa6be-b3f0-42ec-b085-c5310791f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches, print_loss=True) -> float:\n",
    "    model.eval()\n",
    "    data_loader.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_accum = 0.0\n",
    "        loss_steps = num_batches\n",
    "        for _ in range(loss_steps):\n",
    "            x, y = data_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "            loss = loss / loss_steps\n",
    "            # print(f'loss at step {_}: {loss.detach().item():.4f}')\n",
    "            loss_accum += loss.detach()\n",
    "    \n",
    "    if print_loss:\n",
    "        # averaged per-step loss, averaged over `num_batches` batches or steps\n",
    "        print(f\"Validation loss: {loss_accum.item():.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    return loss_accum.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f97447-4c76-4a85-8bcd-a7831bb586af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_and_print_samples(model, tokenizer, device)\n",
    "print(f\"Val loss for random tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d53c56-3540-4d59-a4ae-b6b9769f27f6",
   "metadata": {},
   "source": [
    "Observation: It is comparable to GPT2 loss of around 10, so everything going good so far!\n",
    "\n",
    "## 2. load official pre-trained weights and check its val-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed4760f3-8f30-41ae-af46-870f27ee0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(num_layers):\n",
    "    # Create mapping for model weights\n",
    "    remap_dict = {\n",
    "        'sequential.0.word_embeddings': 'word_embeddings'\n",
    "    }\n",
    "    \n",
    "    # Map layers\n",
    "    for i in range(2, num_layers + 2):\n",
    "        new_idx = i - 2\n",
    "        remap_dict[f'sequential.{i}.attention'] = f'layers.{new_idx}'\n",
    "        remap_dict[f'sequential.{i}.mlp'] = f'layers.{new_idx}.ffn'\n",
    "    \n",
    "    # Map final norm\n",
    "    remap_dict[f'sequential.{num_layers + 3}.norm'] = 'norm_f'\n",
    "    \n",
    "    return remap_dict\n",
    "\n",
    "\n",
    "def load_model(model, weights_path, num_layers):\n",
    "    \"\"\"Load model weights from state dict\"\"\"\n",
    "    state_dict = torch.load(weights_path, weights_only=True)\n",
    "    \n",
    "    remap_dict = create_mapping(num_layers)\n",
    "    \n",
    "    # Copy output weight from word embeddings if missing\n",
    "    if 'output.weight' not in state_dict:\n",
    "        state_dict['output.weight'] = state_dict['sequential.0.word_embeddings.weight']\n",
    "    \n",
    "    # Rename keys in-place by replacing prefixes\n",
    "    for old_prefix, new_prefix in remap_dict.items():\n",
    "        for key in list(state_dict.keys()):\n",
    "            if key.startswith(old_prefix):\n",
    "                new_key = key.replace(old_prefix, new_prefix, 1)\n",
    "                state_dict[new_key] = state_dict.pop(key)\n",
    "    \n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa3bda43-a791-4a1b-b0a6-d7eb3c9d51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init and number of parameters: 151.88M\n",
      "Loading pre-trained tokenformer 150M weights on cuda...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = TokenFormer(\n",
    "    vocab_size = tokenizer.padded_vocab_size,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Loading pre-trained tokenformer {model_size} weights on {device}...\")\n",
    "model_bin_file = f'../tokenformer-minimal/pytorch_model_{model_size}.bin'\n",
    "load_model(model, model_bin_file, config['num_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "281ff13f-a7c0-494c-adfb-b52f32be5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7785ab7e-b53f-4e99-9b6f-7dafd6866e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.4403\n",
      "Val loss for pre-trained tokenformer 150M: 6.440320014953613\n"
     ]
    }
   ],
   "source": [
    "# generate_and_print_samples(model, tokenizer, device)\n",
    "print(f\"Val loss for pre-trained tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58195d53-b469-4715-bb56-cec96eb220fe",
   "metadata": {},
   "source": [
    "Observation: Hmm, this looks a bit suspicious! I think GPT2 124M had a validation loss of around $3.24$ on the same set. Not sure if everythin is working correctly.\n",
    "- I made sure to compute the validation loss on a tokenformer tokenized version of fineweb, and not the GPT2 tiktoken one.\n",
    "- I need to check what they report in the paper, may be I need to check on the owt dataset instead of fineweb!\n",
    "- \n",
    "### Using pre-trained weights from Tokenformer is a pain!\n",
    "- It is using a different tokenizer, not the tiktoken gpt2 tokenizer!\n",
    "- I created another version of the fineweb tokenized data using this tokenizer to test this model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2216e72-d627-4ec2-957d-a9864fa96830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_samples(model, tokenizer, device,\n",
    "                               num_return_sequences = 4,\n",
    "                               max_length = 32,\n",
    "                               start_context = \"Hello, I'm a language model,\",\n",
    "                               random_seed = 42\n",
    "                               ):\n",
    "    \n",
    "    model.eval()\n",
    "    encoder = None\n",
    "    decoder = None\n",
    "    if hasattr(tokenizer, 'encode'):\n",
    "        encoder = tokenizer.encode\n",
    "        decoder = tokenizer.decode\n",
    "    elif hasattr(tokenizer, 'tokenize'):\n",
    "        encoder = tokenizer.tokenize\n",
    "        decoder = tokenizer.detokenize\n",
    "    else:\n",
    "        raise ValueError(f\"Please pass a tokenizer with either encode/decode or tokenize/detokenize methods\")\n",
    "\n",
    "    \n",
    "    tokens = encoder(start_context)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "    xgen = tokens.to(device)\n",
    "    # don't interfere with other seeds\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(random_seed)\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            # print(logits.shape, logits.dtype)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()\n",
    "        decoded = decoder(tokens)\n",
    "        print(f\"sample {i}: {decoded}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e9e0318-2bbc-442e-af05-26ad4c460c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model, I'm from the, we can not your English, please. A is I, the your English is to, is\n",
      "sample 1: Hello, I'm a language model, computer graphics trainer, a lot more video, a way an online\n",
      "can\n",
      "\n",
      "you. I just.\n",
      "be\n",
      "sample 2: Hello, I'm a language model, a new born girl new boy,\n",
      "\n",
      "s, I don'ts, it,\n",
      "\n",
      "c, a kid\n",
      "sample 3: Hello, I'm a language model, you should be\n",
      "\n",
      "\n",
      "I need\n",
      "very\n",
      "I use in my you here is your as if I should can\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "de90dfb7-5f99-45e7-83f2-c5d12f9965d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_and_print_samples(model, enc, device) # doesn't work with tiktoken tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af8e2b16-020b-48f9-a64f-8607b53269a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, {'<|endoftext|>'})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.n_vocab, enc.special_tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2a55794-14d5-431f-9b3c-80c6419fde8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50277"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e973b6d-b834-41ce-a212-6d1b5322edcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['<|endoftext|>'] # another difference from the tiktoken tokenizer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e58d13-d452-4eff-819e-7a8e8d69151c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73708d92-ae6c-461f-89df-032b25c625a3",
   "metadata": {},
   "source": [
    "# Pre-training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec775e-6a35-43e2-9725-cb8cdbf66e06",
   "metadata": {},
   "source": [
    "## Dataloader for the fineweb dataset (assuming shards are already available)\n",
    "- since I am pretraining from scratch, I will just use the original GPT2 fwt data tokenized with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d21ae3ad-cff7-4f72-83c2-5e38099dbd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "# device = \"cpu\" # OVERRIDE\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "589da6b0-a8d9-4724-9dcb-4338cd5432b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288 tokens\n",
      "=> calculated gradient accumulation steps: 64\n",
      "with data root: edu_fineweb10B found: 99 shards for: train split and num processes: 1\n",
      "with data root: edu_fineweb10B found: 1 shards for: val split and num processes: 1\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524_288 # 2**19, closest power of 2 to ~0.5M\n",
    "B = 8    # micro batch size: how many rows we are processing in a single forward-backward step (16 fits nicht in one A100 40GB)\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T) == 0, \"total batch size in number of tokens should be divisible by B*T\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size: {total_batch_size} tokens\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "# initialize the dataloader\n",
    "train_loader = DataLoaderLite(B=B, T=T, split='train', data_root='edu_fineweb10B')\n",
    "val_loader = DataLoaderLite(B=B, T=T, split='val', data_root='edu_fineweb10B')\n",
    "\n",
    "# initialize tokenizer for sampling through the model\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c03a1fc4-7ee6-442e-8067-67e607f5c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tf32, now matmuls will use tf32 (tensor cores from A100)\n",
    "torch.set_float32_matmul_precision('high') # default is highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b41c5d-1ace-4a85-af2d-1ca291aaeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # prev constant lr that we were using\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 100\n",
    "max_steps = 6001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cad1eba-c4c4-49fe-8096-e2e2373081e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # linear warmup\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # if it > lr decay iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1, goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f09aed-869e-460f-8c82-1887a7db6a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(max_steps)\n",
    "lrs = np.array([get_lr(it) for it in steps])\n",
    "\n",
    "plt.plot(steps, lrs); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6445eac-22c0-4277-a5a6-a8f33c8f74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50304"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padded_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47cdc7d2-38a0-4608-ab86-9d9a8b436f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init and number of parameters: 151.88M\n",
      "num decayed parameter tensors: 122, with 190,513,152 parameters\n",
      "num non-decayed parameter tensors: 2, with 1,536 parameters\n",
      "using fused AdamW: True\n",
      "Validation loss: 10.8258\n",
      "sample 0: Hello, I'm a language model,K3*5\"+0,?7EI;1=&01A(#3H&\n",
      "sample 1: Hello, I'm a language model,97B*#5OHM#A5/LB%Q#4:1QI<\n",
      "sample 2: Hello, I'm a language model,/N:5=/B<M.=O)%%A:DJCP*&9\n",
      "sample 3: Hello, I'm a language model,'7J%8($/+K;'D.L3FA/K4E87\n",
      "Step    0 | loss: 10.825845 | lr: 6.0000e-06 | norm: 0.0687 | dt: 109628.65ms | tok/sec: 4782.40\n",
      "Step   10 | loss: 10.824967 | lr: 6.6000e-05 | norm: 0.1053 | dt: 6528.14ms | tok/sec: 80311.98\n",
      "Step   20 | loss: 10.820913 | lr: 1.2600e-04 | norm: 0.1286 | dt: 6569.00ms | tok/sec: 79812.48\n",
      "Step   30 | loss: 10.806940 | lr: 1.8600e-04 | norm: 0.1899 | dt: 6587.19ms | tok/sec: 79592.10\n",
      "Step   40 | loss: 10.767907 | lr: 2.4600e-04 | norm: 0.3034 | dt: 6611.66ms | tok/sec: 79297.52\n",
      "Step   50 | loss: 10.686462 | lr: 3.0600e-04 | norm: 0.4461 | dt: 6623.24ms | tok/sec: 79158.82\n",
      "Step   60 | loss: 10.532628 | lr: 3.6600e-04 | norm: 0.6283 | dt: 6628.80ms | tok/sec: 79092.48\n",
      "Step   70 | loss: 10.278986 | lr: 4.2600e-04 | norm: 0.8264 | dt: 6642.13ms | tok/sec: 78933.66\n",
      "Step   80 | loss: 9.919199 | lr: 4.8600e-04 | norm: 0.9865 | dt: 6641.96ms | tok/sec: 78935.79\n",
      "Step   90 | loss: 9.462133 | lr: 5.4600e-04 | norm: 1.0469 | dt: 6644.93ms | tok/sec: 78900.42\n",
      "Validation loss: 8.9800\n",
      "sample 0: Hello, I'm a language model, study,- or the a on with so will ( its: from can is on from some and. from the is\n",
      "sample 1: Hello, I'm a language model, are not (-. or it into it. The are bes many to time. will: for two its can\n",
      "sample 2: Hello, I'm a language model, be it not are when be some so; This more it\n",
      "- to that when ( even some part- is so\n",
      "sample 3: Hello, I'm a language model, is so even of the\n",
      " the be a even more in ( as in for its that be; for such are have\n",
      "Step  100 | loss: 9.010059 | lr: 6.0000e-04 | norm: 0.9135 | dt: 6639.43ms | tok/sec: 78965.78\n",
      "Step  110 | loss: 8.518903 | lr: 6.0000e-04 | norm: 0.7407 | dt: 6652.62ms | tok/sec: 78809.25\n",
      "Step  120 | loss: 8.117813 | lr: 5.9998e-04 | norm: 0.5759 | dt: 6661.91ms | tok/sec: 78699.31\n",
      "Step  130 | loss: 7.832567 | lr: 5.9997e-04 | norm: 0.4614 | dt: 6696.01ms | tok/sec: 78298.59\n",
      "Step  140 | loss: 7.684131 | lr: 5.9994e-04 | norm: 0.3220 | dt: 6683.97ms | tok/sec: 78439.60\n",
      "Step  150 | loss: 7.487566 | lr: 5.9990e-04 | norm: 0.2502 | dt: 6684.65ms | tok/sec: 78431.65\n",
      "Step  160 | loss: 7.336711 | lr: 5.9986e-04 | norm: 0.2024 | dt: 6692.03ms | tok/sec: 78345.14\n",
      "Step  170 | loss: 7.232697 | lr: 5.9981e-04 | norm: 0.2391 | dt: 6696.01ms | tok/sec: 78298.58\n",
      "Step  180 | loss: 7.082529 | lr: 5.9976e-04 | norm: 0.2192 | dt: 6694.30ms | tok/sec: 78318.55\n",
      "Step  190 | loss: 7.097057 | lr: 5.9969e-04 | norm: 0.2033 | dt: 6918.80ms | tok/sec: 75777.31\n",
      "Validation loss: 6.9645\n",
      "sample 0: Hello, I'm a language model, was theï¿½-\n",
      " is. It were are be from the other first to the and we in, in the a\n",
      "sample 1: Hello, I'm a language model, they can or the, by of our the, at by a. If to be to of a an, are also\n",
      "sample 2: Hello, I'm a language model, and the with- It- on\n",
      "-s than the in of the the in the your the two the a as\n",
      "sample 3: Hello, I'm a language model, in for the and the\n",
      "The most a way\n",
      "- A that of, was be have a this have to their\n",
      "Step  200 | loss: 6.946281 | lr: 5.9962e-04 | norm: 0.2111 | dt: 6704.50ms | tok/sec: 78199.42\n",
      "Step  210 | loss: 6.788534 | lr: 5.9954e-04 | norm: 0.2528 | dt: 6706.29ms | tok/sec: 78178.49\n",
      "Step  220 | loss: 6.683481 | lr: 5.9945e-04 | norm: 0.2138 | dt: 6712.79ms | tok/sec: 78102.88\n",
      "Step  230 | loss: 6.610909 | lr: 5.9935e-04 | norm: 0.1930 | dt: 6713.80ms | tok/sec: 78091.14\n",
      "Step  240 | loss: 6.674554 | lr: 5.9925e-04 | norm: 0.1645 | dt: 6718.37ms | tok/sec: 78037.96\n",
      "Step  250 | loss: 6.581188 | lr: 5.9914e-04 | norm: 0.1640 | dt: 6718.10ms | tok/sec: 78041.12\n",
      "Step  260 | loss: 6.394949 | lr: 5.9902e-04 | norm: 0.1692 | dt: 6720.66ms | tok/sec: 78011.41\n",
      "Step  270 | loss: 6.409761 | lr: 5.9889e-04 | norm: 0.1531 | dt: 6720.67ms | tok/sec: 78011.29\n",
      "Step  280 | loss: 6.463789 | lr: 5.9876e-04 | norm: 0.1814 | dt: 6722.97ms | tok/sec: 77984.63\n",
      "Step  290 | loss: 6.332021 | lr: 5.9862e-04 | norm: 0.1412 | dt: 6722.76ms | tok/sec: 77987.03\n",
      "Validation loss: 6.3483\n",
      "sample 0: Hello, I'm a language model, though, not to the child as not go and are made in any own the best for they were used with the way\n",
      "sample 1: Hello, I'm a language model, when so he has to look in children, it-B, the next, but you to the study that would be\n",
      "sample 2: Hello, I'm a language model, you are used in that may like.\n",
      "For, and even to do not also see.\n",
      "The first of us\n",
      "sample 3: Hello, I'm a language model, they might be more than itât in the way to thisâa in some as and some place?\n",
      "Step  300 | loss: 6.434386 | lr: 5.9847e-04 | norm: 0.2339 | dt: 6722.51ms | tok/sec: 77989.96\n",
      "Step  310 | loss: 6.227962 | lr: 5.9831e-04 | norm: 0.1834 | dt: 6723.70ms | tok/sec: 77976.07\n",
      "Step  320 | loss: 6.140471 | lr: 5.9815e-04 | norm: 0.1816 | dt: 6724.71ms | tok/sec: 77964.42\n",
      "Step  330 | loss: 6.307300 | lr: 5.9798e-04 | norm: 0.2131 | dt: 6728.91ms | tok/sec: 77915.76\n",
      "Step  340 | loss: 6.234207 | lr: 5.9780e-04 | norm: 0.2313 | dt: 6737.56ms | tok/sec: 77815.68\n",
      "Step  350 | loss: 6.135430 | lr: 5.9761e-04 | norm: 0.1686 | dt: 6729.88ms | tok/sec: 77904.48\n",
      "Step  360 | loss: 6.070980 | lr: 5.9742e-04 | norm: 0.1654 | dt: 6730.70ms | tok/sec: 77895.05\n",
      "Step  370 | loss: 6.049644 | lr: 5.9722e-04 | norm: 0.2184 | dt: 6732.75ms | tok/sec: 77871.30\n",
      "Step  380 | loss: 6.147229 | lr: 5.9701e-04 | norm: 0.2660 | dt: 6731.16ms | tok/sec: 77889.64\n",
      "Step  390 | loss: 6.095415 | lr: 5.9679e-04 | norm: 0.2629 | dt: 6730.30ms | tok/sec: 77899.69\n",
      "Validation loss: 6.0493\n",
      "sample 0: Hello, I'm a language model,000, in the first way for more accurate?\n",
      "All the end as âthe last. âTheï¿½\n",
      "sample 1: Hello, I'm a language model, that in the most common from a simple. It't to be an to have to a variety of them in the point\n",
      "sample 2: Hello, I'm a language model, and I just if you may try, and can tell you, we are a great way.\n",
      "Most of your ability\n",
      "sample 3: Hello, I'm a language model, a way to make a number. They are given for a couple of time to build it as well as a big point\n",
      "Step  400 | loss: 5.984641 | lr: 5.9656e-04 | norm: 0.1909 | dt: 6722.86ms | tok/sec: 77985.84\n",
      "Step  410 | loss: 5.913899 | lr: 5.9633e-04 | norm: 0.3057 | dt: 6740.59ms | tok/sec: 77780.78\n",
      "Step  420 | loss: 6.014124 | lr: 5.9609e-04 | norm: 0.1942 | dt: 6728.69ms | tok/sec: 77918.33\n",
      "Step  430 | loss: 6.030522 | lr: 5.9584e-04 | norm: 0.3363 | dt: 6730.42ms | tok/sec: 77898.22\n",
      "Step  440 | loss: 5.854567 | lr: 5.9559e-04 | norm: 0.2526 | dt: 6726.80ms | tok/sec: 77940.12\n",
      "Step  450 | loss: 5.879199 | lr: 5.9533e-04 | norm: 0.1763 | dt: 6731.29ms | tok/sec: 77888.15\n",
      "Step  460 | loss: 5.818698 | lr: 5.9506e-04 | norm: 0.2385 | dt: 6726.97ms | tok/sec: 77938.19\n",
      "Step  470 | loss: 5.901634 | lr: 5.9478e-04 | norm: 0.2782 | dt: 6728.35ms | tok/sec: 77922.23\n",
      "Step  480 | loss: 5.868943 | lr: 5.9449e-04 | norm: 0.3534 | dt: 6724.50ms | tok/sec: 77966.85\n",
      "Step  490 | loss: 5.799409 | lr: 5.9420e-04 | norm: 0.2011 | dt: 6729.05ms | tok/sec: 77914.09\n",
      "Validation loss: 5.8032\n",
      "sample 0: Hello, I'm a language model, however, not a few months or if, this is how we would seem the same,âre able to get\n",
      "sample 1: Hello, I'm a language model, there may not a lot of this study. I had all of our website and a few-based computer and any is\n",
      "sample 2: Hello, I'm a language model, but the problem of the problem. If you don't have no matter, you don't want to play a good information\n",
      "sample 3: Hello, I'm a language model, it as a little more the time to a special idea of the book in this blog that, but when the two main\n",
      "Step  500 | loss: 5.747663 | lr: 5.9390e-04 | norm: 0.2740 | dt: 6724.37ms | tok/sec: 77968.38\n",
      "Step  510 | loss: 5.792906 | lr: 5.9359e-04 | norm: 0.2475 | dt: 6723.49ms | tok/sec: 77978.58\n",
      "Step  520 | loss: 5.854936 | lr: 5.9328e-04 | norm: 0.3363 | dt: 6724.59ms | tok/sec: 77965.81\n",
      "Step  530 | loss: 5.764890 | lr: 5.9296e-04 | norm: 0.2453 | dt: 6721.90ms | tok/sec: 77997.01\n",
      "Step  540 | loss: 5.716976 | lr: 5.9263e-04 | norm: 0.3795 | dt: 6728.16ms | tok/sec: 77924.41\n",
      "Step  550 | loss: 5.633322 | lr: 5.9229e-04 | norm: 0.3562 | dt: 6727.72ms | tok/sec: 77929.48\n",
      "Step  560 | loss: 5.739909 | lr: 5.9194e-04 | norm: 0.3249 | dt: 6730.65ms | tok/sec: 77895.63\n",
      "Step  570 | loss: 5.628808 | lr: 5.9159e-04 | norm: 0.2560 | dt: 6725.96ms | tok/sec: 77949.89\n",
      "Step  580 | loss: 5.596092 | lr: 5.9123e-04 | norm: 0.5371 | dt: 6726.25ms | tok/sec: 77946.53\n",
      "Step  590 | loss: 5.566460 | lr: 5.9086e-04 | norm: 0.3062 | dt: 6719.26ms | tok/sec: 78027.65\n",
      "Validation loss: 5.5900\n",
      "sample 0: Hello, I'm a language model, many of which is the case is used because all of reading, in general-based. Here's the word is no\n",
      "sample 1: Hello, I'm a language model, an event, so you've found thereâll go on your voice you can also start. They will probably don\n",
      "sample 2: Hello, I'm a language model, Iâre an âHâ because it. The article is just like the âtheâ\n",
      "sample 3: Hello, I'm a language model, the time to use of the studentâs.\n",
      "Mirin the second book to be one of this project\n",
      "Step  600 | loss: 5.484452 | lr: 5.9049e-04 | norm: 0.3648 | dt: 6717.37ms | tok/sec: 78049.63\n",
      "Step  610 | loss: 5.605820 | lr: 5.9011e-04 | norm: 0.4406 | dt: 6724.82ms | tok/sec: 77963.08\n",
      "Step  620 | loss: 5.575811 | lr: 5.8972e-04 | norm: 0.3208 | dt: 6732.06ms | tok/sec: 77879.24\n",
      "Step  630 | loss: 5.480040 | lr: 5.8932e-04 | norm: 0.3059 | dt: 6729.35ms | tok/sec: 77910.60\n",
      "Step  640 | loss: 5.246918 | lr: 5.8892e-04 | norm: 0.4331 | dt: 6728.70ms | tok/sec: 77918.22\n",
      "Step  650 | loss: 5.449978 | lr: 5.8851e-04 | norm: 0.3710 | dt: 6732.00ms | tok/sec: 77880.01\n",
      "Step  660 | loss: 5.462544 | lr: 5.8809e-04 | norm: 0.3834 | dt: 6727.52ms | tok/sec: 77931.80\n",
      "Step  670 | loss: 5.323387 | lr: 5.8766e-04 | norm: 0.4753 | dt: 6727.33ms | tok/sec: 77934.00\n",
      "Step  680 | loss: 5.337374 | lr: 5.8723e-04 | norm: 0.5199 | dt: 6736.22ms | tok/sec: 77831.13\n",
      "Step  690 | loss: 5.255282 | lr: 5.8679e-04 | norm: 0.4536 | dt: 6728.94ms | tok/sec: 77915.44\n",
      "Validation loss: 5.3825\n",
      "sample 0: Hello, I'm a language model, Iâve always read this: there all of students to my thoughts that you will look up with my students!\n",
      "sample 1: Hello, I'm a language model, this blog, I am using it an essential part - an additional examples of an individual or text, not.\n",
      "If\n",
      "sample 2: Hello, I'm a language model, and I must always come down out a student who. I would say it, or not Iâre going too\n",
      "sample 3: Hello, I'm a language model, or âTheâ, and âIâ but I have changed a word, not just the subject\n",
      "Step  700 | loss: 5.462011 | lr: 5.8634e-04 | norm: 0.7412 | dt: 6719.26ms | tok/sec: 78027.63\n",
      "Step  710 | loss: 5.320077 | lr: 5.8589e-04 | norm: 0.3466 | dt: 6722.03ms | tok/sec: 77995.49\n",
      "Step  720 | loss: 5.290831 | lr: 5.8542e-04 | norm: 0.5184 | dt: 6717.01ms | tok/sec: 78053.83\n",
      "Step  730 | loss: 5.220113 | lr: 5.8496e-04 | norm: 0.3337 | dt: 6732.92ms | tok/sec: 77869.32\n",
      "Step  740 | loss: 5.099575 | lr: 5.8448e-04 | norm: 0.5310 | dt: 6724.64ms | tok/sec: 77965.19\n",
      "Step  750 | loss: 5.301366 | lr: 5.8399e-04 | norm: 0.4437 | dt: 6720.06ms | tok/sec: 78018.34\n",
      "Step  760 | loss: 5.168576 | lr: 5.8350e-04 | norm: 0.3897 | dt: 6717.96ms | tok/sec: 78042.74\n",
      "Step  770 | loss: 5.184187 | lr: 5.8300e-04 | norm: 0.5743 | dt: 6717.68ms | tok/sec: 78046.04\n",
      "Step  780 | loss: 5.051894 | lr: 5.8250e-04 | norm: 0.4369 | dt: 6718.53ms | tok/sec: 78036.12\n",
      "Step  790 | loss: 5.154297 | lr: 5.8199e-04 | norm: 0.4311 | dt: 6716.40ms | tok/sec: 78060.82\n",
      "Validation loss: 5.1146\n",
      "sample 0: Hello, I'm a language model, and I'd have to use. If for them, go, to talk and other questions; they're the best part\n",
      "sample 1: Hello, I'm a language model, with many areas of which are. Maybe I've only say that the one knows is a little more time, as the\n",
      "sample 2: Hello, I'm a language model, I've like with, in its field. In early, this was not a lot of the world.\n",
      "The History\n",
      "sample 3: Hello, I'm a language model, which says, \"the\" of life. Even if you're reading that is more. There are you, I was\n",
      "Step  800 | loss: 5.193605 | lr: 5.8147e-04 | norm: 0.5770 | dt: 6717.98ms | tok/sec: 78042.52\n",
      "Step  810 | loss: 5.125810 | lr: 5.8094e-04 | norm: 0.3879 | dt: 6719.53ms | tok/sec: 78024.56\n",
      "Step  820 | loss: 5.015466 | lr: 5.8041e-04 | norm: 0.5126 | dt: 6714.14ms | tok/sec: 78087.17\n",
      "Step  830 | loss: 4.898965 | lr: 5.7986e-04 | norm: 0.4272 | dt: 6721.19ms | tok/sec: 78005.21\n",
      "Step  840 | loss: 5.037427 | lr: 5.7932e-04 | norm: 0.4946 | dt: 6719.41ms | tok/sec: 78025.88\n",
      "Step  850 | loss: 5.037181 | lr: 5.7876e-04 | norm: 0.4367 | dt: 6724.48ms | tok/sec: 77967.08\n",
      "Step  860 | loss: 4.938249 | lr: 5.7820e-04 | norm: 0.3689 | dt: 6719.43ms | tok/sec: 78025.67\n",
      "Step  870 | loss: 4.865870 | lr: 5.7763e-04 | norm: 0.3548 | dt: 6713.95ms | tok/sec: 78089.36\n",
      "Step  880 | loss: 4.859737 | lr: 5.7705e-04 | norm: 0.8455 | dt: 6720.52ms | tok/sec: 78012.97\n",
      "Step  890 | loss: 5.018443 | lr: 5.7647e-04 | norm: 0.3738 | dt: 6713.75ms | tok/sec: 78091.68\n",
      "Validation loss: 4.9101\n",
      "sample 0: Hello, I'm a language model, and I am not sure to say something (though the ones you get from this word), like: \"How is that\n",
      "sample 1: Hello, I'm a language model, this kind of problems in the context of the world from Earth, in their first place in the universe are, and then\n",
      "sample 2: Hello, I'm a language model, but I could take notes about the key. But is, what's not the first thing that we can do with in\n",
      "sample 3: Hello, I'm a language model, we also create a new concept thatâs in the two things you need?<|endoftext|>If you read the page as\n",
      "Step  900 | loss: 4.915474 | lr: 5.7588e-04 | norm: 0.2931 | dt: 6708.44ms | tok/sec: 78153.50\n",
      "Step  910 | loss: 4.898431 | lr: 5.7528e-04 | norm: 0.5009 | dt: 6724.08ms | tok/sec: 77971.67\n",
      "Step  920 | loss: 4.703256 | lr: 5.7468e-04 | norm: 0.4566 | dt: 6725.66ms | tok/sec: 77953.33\n",
      "Step  930 | loss: 4.871510 | lr: 5.7407e-04 | norm: 0.3799 | dt: 6724.44ms | tok/sec: 77967.48\n",
      "Step  940 | loss: 4.931932 | lr: 5.7345e-04 | norm: 0.4515 | dt: 6721.19ms | tok/sec: 78005.25\n",
      "Step  950 | loss: 4.821862 | lr: 5.7282e-04 | norm: 0.4809 | dt: 6728.76ms | tok/sec: 77917.43\n",
      "Step  960 | loss: 4.743432 | lr: 5.7219e-04 | norm: 0.4441 | dt: 6721.26ms | tok/sec: 78004.38\n",
      "Step  970 | loss: 4.697965 | lr: 5.7155e-04 | norm: 0.3326 | dt: 6721.98ms | tok/sec: 77996.12\n",
      "Step  980 | loss: 4.842749 | lr: 5.7091e-04 | norm: 0.4306 | dt: 6724.87ms | tok/sec: 77962.51\n",
      "Step  990 | loss: 4.761307 | lr: 5.7025e-04 | norm: 0.3731 | dt: 6723.41ms | tok/sec: 77979.48\n",
      "Validation loss: 4.7443\n",
      "sample 0: Hello, I'm a language model, and I hope's not all youâve found on my work. But I can look out the whole thing and\n",
      "sample 1: Hello, I'm a language model, this simple:\n",
      "1. What: What's -A, is it, and is. It will help you learn\n",
      "sample 2: Hello, I'm a language model, but I will show that many people in the class is the second in the world. I have to worry about how my\n",
      "sample 3: Hello, I'm a language model, you use it.\n",
      "What's the Difference on the topic?\n",
      "A new student, with the use of an experiment\n",
      "Step 1000 | loss: 4.710921 | lr: 5.6960e-04 | norm: 0.3907 | dt: 6716.56ms | tok/sec: 78058.95\n",
      "Step 1010 | loss: 4.618645 | lr: 5.6893e-04 | norm: 0.5001 | dt: 6715.60ms | tok/sec: 78070.21\n",
      "Step 1020 | loss: 4.835210 | lr: 5.6826e-04 | norm: 0.3903 | dt: 6721.24ms | tok/sec: 78004.67\n",
      "Step 1030 | loss: 4.767427 | lr: 5.6758e-04 | norm: 0.3688 | dt: 6716.72ms | tok/sec: 78057.14\n",
      "Step 1040 | loss: 4.696751 | lr: 5.6689e-04 | norm: 0.3776 | dt: 6719.28ms | tok/sec: 78027.44\n",
      "Step 1050 | loss: 4.619686 | lr: 5.6620e-04 | norm: 0.3446 | dt: 6719.86ms | tok/sec: 78020.68\n",
      "Step 1060 | loss: 4.608050 | lr: 5.6550e-04 | norm: 0.4881 | dt: 6716.58ms | tok/sec: 78058.76\n",
      "Step 1070 | loss: 4.745737 | lr: 5.6479e-04 | norm: 0.4808 | dt: 6717.68ms | tok/sec: 78045.97\n",
      "Step 1080 | loss: 4.696422 | lr: 5.6408e-04 | norm: 0.4035 | dt: 6716.95ms | tok/sec: 78054.42\n",
      "Step 1090 | loss: 4.677544 | lr: 5.6336e-04 | norm: 0.3675 | dt: 6722.33ms | tok/sec: 77991.99\n",
      "Validation loss: 4.6053\n",
      "sample 0: Hello, I'm a language model, and I also have to find out which links do so. I know them to be using word phrases and use them,\n",
      "sample 1: Hello, I'm a language model, with more learners, and your children enjoy reading and getting better results from writing to your kids. You know what you are\n",
      "sample 2: Hello, I'm a language model, but I write the worksheet like it's simple, but not. So, I know that the spelling of a single\n",
      "sample 3: Hello, I'm a language model, I got a lot of interesting and entertaining books over the years. There are lots of interesting lessons, from the time she\n",
      "Step 1100 | loss: 4.482861 | lr: 5.6263e-04 | norm: 0.3360 | dt: 6716.00ms | tok/sec: 78065.48\n",
      "Step 1110 | loss: 4.557768 | lr: 5.6190e-04 | norm: 0.5235 | dt: 6722.01ms | tok/sec: 77995.75\n",
      "Step 1120 | loss: 4.663823 | lr: 5.6116e-04 | norm: 0.3774 | dt: 6717.56ms | tok/sec: 78047.43\n",
      "Step 1130 | loss: 4.666461 | lr: 5.6041e-04 | norm: 0.3931 | dt: 6716.39ms | tok/sec: 78061.00\n",
      "Step 1140 | loss: 4.615852 | lr: 5.5966e-04 | norm: 0.4181 | dt: 6719.88ms | tok/sec: 78020.49\n",
      "Step 1150 | loss: 4.375802 | lr: 5.5890e-04 | norm: 0.4683 | dt: 6713.60ms | tok/sec: 78093.36\n",
      "Step 1160 | loss: 4.605561 | lr: 5.5814e-04 | norm: 0.3799 | dt: 6713.92ms | tok/sec: 78089.69\n",
      "Step 1170 | loss: 4.583200 | lr: 5.5736e-04 | norm: 0.3861 | dt: 6715.97ms | tok/sec: 78065.88\n",
      "Step 1180 | loss: 4.476893 | lr: 5.5659e-04 | norm: 0.2979 | dt: 6713.41ms | tok/sec: 78095.64\n",
      "Step 1190 | loss: 4.484031 | lr: 5.5580e-04 | norm: 0.4115 | dt: 6718.19ms | tok/sec: 78040.09\n",
      "Validation loss: 4.4815\n",
      "sample 0: Hello, I'm a language model, but I was trying it out! You haven't been using a problem until now. And how would it have it become\n",
      "sample 1: Hello, I'm a language model, I guess, I think, but most of that and here, and yet it's just about that.)\n",
      "Let's\n",
      "sample 2: Hello, I'm a language model, and I havenât read a book like this:â says I mean.\n",
      "âI think I\n",
      "sample 3: Hello, I'm a language model, so much of my life is a personal and there's a lot of a little good to you, if you've learned\n",
      "Step 1200 | loss: 4.356196 | lr: 5.5501e-04 | norm: 0.4650 | dt: 6713.72ms | tok/sec: 78092.06\n",
      "Step 1210 | loss: 4.554304 | lr: 5.5421e-04 | norm: 0.5462 | dt: 6719.13ms | tok/sec: 78029.19\n",
      "Step 1220 | loss: 4.491189 | lr: 5.5341e-04 | norm: 0.3211 | dt: 6715.93ms | tok/sec: 78066.32\n",
      "Step 1230 | loss: 4.368758 | lr: 5.5260e-04 | norm: 0.3661 | dt: 6718.50ms | tok/sec: 78036.45\n",
      "Step 1240 | loss: 4.369633 | lr: 5.5178e-04 | norm: 0.5330 | dt: 6718.67ms | tok/sec: 78034.45\n",
      "Step 1250 | loss: 4.281031 | lr: 5.5096e-04 | norm: 0.3207 | dt: 6717.46ms | tok/sec: 78048.58\n",
      "Step 1260 | loss: 4.512170 | lr: 5.5013e-04 | norm: 0.5502 | dt: 6721.21ms | tok/sec: 78004.98\n",
      "Step 1270 | loss: 4.379743 | lr: 5.4929e-04 | norm: 0.3609 | dt: 6717.14ms | tok/sec: 78052.32\n",
      "Step 1280 | loss: 4.333202 | lr: 5.4845e-04 | norm: 0.2959 | dt: 6719.87ms | tok/sec: 78020.55\n",
      "Step 1290 | loss: 4.247358 | lr: 5.4760e-04 | norm: 0.4671 | dt: 6714.60ms | tok/sec: 78081.77\n",
      "Validation loss: 4.3395\n",
      "sample 0: Hello, I'm a language model, and I just try it all in order to figure out how to say my class. So, thatâs not\n",
      "sample 1: Hello, I'm a language model, but when I started a project that comes with my community's social learning from an online community, it just makes me wonder\n",
      "sample 2: Hello, I'm a language model, but I thought the above shows them the same at all. This could be a little more complicated than the other language classes\n",
      "sample 3: Hello, I'm a language model, or we can say that when you take it correctly, you can just go through this task without playing!\n",
      "The goal\n",
      "Step 1300 | loss: 4.272845 | lr: 5.4675e-04 | norm: 0.3836 | dt: 6721.29ms | tok/sec: 78004.07\n",
      "Step 1310 | loss: 4.445542 | lr: 5.4589e-04 | norm: 0.4321 | dt: 6713.37ms | tok/sec: 78096.13\n",
      "Step 1320 | loss: 4.355260 | lr: 5.4502e-04 | norm: 0.3707 | dt: 6722.34ms | tok/sec: 77991.94\n",
      "Step 1330 | loss: 4.286616 | lr: 5.4415e-04 | norm: 0.3583 | dt: 6722.08ms | tok/sec: 77994.87\n",
      "Step 1340 | loss: 4.236525 | lr: 5.4327e-04 | norm: 0.4768 | dt: 6721.20ms | tok/sec: 78005.06\n",
      "Step 1350 | loss: 4.374050 | lr: 5.4239e-04 | norm: 0.4825 | dt: 6716.47ms | tok/sec: 78060.09\n",
      "Step 1360 | loss: 4.341254 | lr: 5.4150e-04 | norm: 0.4076 | dt: 6714.08ms | tok/sec: 78087.90\n",
      "Step 1370 | loss: 4.235325 | lr: 5.4060e-04 | norm: 0.3290 | dt: 6723.44ms | tok/sec: 77979.17\n",
      "Step 1380 | loss: 4.236091 | lr: 5.3970e-04 | norm: 0.3189 | dt: 6711.31ms | tok/sec: 78120.10\n",
      "Step 1390 | loss: 4.108567 | lr: 5.3879e-04 | norm: 0.4998 | dt: 6717.94ms | tok/sec: 78043.03\n",
      "Validation loss: 4.2423\n",
      "sample 0: Hello, I'm a language model, and I am a student to a learnerâs (we really don't), then that is really a lot\n",
      "sample 1: Hello, I'm a language model, and not much of what I want all of, at least of what I need. It's not all that I'm\n",
      "sample 2: Hello, I'm a language model, but I did my maths on a graph. Now I'm using my computer, the same way I haven't used.\"\n",
      "sample 3: Hello, I'm a language model, but for me, I just wanted to have me. I wouldn't think of some. There was more to me!\n",
      "Step 1400 | loss: 4.375805 | lr: 5.3788e-04 | norm: 0.3817 | dt: 6718.55ms | tok/sec: 78035.87\n",
      "Step 1410 | loss: 4.263617 | lr: 5.3696e-04 | norm: 0.3104 | dt: 6710.59ms | tok/sec: 78128.43\n",
      "Step 1420 | loss: 4.171401 | lr: 5.3603e-04 | norm: 0.3450 | dt: 6721.02ms | tok/sec: 78007.22\n",
      "Step 1430 | loss: 4.133279 | lr: 5.3510e-04 | norm: 0.3947 | dt: 6714.43ms | tok/sec: 78083.75\n",
      "Step 1440 | loss: 3.998137 | lr: 5.3416e-04 | norm: 0.3650 | dt: 6709.69ms | tok/sec: 78138.96\n",
      "Step 1450 | loss: 4.263148 | lr: 5.3322e-04 | norm: 0.5302 | dt: 6710.72ms | tok/sec: 78126.93\n",
      "Step 1460 | loss: 4.221072 | lr: 5.3227e-04 | norm: 0.3242 | dt: 6712.22ms | tok/sec: 78109.51\n",
      "Step 1470 | loss: 4.152783 | lr: 5.3131e-04 | norm: 0.3226 | dt: 6712.81ms | tok/sec: 78102.62\n",
      "Step 1480 | loss: 4.059402 | lr: 5.3035e-04 | norm: 0.3338 | dt: 6712.20ms | tok/sec: 78109.70\n",
      "Step 1490 | loss: 4.213639 | lr: 5.2938e-04 | norm: 0.4311 | dt: 6725.90ms | tok/sec: 77950.64\n",
      "Validation loss: 4.1543\n",
      "sample 0: Hello, I'm a language model, and I can't say all those who wish to see, I need to make a big difference if they can't be\n",
      "sample 1: Hello, I'm a language model, but now I'm going to know at the top we will see some 'pink', you can now know that you\n",
      "sample 2: Hello, I'm a language model, but I love this kind of concept of the future.\n",
      "It turns out that if you have a bit of the wrong\n",
      "sample 3: Hello, I'm a language model, so don't you think the same thing, such as the old age class? Because, like what makes a big thing\n",
      "Step 1500 | loss: 4.269967 | lr: 5.2841e-04 | norm: 0.3673 | dt: 6720.55ms | tok/sec: 78012.62\n",
      "Step 1510 | loss: 4.148214 | lr: 5.2743e-04 | norm: 0.3393 | dt: 6725.10ms | tok/sec: 77959.92\n",
      "Step 1520 | loss: 4.108917 | lr: 5.2645e-04 | norm: 0.3336 | dt: 6719.11ms | tok/sec: 78029.41\n",
      "Step 1530 | loss: 3.985833 | lr: 5.2546e-04 | norm: 0.3503 | dt: 6717.53ms | tok/sec: 78047.73\n",
      "Step 1540 | loss: 4.181087 | lr: 5.2447e-04 | norm: 0.5106 | dt: 6711.98ms | tok/sec: 78112.29\n",
      "Step 1550 | loss: 4.150030 | lr: 5.2347e-04 | norm: 0.3161 | dt: 6721.65ms | tok/sec: 77999.91\n",
      "Step 1560 | loss: 4.099869 | lr: 5.2246e-04 | norm: 0.3360 | dt: 6710.02ms | tok/sec: 78135.11\n",
      "Step 1570 | loss: 3.963154 | lr: 5.2145e-04 | norm: 0.2994 | dt: 6711.89ms | tok/sec: 78113.35\n",
      "Step 1580 | loss: 3.995152 | lr: 5.2044e-04 | norm: 0.3206 | dt: 6711.16ms | tok/sec: 78121.85\n",
      "Step 1590 | loss: 4.155923 | lr: 5.1941e-04 | norm: 0.3676 | dt: 6706.65ms | tok/sec: 78174.37\n",
      "Validation loss: 4.0798\n",
      "sample 0: Hello, I'm a language model, but I know that it's important to teach, the task is all our work. We only learn from each other.\n",
      "sample 1: Hello, I'm a language model, and when I'm not sure what we're doing! For the rest of time I have to go home with the opportunity\n",
      "sample 2: Hello, I'm a language model, and I also do very interesting things, but my role is itâs important.\n",
      "Iâve spent\n",
      "sample 3: Hello, I'm a language model, it takes a lot of time and time, while I'm now teaching. It shows a new course as well, though\n",
      "Step 1600 | loss: 4.205638 | lr: 5.1839e-04 | norm: 0.3788 | dt: 6701.57ms | tok/sec: 78233.58\n",
      "Step 1610 | loss: 4.067874 | lr: 5.1735e-04 | norm: 0.3081 | dt: 6709.00ms | tok/sec: 78146.94\n",
      "Step 1620 | loss: 3.999053 | lr: 5.1632e-04 | norm: 0.3141 | dt: 6714.09ms | tok/sec: 78087.77\n",
      "Step 1630 | loss: 4.147182 | lr: 5.1527e-04 | norm: 0.3971 | dt: 6707.81ms | tok/sec: 78160.85\n",
      "Step 1640 | loss: 4.039673 | lr: 5.1423e-04 | norm: 0.2929 | dt: 6707.90ms | tok/sec: 78159.74\n",
      "Step 1650 | loss: 4.087552 | lr: 5.1317e-04 | norm: 0.3789 | dt: 6711.15ms | tok/sec: 78121.92\n",
      "Step 1660 | loss: 4.106343 | lr: 5.1211e-04 | norm: 0.3367 | dt: 6706.08ms | tok/sec: 78180.96\n",
      "Step 1670 | loss: 3.957895 | lr: 5.1105e-04 | norm: 0.3110 | dt: 6708.29ms | tok/sec: 78155.25\n",
      "Step 1680 | loss: 4.092268 | lr: 5.0998e-04 | norm: 0.3086 | dt: 6727.27ms | tok/sec: 77934.74\n",
      "Step 1690 | loss: 4.067338 | lr: 5.0891e-04 | norm: 0.4219 | dt: 6721.98ms | tok/sec: 77996.07\n",
      "Validation loss: 4.0251\n",
      "sample 0: Hello, I'm a language model, and I will be on an exam at ILLI with an assessment using two different strategies. We'll go back in\n",
      "sample 1: Hello, I'm a language model, but in my opinion, I've listed the whole sequence so that if I get the correct answer to me, I'm\n",
      "sample 2: Hello, I'm a language model, but I'd love to find a solution that helps me understand more complex patterns of data that I've learned. I'm\n",
      "sample 3: Hello, I'm a language model, it does not seem to be the same as taking a bit of an active, happy world with a much more personal learning\n",
      "Step 1700 | loss: 3.972537 | lr: 5.0783e-04 | norm: 0.3108 | dt: 6723.90ms | tok/sec: 77973.79\n",
      "Step 1710 | loss: 3.954516 | lr: 5.0674e-04 | norm: 0.3667 | dt: 6711.99ms | tok/sec: 78112.17\n",
      "Step 1720 | loss: 3.928042 | lr: 5.0565e-04 | norm: 0.2658 | dt: 6706.44ms | tok/sec: 78176.77\n",
      "Step 1730 | loss: 4.070118 | lr: 5.0456e-04 | norm: 0.3667 | dt: 6709.09ms | tok/sec: 78145.93\n",
      "Step 1740 | loss: 4.017544 | lr: 5.0346e-04 | norm: 0.3358 | dt: 6721.92ms | tok/sec: 77996.82\n",
      "Step 1750 | loss: 3.998060 | lr: 5.0236e-04 | norm: 0.3464 | dt: 6723.09ms | tok/sec: 77983.20\n",
      "Step 1760 | loss: 3.864556 | lr: 5.0125e-04 | norm: 0.3922 | dt: 6706.23ms | tok/sec: 78179.29\n",
      "Step 1770 | loss: 3.992989 | lr: 5.0013e-04 | norm: 0.3052 | dt: 6707.06ms | tok/sec: 78169.57\n",
      "Step 1780 | loss: 4.005996 | lr: 4.9902e-04 | norm: 0.4415 | dt: 6712.09ms | tok/sec: 78111.03\n",
      "Step 1790 | loss: 3.986118 | lr: 4.9789e-04 | norm: 0.3031 | dt: 6707.04ms | tok/sec: 78169.76\n",
      "Validation loss: 3.9672\n",
      "sample 0: Hello, I'm a language model, and I just want it up in our area is really one of the only way to find out. It was a lot\n",
      "sample 1: Hello, I'm a language model, but with a few exceptions, the main difference in most numbers between is whether I am going to have another choice.\n",
      "\n",
      "sample 2: Hello, I'm a language model, but I feel like doing research stuff on the brain, and there have been a lot of research in the past years,\n",
      "sample 3: Hello, I'm a language model, that seems to be a way of improving your mood.\n",
      "However, this isn't about taking the class in your next\n",
      "Step 1800 | loss: 3.927480 | lr: 4.9676e-04 | norm: 0.2896 | dt: 6704.37ms | tok/sec: 78200.97\n",
      "Step 1810 | loss: 3.857785 | lr: 4.9563e-04 | norm: 0.3432 | dt: 6709.77ms | tok/sec: 78137.94\n",
      "Step 1820 | loss: 4.057292 | lr: 4.9449e-04 | norm: 0.3586 | dt: 6712.47ms | tok/sec: 78106.59\n",
      "Step 1830 | loss: 3.986546 | lr: 4.9335e-04 | norm: 0.2828 | dt: 6711.64ms | tok/sec: 78116.28\n",
      "Step 1840 | loss: 3.921641 | lr: 4.9220e-04 | norm: 0.3323 | dt: 6710.24ms | tok/sec: 78132.56\n",
      "Step 1850 | loss: 3.857717 | lr: 4.9105e-04 | norm: 0.2808 | dt: 6713.14ms | tok/sec: 78098.73\n",
      "Step 1860 | loss: 3.964254 | lr: 4.8990e-04 | norm: 0.3220 | dt: 6710.61ms | tok/sec: 78128.21\n",
      "Step 1870 | loss: 4.030055 | lr: 4.8874e-04 | norm: 0.4148 | dt: 6711.24ms | tok/sec: 78120.91\n",
      "Step 1880 | loss: 3.963015 | lr: 4.8757e-04 | norm: 0.2771 | dt: 6703.34ms | tok/sec: 78213.01\n",
      "Step 1890 | loss: 3.896142 | lr: 4.8640e-04 | norm: 0.3368 | dt: 6703.63ms | tok/sec: 78209.55\n",
      "Validation loss: 3.9406\n",
      "sample 0: Hello, I'm a language model, and I just want a different set of these four approaches. I hope you are able to reach them, but I don\n",
      "sample 1: Hello, I'm a language model, but can't be the same with both. It goes along with what follows is that there is no reason to consider it\n",
      "sample 2: Hello, I'm a language model, but I did have a really easy job with an ESL teacher at a local school.\n",
      "There are a few ways that\n",
      "sample 3: Hello, I'm a language model, that could be an example of how one could imagine it would be as well, rather than what I want to say â\n",
      "Step 1900 | loss: 3.824209 | lr: 4.8523e-04 | norm: 0.3280 | dt: 6701.56ms | tok/sec: 78233.75\n",
      "Step 1910 | loss: 4.005099 | lr: 4.8405e-04 | norm: 0.2624 | dt: 6705.80ms | tok/sec: 78184.31\n",
      "Step 1920 | loss: 3.986984 | lr: 4.8287e-04 | norm: 0.3341 | dt: 6702.35ms | tok/sec: 78224.52\n",
      "Step 1930 | loss: 3.891156 | lr: 4.8168e-04 | norm: 0.2599 | dt: 6708.81ms | tok/sec: 78149.13\n",
      "Step 1940 | loss: 3.907041 | lr: 4.8049e-04 | norm: 0.2801 | dt: 6709.18ms | tok/sec: 78144.87\n",
      "Step 1950 | loss: 3.805408 | lr: 4.7929e-04 | norm: 0.3237 | dt: 6716.59ms | tok/sec: 78058.64\n",
      "Step 1960 | loss: 3.929857 | lr: 4.7809e-04 | norm: 0.3038 | dt: 6726.28ms | tok/sec: 77946.17\n",
      "Step 1970 | loss: 4.018298 | lr: 4.7689e-04 | norm: 0.2928 | dt: 6709.57ms | tok/sec: 78140.35\n",
      "Step 1980 | loss: 3.892967 | lr: 4.7568e-04 | norm: 0.3441 | dt: 6706.01ms | tok/sec: 78181.85\n",
      "Step 1990 | loss: 3.876586 | lr: 4.7447e-04 | norm: 0.2640 | dt: 6705.37ms | tok/sec: 78189.24\n",
      "Validation loss: 3.8924\n",
      "sample 0: Hello, I'm a language model, and I really like the rest of the code with the toolbox of the language. And while this will give you an\n",
      "sample 1: Hello, I'm a language model, but don't know that I have many of my language groups, especially during times of conflict. I had the task of\n",
      "sample 2: Hello, I'm a language model, but I didn't know this; the language did not have this problem.\n",
      "And finally, I'm going to take\n",
      "sample 3: Hello, I'm a language model, you get a lot of ideas and experiences that give you a certain quality of the materials in it, too. And how\n",
      "Step 2000 | loss: 3.980561 | lr: 4.7325e-04 | norm: 0.3426 | dt: 6702.34ms | tok/sec: 78224.66\n",
      "Step 2010 | loss: 3.904305 | lr: 4.7203e-04 | norm: 0.2788 | dt: 6717.44ms | tok/sec: 78048.75\n",
      "Step 2020 | loss: 3.873458 | lr: 4.7081e-04 | norm: 0.2852 | dt: 6705.39ms | tok/sec: 78189.00\n",
      "Step 2030 | loss: 3.873695 | lr: 4.6958e-04 | norm: 0.3370 | dt: 6707.12ms | tok/sec: 78168.86\n",
      "Step 2040 | loss: 3.766736 | lr: 4.6835e-04 | norm: 0.2714 | dt: 6711.00ms | tok/sec: 78123.65\n",
      "Step 2050 | loss: 3.896893 | lr: 4.6711e-04 | norm: 0.4638 | dt: 6705.03ms | tok/sec: 78193.23\n",
      "Step 2060 | loss: 3.931736 | lr: 4.6587e-04 | norm: 0.3217 | dt: 6706.06ms | tok/sec: 78181.26\n",
      "Step 2070 | loss: 3.848897 | lr: 4.6463e-04 | norm: 0.3005 | dt: 6713.89ms | tok/sec: 78090.05\n",
      "Step 2080 | loss: 3.854773 | lr: 4.6338e-04 | norm: 0.2690 | dt: 6719.43ms | tok/sec: 78025.71\n",
      "Step 2090 | loss: 3.786108 | lr: 4.6213e-04 | norm: 0.2791 | dt: 6723.92ms | tok/sec: 77973.53\n",
      "Validation loss: 3.8613\n",
      "sample 0: Hello, I'm a language model, and I will be using these models to provide these models that we do not yet know, rather the ones we do,\n",
      "sample 1: Hello, I'm a language model, but a nice way to keep the house clean, well drained. The real estate is a \"living life\" and \"\n",
      "sample 2: Hello, I'm a language model, but Iâm here to keep my time off the street with my friends. Iâm here for you\n",
      "sample 3: Hello, I'm a language model, and she's also a model for developing the syntax for the data center. The most interesting question is whether it's better\n",
      "Step 2100 | loss: 4.037033 | lr: 4.6087e-04 | norm: 0.3852 | dt: 6709.69ms | tok/sec: 78138.93\n",
      "Step 2110 | loss: 3.882665 | lr: 4.5961e-04 | norm: 0.2685 | dt: 6709.87ms | tok/sec: 78136.82\n",
      "Step 2120 | loss: 3.844101 | lr: 4.5835e-04 | norm: 0.2557 | dt: 6708.63ms | tok/sec: 78151.28\n",
      "Step 2130 | loss: 3.747902 | lr: 4.5708e-04 | norm: 0.3619 | dt: 6711.75ms | tok/sec: 78114.90\n",
      "Step 2140 | loss: 3.941701 | lr: 4.5581e-04 | norm: 0.3727 | dt: 6714.17ms | tok/sec: 78086.84\n",
      "Step 2150 | loss: 3.990986 | lr: 4.5454e-04 | norm: 0.5274 | dt: 6712.31ms | tok/sec: 78108.39\n",
      "Step 2160 | loss: 3.917442 | lr: 4.5326e-04 | norm: 0.2616 | dt: 6710.39ms | tok/sec: 78130.79\n",
      "Step 2170 | loss: 3.801756 | lr: 4.5198e-04 | norm: 0.2166 | dt: 6710.28ms | tok/sec: 78132.07\n",
      "Step 2180 | loss: 3.667463 | lr: 4.5070e-04 | norm: 0.2906 | dt: 6708.44ms | tok/sec: 78153.46\n",
      "Step 2190 | loss: 3.896672 | lr: 4.4941e-04 | norm: 0.3463 | dt: 6702.37ms | tok/sec: 78224.28\n",
      "Validation loss: 3.8287\n",
      "sample 0: Hello, I'm a language model, and I was a very big hit! How good is that? Let's take a few basic and interesting examples of it\n",
      "sample 1: Hello, I'm a language model, I really really wanted to be a really good listener - they are really there to help. They're like that. And\n",
      "sample 2: Hello, I'm a language model, I'm quite certain about all the aspects of English language. In addition, I'm not a language model, but only\n",
      "sample 3: Hello, I'm a language model, it's a good idea to be creative and your best friend. Let's take our tips from a dictionary. We love\n",
      "Step 2200 | loss: 3.901664 | lr: 4.4812e-04 | norm: 0.2642 | dt: 6700.89ms | tok/sec: 78241.54\n",
      "Step 2210 | loss: 3.774314 | lr: 4.4682e-04 | norm: 0.2361 | dt: 6712.52ms | tok/sec: 78106.02\n",
      "Step 2220 | loss: 3.758501 | lr: 4.4553e-04 | norm: 0.2633 | dt: 6714.63ms | tok/sec: 78081.43\n",
      "Step 2230 | loss: 3.667964 | lr: 4.4423e-04 | norm: 0.3112 | dt: 6704.36ms | tok/sec: 78201.02\n",
      "Step 2240 | loss: 3.915277 | lr: 4.4292e-04 | norm: 0.2771 | dt: 6704.23ms | tok/sec: 78202.58\n",
      "Step 2250 | loss: 3.843072 | lr: 4.4161e-04 | norm: 0.2548 | dt: 6701.08ms | tok/sec: 78239.34\n",
      "Step 2260 | loss: 3.815806 | lr: 4.4030e-04 | norm: 0.3927 | dt: 6707.29ms | tok/sec: 78166.89\n",
      "Step 2270 | loss: 3.613233 | lr: 4.3899e-04 | norm: 0.2474 | dt: 6703.66ms | tok/sec: 78209.22\n",
      "Step 2280 | loss: 3.872802 | lr: 4.3767e-04 | norm: 0.3181 | dt: 6717.50ms | tok/sec: 78048.09\n",
      "Step 2290 | loss: 3.955838 | lr: 4.3635e-04 | norm: 0.2759 | dt: 6715.64ms | tok/sec: 78069.74\n",
      "Validation loss: 3.7871\n",
      "sample 0: Hello, I'm a language model, and I know that it looks really bad? My point is that even we do not really understand language and grammar. And\n",
      "sample 1: Hello, I'm a language model, but they're so good. And on the way here the results will see a difference in the results?\n",
      "You can\n",
      "sample 2: Hello, I'm a language model, but I guess it's what I do. That's what happens, so I just don't know what I mean,\n",
      "sample 3: Hello, I'm a language model, so how do I know that? That is part of the concept that's the idea you know. How do we handle\n",
      "Step 2300 | loss: 3.837919 | lr: 4.3503e-04 | norm: 0.2618 | dt: 6699.98ms | tok/sec: 78252.13\n",
      "Step 2310 | loss: 3.888576 | lr: 4.3371e-04 | norm: 0.2816 | dt: 6699.08ms | tok/sec: 78262.65\n",
      "Step 2320 | loss: 3.629511 | lr: 4.3238e-04 | norm: 0.2938 | dt: 6702.91ms | tok/sec: 78218.02\n",
      "Step 2330 | loss: 3.856682 | lr: 4.3105e-04 | norm: 0.2958 | dt: 6703.85ms | tok/sec: 78207.01\n",
      "Step 2340 | loss: 3.902990 | lr: 4.2971e-04 | norm: 0.3029 | dt: 6708.29ms | tok/sec: 78155.23\n",
      "Step 2350 | loss: 3.801858 | lr: 4.2837e-04 | norm: 0.2373 | dt: 6703.13ms | tok/sec: 78215.43\n",
      "Step 2360 | loss: 3.602872 | lr: 4.2703e-04 | norm: 0.3202 | dt: 6706.62ms | tok/sec: 78174.74\n",
      "Step 2370 | loss: 3.694292 | lr: 4.2569e-04 | norm: 0.2817 | dt: 6706.28ms | tok/sec: 78178.63\n",
      "Step 2380 | loss: 3.901306 | lr: 4.2435e-04 | norm: 0.3899 | dt: 6709.52ms | tok/sec: 78140.96\n",
      "Step 2390 | loss: 3.794693 | lr: 4.2300e-04 | norm: 0.2314 | dt: 6708.73ms | tok/sec: 78150.06\n",
      "Validation loss: 3.7653\n",
      "sample 0: Hello, I'm a language model, and I really like it.â\n",
      "If you decide that an approach for the âeclipse projectâ\n",
      "sample 1: Hello, I'm a language model, I'll call it, I'm about to be specific if I don't understand how the words in one voice are connected\n",
      "sample 2: Hello, I'm a language model, I'm on my personal website. If you think about it please click here.\n",
      "We're going to be a website\n",
      "sample 3: Hello, I'm a language model, it turns out there is a way I think every day.\n",
      "There are so many benefits of having access to people at\n",
      "Step 2400 | loss: 3.707396 | lr: 4.2165e-04 | norm: 0.2835 | dt: 6713.02ms | tok/sec: 78100.22\n",
      "Step 2410 | loss: 3.588662 | lr: 4.2029e-04 | norm: 0.2555 | dt: 6710.61ms | tok/sec: 78128.19\n",
      "Step 2420 | loss: 3.813970 | lr: 4.1894e-04 | norm: 0.3118 | dt: 6708.48ms | tok/sec: 78153.04\n",
      "Step 2430 | loss: 3.823472 | lr: 4.1758e-04 | norm: 0.2685 | dt: 6708.32ms | tok/sec: 78154.91\n",
      "Step 2440 | loss: 3.807203 | lr: 4.1622e-04 | norm: 0.2901 | dt: 6709.09ms | tok/sec: 78145.94\n",
      "Step 2450 | loss: 3.798066 | lr: 4.1486e-04 | norm: 0.2876 | dt: 6708.86ms | tok/sec: 78148.65\n",
      "Step 2460 | loss: 3.571304 | lr: 4.1349e-04 | norm: 0.3054 | dt: 6713.20ms | tok/sec: 78098.06\n",
      "Step 2470 | loss: 3.769383 | lr: 4.1212e-04 | norm: 0.2977 | dt: 6713.63ms | tok/sec: 78093.12\n",
      "Step 2480 | loss: 3.772290 | lr: 4.1075e-04 | norm: 0.3150 | dt: 6701.88ms | tok/sec: 78230.00\n",
      "Step 2490 | loss: 3.752957 | lr: 4.0938e-04 | norm: 0.2625 | dt: 6706.09ms | tok/sec: 78180.84\n",
      "Validation loss: 3.7410\n",
      "sample 0: Hello, I'm a language model, and I would like it better for some types of things. I was still working on that sort of language in the last\n",
      "sample 1: Hello, I'm a language model, so there's a way to use this to learn these features, just maybe we'll do something about using them in this\n",
      "sample 2: Hello, I'm a language model, I'm quite thinking about learning my family's skills, and the skills it's trying to learn.\n",
      "I've taken\n",
      "sample 3: Hello, I'm a language model, you think it's a tool for all kinds of people.\n",
      "There are lots of other games out there, so all\n",
      "Step 2500 | loss: 3.540446 | lr: 4.0800e-04 | norm: 0.2792 | dt: 6695.90ms | tok/sec: 78299.89\n",
      "Step 2510 | loss: 3.568723 | lr: 4.0663e-04 | norm: 0.2719 | dt: 6710.73ms | tok/sec: 78126.84\n",
      "Step 2520 | loss: 3.791422 | lr: 4.0525e-04 | norm: 0.3033 | dt: 6714.29ms | tok/sec: 78085.36\n",
      "Step 2530 | loss: 3.774578 | lr: 4.0386e-04 | norm: 0.2811 | dt: 6717.73ms | tok/sec: 78045.42\n",
      "Step 2540 | loss: 3.705592 | lr: 4.0248e-04 | norm: 0.2598 | dt: 6721.00ms | tok/sec: 78007.45\n",
      "Step 2550 | loss: 3.577302 | lr: 4.0110e-04 | norm: 0.3052 | dt: 6712.45ms | tok/sec: 78106.81\n",
      "Step 2560 | loss: 3.822455 | lr: 3.9971e-04 | norm: 0.2804 | dt: 6711.52ms | tok/sec: 78117.66\n",
      "Step 2570 | loss: 3.809612 | lr: 3.9832e-04 | norm: 0.2904 | dt: 6711.46ms | tok/sec: 78118.35\n",
      "Step 2580 | loss: 3.777179 | lr: 3.9693e-04 | norm: 0.2332 | dt: 6711.33ms | tok/sec: 78119.88\n",
      "Step 2590 | loss: 3.702121 | lr: 3.9553e-04 | norm: 0.2246 | dt: 6712.13ms | tok/sec: 78110.51\n",
      "Validation loss: 3.7148\n",
      "sample 0: Hello, I'm a language model, and I know that it's so different I wouldn't. I also saw some of it. So it just got so\n",
      "sample 1: Hello, I'm a language model, so don't worry about it, anyway. It actually creates the language through which it can perform its runtime.\n",
      "-\n",
      "sample 2: Hello, I'm a language model, but I didn't learn something when my first book was published at my time.\n",
      "If you're interested in reading how\n",
      "sample 3: Hello, I'm a language model, it comes with a lot of other frameworks that define the way we teach students, teaching them a lot, and not letting\n",
      "Step 2600 | loss: 3.576136 | lr: 3.9414e-04 | norm: 0.3002 | dt: 6707.75ms | tok/sec: 78161.55\n",
      "Step 2610 | loss: 3.790399 | lr: 3.9274e-04 | norm: 0.2869 | dt: 6718.11ms | tok/sec: 78041.00\n",
      "Step 2620 | loss: 3.728557 | lr: 3.9134e-04 | norm: 0.3046 | dt: 6714.68ms | tok/sec: 78080.81\n",
      "Step 2630 | loss: 3.720603 | lr: 3.8994e-04 | norm: 0.2538 | dt: 6713.06ms | tok/sec: 78099.72\n",
      "Step 2640 | loss: 3.653821 | lr: 3.8854e-04 | norm: 0.2526 | dt: 6711.28ms | tok/sec: 78120.38\n",
      "Step 2650 | loss: 3.589983 | lr: 3.8713e-04 | norm: 0.2459 | dt: 6725.64ms | tok/sec: 77953.59\n",
      "Step 2660 | loss: 3.768307 | lr: 3.8573e-04 | norm: 0.2751 | dt: 6728.09ms | tok/sec: 77925.22\n",
      "Step 2670 | loss: 3.826179 | lr: 3.8432e-04 | norm: 0.2482 | dt: 6883.82ms | tok/sec: 76162.31\n",
      "Step 2680 | loss: 3.729803 | lr: 3.8291e-04 | norm: 0.3123 | dt: 6714.62ms | tok/sec: 78081.58\n",
      "Step 2690 | loss: 3.585998 | lr: 3.8150e-04 | norm: 0.2573 | dt: 6716.71ms | tok/sec: 78057.25\n",
      "Validation loss: 3.6933\n",
      "sample 0: Hello, I'm a language model, and I really like to go on there if I need it, maybe by my own country.â\n",
      "I have\n",
      "sample 1: Hello, I'm a language model, I use Spanish, I'm using something that I teach all the language groups I have in my class so they can be\n",
      "sample 2: Hello, I'm a language model, so I had to teach all of that. You learn all my vocabulary in a little bit of a grammatical form,\n",
      "sample 3: Hello, I'm a language model, and will be able to learn the most important functions of the language.\n",
      "The problem is that it may be impossible than\n",
      "Step 2700 | loss: 3.583580 | lr: 3.8009e-04 | norm: 0.2596 | dt: 6699.13ms | tok/sec: 78262.08\n",
      "Step 2710 | loss: 3.725590 | lr: 3.7868e-04 | norm: 0.2784 | dt: 6703.04ms | tok/sec: 78216.45\n",
      "Step 2720 | loss: 3.783198 | lr: 3.7726e-04 | norm: 0.3325 | dt: 6702.20ms | tok/sec: 78226.27\n",
      "Step 2730 | loss: 3.671478 | lr: 3.7585e-04 | norm: 0.2435 | dt: 6706.73ms | tok/sec: 78173.41\n",
      "Step 2740 | loss: 3.511194 | lr: 3.7443e-04 | norm: 0.2406 | dt: 6714.20ms | tok/sec: 78086.46\n",
      "Step 2750 | loss: 3.736448 | lr: 3.7301e-04 | norm: 0.2740 | dt: 6701.47ms | tok/sec: 78234.76\n",
      "Step 2760 | loss: 3.786646 | lr: 3.7159e-04 | norm: 0.3457 | dt: 6699.75ms | tok/sec: 78254.87\n",
      "Step 2770 | loss: 3.696272 | lr: 3.7017e-04 | norm: 0.2671 | dt: 6696.70ms | tok/sec: 78290.51\n",
      "Step 2780 | loss: 3.710873 | lr: 3.6875e-04 | norm: 0.2578 | dt: 6710.00ms | tok/sec: 78135.31\n",
      "Step 2790 | loss: 3.502129 | lr: 3.6733e-04 | norm: 0.2445 | dt: 6712.31ms | tok/sec: 78108.49\n",
      "Validation loss: 3.6748\n",
      "sample 0: Hello, I'm a language model, and I'd like you to go ahead and use some of those methods together in a few chapters!<|endoftext|>When the US\n",
      "sample 1: Hello, I'm a language model, but a one-time service to those who have, really.<|endoftext|>New Jersey: New Jersey: W.F.\n",
      "sample 2: Hello, I'm a language model, so I use this concept here to show you two ways to work in my classroom. I've been using this model to\n",
      "sample 3: Hello, I'm a language model, and when I first started my class, the following were the sounds I learned. Well, then, she told me why\n",
      "Step 2800 | loss: 3.772303 | lr: 3.6590e-04 | norm: 0.2947 | dt: 6697.60ms | tok/sec: 78280.03\n",
      "Step 2810 | loss: 3.730132 | lr: 3.6448e-04 | norm: 0.2886 | dt: 6695.38ms | tok/sec: 78305.89\n",
      "Step 2820 | loss: 3.696310 | lr: 3.6305e-04 | norm: 0.2224 | dt: 6697.04ms | tok/sec: 78286.51\n",
      "Step 2830 | loss: 3.614824 | lr: 3.6162e-04 | norm: 0.2690 | dt: 6700.33ms | tok/sec: 78248.13\n",
      "Step 2840 | loss: 3.509148 | lr: 3.6019e-04 | norm: 0.2861 | dt: 6700.48ms | tok/sec: 78246.34\n",
      "Step 2850 | loss: 3.755057 | lr: 3.5877e-04 | norm: 0.2804 | dt: 6701.19ms | tok/sec: 78238.08\n",
      "Step 2860 | loss: 3.745370 | lr: 3.5734e-04 | norm: 0.2481 | dt: 6710.74ms | tok/sec: 78126.65\n",
      "Step 2870 | loss: 3.665188 | lr: 3.5591e-04 | norm: 0.2540 | dt: 6709.02ms | tok/sec: 78146.73\n",
      "Step 2880 | loss: 3.573219 | lr: 3.5447e-04 | norm: 0.2605 | dt: 6713.04ms | tok/sec: 78099.99\n",
      "Step 2890 | loss: 3.517022 | lr: 3.5304e-04 | norm: 0.3112 | dt: 6718.52ms | tok/sec: 78036.18\n",
      "Validation loss: 3.6503\n",
      "sample 0: Hello, I'm a language model, and I was so happy to talk about each other.\n",
      "- So... and I have made an effort to make some\n",
      "sample 1: Hello, I'm a language model, but here's a good deal more data. It uses language and a host of other things and is built around the other\n",
      "sample 2: Hello, I'm a language model, but I didn't go away but to try some of the tools and the software. I'm not going to try or\n",
      "sample 3: Hello, I'm a language model, and not a professional. I've read a lot of books on grammar, so hopefully they have some time to write anything\n",
      "Step 2900 | loss: 3.702484 | lr: 3.5161e-04 | norm: 0.2841 | dt: 6718.26ms | tok/sec: 78039.29\n",
      "Step 2910 | loss: 3.711073 | lr: 3.5018e-04 | norm: 0.2273 | dt: 6706.00ms | tok/sec: 78181.91\n",
      "Step 2920 | loss: 3.650241 | lr: 3.4874e-04 | norm: 0.2934 | dt: 6706.17ms | tok/sec: 78179.99\n",
      "Step 2930 | loss: 3.463048 | lr: 3.4731e-04 | norm: 0.2730 | dt: 6722.42ms | tok/sec: 77991.01\n",
      "Step 2940 | loss: 3.742051 | lr: 3.4587e-04 | norm: 0.2531 | dt: 6723.60ms | tok/sec: 77977.25\n",
      "Step 2950 | loss: 3.748296 | lr: 3.4444e-04 | norm: 0.3397 | dt: 6724.26ms | tok/sec: 77969.64\n",
      "Step 2960 | loss: 3.686138 | lr: 3.4300e-04 | norm: 0.2269 | dt: 6716.39ms | tok/sec: 78061.03\n",
      "Step 2970 | loss: 3.633783 | lr: 3.4157e-04 | norm: 0.2510 | dt: 6715.50ms | tok/sec: 78071.34\n",
      "Step 2980 | loss: 3.569292 | lr: 3.4013e-04 | norm: 0.2879 | dt: 6712.77ms | tok/sec: 78103.12\n",
      "Step 2990 | loss: 3.658831 | lr: 3.3869e-04 | norm: 0.2533 | dt: 6703.84ms | tok/sec: 78207.09\n",
      "Validation loss: 3.6307\n",
      "sample 0: Hello, I'm a language model, and I just want the way we've taken you and are able to predict if we have a lot of variables. But\n",
      "sample 1: Hello, I'm a language model, but with a little background in language from the same word to the same word as I am and the first one is to\n",
      "sample 2: Hello, I'm a language model, but I found out that my children didn't always come up with their language. My kids didn't even know that I\n",
      "sample 3: Hello, I'm a language model, you use the term \"measurement\",\n",
      "and you use it on a day and every day...\n",
      "The difference\n",
      "Step 3000 | loss: 3.672446 | lr: 3.3726e-04 | norm: 0.2329 | dt: 6692.18ms | tok/sec: 78343.40\n",
      "Step 3010 | loss: 3.669169 | lr: 3.3582e-04 | norm: 0.3191 | dt: 6701.34ms | tok/sec: 78236.25\n",
      "Step 3020 | loss: 3.554299 | lr: 3.3438e-04 | norm: 0.2493 | dt: 6698.77ms | tok/sec: 78266.34\n",
      "Step 3030 | loss: 3.463331 | lr: 3.3295e-04 | norm: 0.2489 | dt: 6695.69ms | tok/sec: 78302.32\n",
      "Step 3040 | loss: 3.678478 | lr: 3.3151e-04 | norm: 0.3056 | dt: 6701.10ms | tok/sec: 78239.10\n",
      "Step 3050 | loss: 3.734417 | lr: 3.3007e-04 | norm: 0.2713 | dt: 6699.50ms | tok/sec: 78257.81\n",
      "Step 3060 | loss: 3.588099 | lr: 3.2863e-04 | norm: 0.2368 | dt: 6700.94ms | tok/sec: 78240.92\n",
      "Step 3070 | loss: 3.460265 | lr: 3.2720e-04 | norm: 0.2652 | dt: 6702.88ms | tok/sec: 78218.31\n",
      "Step 3080 | loss: 3.713486 | lr: 3.2576e-04 | norm: 0.2815 | dt: 6704.45ms | tok/sec: 78200.05\n",
      "Step 3090 | loss: 3.643716 | lr: 3.2432e-04 | norm: 0.2330 | dt: 6705.84ms | tok/sec: 78183.78\n",
      "Validation loss: 3.6138\n",
      "sample 0: Hello, I'm a language model, and I just want it all together for myself.<|endoftext|>by: Bob Hegner, Pizi T.C\n",
      "sample 1: Hello, I'm a language model, I would like to use it for quite a long time - it really helped me in that I think was really an example\n",
      "sample 2: Hello, I'm a language model, I'm gonna give you what you mean. This is a model for what you say, but I'm not sure because\n",
      "sample 3: Hello, I'm a language model, and as a result, I am trying to achieve it. I was not going to work anymore, because I'm only\n",
      "Step 3100 | loss: 3.720696 | lr: 3.2289e-04 | norm: 0.2653 | dt: 6702.31ms | tok/sec: 78224.94\n",
      "Step 3110 | loss: 3.698958 | lr: 3.2145e-04 | norm: 0.2335 | dt: 6700.65ms | tok/sec: 78244.33\n",
      "Step 3120 | loss: 3.467657 | lr: 3.2001e-04 | norm: 0.2530 | dt: 6705.96ms | tok/sec: 78182.41\n",
      "Step 3130 | loss: 3.598493 | lr: 3.1858e-04 | norm: 0.2750 | dt: 6690.96ms | tok/sec: 78357.66\n",
      "Step 3140 | loss: 3.713373 | lr: 3.1714e-04 | norm: 0.2779 | dt: 6690.26ms | tok/sec: 78365.91\n",
      "Step 3150 | loss: 3.674434 | lr: 3.1570e-04 | norm: 0.2661 | dt: 6695.07ms | tok/sec: 78309.56\n",
      "Step 3160 | loss: 3.608994 | lr: 3.1427e-04 | norm: 0.2141 | dt: 6694.49ms | tok/sec: 78316.35\n",
      "Step 3170 | loss: 3.494492 | lr: 3.1283e-04 | norm: 0.2393 | dt: 6692.98ms | tok/sec: 78334.00\n",
      "Step 3180 | loss: 3.641214 | lr: 3.1140e-04 | norm: 0.3419 | dt: 6696.94ms | tok/sec: 78287.69\n",
      "Step 3190 | loss: 3.573853 | lr: 3.0997e-04 | norm: 0.2924 | dt: 6697.37ms | tok/sec: 78282.66\n",
      "Validation loss: 3.5998\n",
      "sample 0: Hello, I'm a language model, and I want to make sure to show other parts of the world so that the world has a great opportunity to learn new\n",
      "sample 1: Hello, I'm a language model, but they are not.\n",
      "If you are looking for programs, it would be best for you to read on the websites\n",
      "sample 2: Hello, I'm a language model, I'm like thatâ¦ that's how I work with my children in school. You're not a language model. I\n",
      "sample 3: Hello, I'm a language model, which basically means \"I'm going to try with the language model in the language model in the language model. The project\n",
      "Step 3200 | loss: 3.641024 | lr: 3.0853e-04 | norm: 0.2446 | dt: 6705.38ms | tok/sec: 78189.15\n",
      "Step 3210 | loss: 3.463746 | lr: 3.0710e-04 | norm: 0.2535 | dt: 6696.69ms | tok/sec: 78290.67\n",
      "Step 3220 | loss: 3.433309 | lr: 3.0567e-04 | norm: 0.2449 | dt: 6696.43ms | tok/sec: 78293.65\n",
      "Step 3230 | loss: 3.612898 | lr: 3.0424e-04 | norm: 0.2739 | dt: 6697.13ms | tok/sec: 78285.49\n",
      "Step 3240 | loss: 3.682239 | lr: 3.0281e-04 | norm: 0.2339 | dt: 6696.28ms | tok/sec: 78295.42\n",
      "Step 3250 | loss: 3.606112 | lr: 3.0138e-04 | norm: 0.2334 | dt: 6697.90ms | tok/sec: 78276.49\n",
      "Step 3260 | loss: 3.432320 | lr: 2.9995e-04 | norm: 0.2391 | dt: 6698.82ms | tok/sec: 78265.68\n",
      "Step 3270 | loss: 3.599157 | lr: 2.9852e-04 | norm: 0.2398 | dt: 6720.42ms | tok/sec: 78014.21\n",
      "Step 3280 | loss: 3.600950 | lr: 2.9709e-04 | norm: 0.2728 | dt: 6700.51ms | tok/sec: 78246.04\n",
      "Step 3290 | loss: 3.544005 | lr: 2.9567e-04 | norm: 0.2348 | dt: 6708.64ms | tok/sec: 78151.15\n",
      "Validation loss: 3.5837\n",
      "sample 0: Hello, I'm a language model, and I was not a programming engineer to run one.\n",
      "The only change to the game is in programming it's rules\n",
      "sample 1: Hello, I'm a language model, and they're both pretty cool, so I think most of the time. It's not like the people that talked about\n",
      "sample 2: Hello, I'm a language model, I'm writing in some style, but I also make sure, this is a pretty cool, very cool, and you\n",
      "sample 3: Hello, I'm a language model, so for example, I'm going to talk about the language in our house and go out some other resources. So maybe\n",
      "Step 3300 | loss: 3.588282 | lr: 2.9424e-04 | norm: 0.2514 | dt: 6708.12ms | tok/sec: 78157.24\n",
      "Step 3310 | loss: 3.432677 | lr: 2.9282e-04 | norm: 0.2680 | dt: 6707.87ms | tok/sec: 78160.16\n",
      "Step 3320 | loss: 3.623125 | lr: 2.9139e-04 | norm: 0.2554 | dt: 6708.66ms | tok/sec: 78150.93\n",
      "Step 3330 | loss: 3.621492 | lr: 2.8997e-04 | norm: 0.2742 | dt: 6705.73ms | tok/sec: 78185.06\n",
      "Step 3340 | loss: 3.585695 | lr: 2.8855e-04 | norm: 0.2544 | dt: 6701.66ms | tok/sec: 78232.54\n",
      "Step 3350 | loss: 3.410170 | lr: 2.8713e-04 | norm: 0.2437 | dt: 6687.99ms | tok/sec: 78392.47\n",
      "Step 3360 | loss: 3.712076 | lr: 2.8571e-04 | norm: 0.2837 | dt: 6691.34ms | tok/sec: 78353.18\n",
      "Step 3370 | loss: 3.671279 | lr: 2.8430e-04 | norm: 0.2840 | dt: 6692.51ms | tok/sec: 78339.47\n",
      "Step 3380 | loss: 3.699646 | lr: 2.8288e-04 | norm: 0.2642 | dt: 6702.82ms | tok/sec: 78219.06\n",
      "Step 3390 | loss: 3.577235 | lr: 2.8146e-04 | norm: 0.2242 | dt: 6704.43ms | tok/sec: 78200.24\n",
      "Validation loss: 3.5756\n",
      "sample 0: Hello, I'm a language model, and I like to think so many more examples here.\n",
      "So that's my first post.<|endoftext|>A. The name\n",
      "sample 1: Hello, I'm a language model, but now I'm not going to like it anymore â they are so close and so you'll have one.\n",
      "The\n",
      "sample 2: Hello, I'm a language model, I'm happy to play for you as well in my classroom so if you're an ESL teacher, you can help build\n",
      "sample 3: Hello, I'm a language model, and he's been working on the software that creates the program. If you have children working on a group of children it\n",
      "Step 3400 | loss: 3.398116 | lr: 2.8005e-04 | norm: 0.2573 | dt: 6693.72ms | tok/sec: 78325.31\n",
      "Step 3410 | loss: 3.634767 | lr: 2.7864e-04 | norm: 0.2540 | dt: 6712.02ms | tok/sec: 78111.79\n",
      "Step 3420 | loss: 3.679767 | lr: 2.7723e-04 | norm: 0.2940 | dt: 6713.17ms | tok/sec: 78098.47\n",
      "Step 3430 | loss: 3.574246 | lr: 2.7582e-04 | norm: 0.2438 | dt: 6702.29ms | tok/sec: 78225.26\n",
      "Step 3440 | loss: 3.607381 | lr: 2.7441e-04 | norm: 0.2799 | dt: 6713.86ms | tok/sec: 78090.41\n",
      "Step 3450 | loss: 3.356236 | lr: 2.7301e-04 | norm: 0.2443 | dt: 6699.54ms | tok/sec: 78257.29\n",
      "Step 3460 | loss: 3.577219 | lr: 2.7160e-04 | norm: 0.2461 | dt: 6704.15ms | tok/sec: 78203.51\n",
      "Step 3470 | loss: 3.672349 | lr: 2.7020e-04 | norm: 0.2574 | dt: 6709.51ms | tok/sec: 78141.08\n",
      "Step 3480 | loss: 3.572992 | lr: 2.6880e-04 | norm: 0.2307 | dt: 6715.72ms | tok/sec: 78068.73\n",
      "Step 3490 | loss: 3.380353 | lr: 2.6740e-04 | norm: 0.2662 | dt: 6704.32ms | tok/sec: 78201.57\n",
      "Validation loss: 3.5626\n",
      "sample 0: Hello, I'm a language model, and I don't want to do so soon. I'm going to create some nice animations... but, it's still\n",
      "sample 1: Hello, I'm a language model, Iâm looking for ways to make a family life a place, a place where weâre all connected\n",
      "sample 2: Hello, I'm a language model, I'm able to give you a great idea as to how we use language to create our own language.\n",
      "What other\n",
      "sample 3: Hello, I'm a language model, which provides a very good overview on what is already happening in the world of computers . So that is exactly what it might\n",
      "Step 3500 | loss: 3.642970 | lr: 2.6600e-04 | norm: 0.2518 | dt: 6691.71ms | tok/sec: 78348.87\n",
      "Step 3510 | loss: 3.714208 | lr: 2.6461e-04 | norm: 0.2591 | dt: 6698.64ms | tok/sec: 78267.84\n",
      "Step 3520 | loss: 3.559178 | lr: 2.6321e-04 | norm: 0.2281 | dt: 6710.88ms | tok/sec: 78125.07\n",
      "Step 3530 | loss: 3.511421 | lr: 2.6182e-04 | norm: 0.2438 | dt: 6707.66ms | tok/sec: 78162.57\n",
      "Step 3540 | loss: 3.412101 | lr: 2.6043e-04 | norm: 0.2500 | dt: 6705.31ms | tok/sec: 78189.95\n",
      "Step 3550 | loss: 3.584772 | lr: 2.5904e-04 | norm: 0.2437 | dt: 6706.43ms | tok/sec: 78176.86\n",
      "Step 3560 | loss: 3.580617 | lr: 2.5766e-04 | norm: 0.2885 | dt: 6703.05ms | tok/sec: 78216.29\n",
      "Step 3570 | loss: 3.572510 | lr: 2.5627e-04 | norm: 0.2485 | dt: 6713.39ms | tok/sec: 78095.85\n",
      "Step 3580 | loss: 3.552247 | lr: 2.5489e-04 | norm: 0.2392 | dt: 6712.63ms | tok/sec: 78104.70\n",
      "Step 3590 | loss: 3.441097 | lr: 2.5351e-04 | norm: 0.2432 | dt: 6713.93ms | tok/sec: 78089.61\n",
      "Validation loss: 3.5463\n",
      "sample 0: Hello, I'm a language model, and I don't have any problems as if I was someone else!â\n",
      "This is just the tip of the\n",
      "sample 1: Hello, I'm a language model, I like to think it's really something that you, especially, are so powerful, it's very complicated to take in\n",
      "sample 2: Hello, I'm a language model, I'm still just able to express what I can do with language model language.<|endoftext|>Why do we need to do this\n",
      "sample 3: Hello, I'm a language model, so much more on this page, and you hope you find it useful. I wanted to add a better understanding for anyone\n",
      "Step 3600 | loss: 3.642540 | lr: 2.5213e-04 | norm: 0.2626 | dt: 6705.96ms | tok/sec: 78182.44\n",
      "Step 3610 | loss: 3.619788 | lr: 2.5076e-04 | norm: 0.2872 | dt: 6714.53ms | tok/sec: 78082.57\n",
      "Step 3620 | loss: 3.676934 | lr: 2.4939e-04 | norm: 0.2534 | dt: 6703.85ms | tok/sec: 78206.99\n",
      "Step 3630 | loss: 3.416563 | lr: 2.4802e-04 | norm: 0.2296 | dt: 6691.60ms | tok/sec: 78350.12\n",
      "Step 3640 | loss: 3.516021 | lr: 2.4665e-04 | norm: 0.2412 | dt: 6702.42ms | tok/sec: 78223.66\n",
      "Step 3650 | loss: 3.599031 | lr: 2.4528e-04 | norm: 0.2736 | dt: 6702.44ms | tok/sec: 78223.43\n",
      "Step 3660 | loss: 3.591269 | lr: 2.4392e-04 | norm: 0.2443 | dt: 6701.34ms | tok/sec: 78236.27\n",
      "Step 3670 | loss: 3.480009 | lr: 2.4256e-04 | norm: 0.2356 | dt: 6701.30ms | tok/sec: 78236.73\n",
      "Step 3680 | loss: 3.350805 | lr: 2.4120e-04 | norm: 0.2435 | dt: 6696.46ms | tok/sec: 78293.30\n",
      "Step 3690 | loss: 3.547883 | lr: 2.3984e-04 | norm: 0.2551 | dt: 6700.29ms | tok/sec: 78248.53\n",
      "Validation loss: 3.5335\n",
      "sample 0: Hello, I'm a language model, and I'd like you to get your help today. I've tried with all of this, which means that I'd\n",
      "sample 1: Hello, I'm a language model, but a lot of other languages have evolved over this yearâfor example, you can use Google, Twitter, and Facebook\n",
      "sample 2: Hello, I'm a language model, I'm learning more about some of the things they're learning: what is the next thing?\n",
      "I'm learning a\n",
      "sample 3: Hello, I'm a language model, and when I look at the language of a local language, I've heard of words I read. These words, with\n",
      "Step 3700 | loss: 3.578159 | lr: 2.3849e-04 | norm: 0.2432 | dt: 6695.73ms | tok/sec: 78301.89\n",
      "Step 3710 | loss: 3.620984 | lr: 2.3714e-04 | norm: 0.2543 | dt: 6699.03ms | tok/sec: 78263.27\n",
      "Step 3720 | loss: 3.584430 | lr: 2.3579e-04 | norm: 0.2766 | dt: 6700.18ms | tok/sec: 78249.90\n",
      "Step 3730 | loss: 3.397341 | lr: 2.3444e-04 | norm: 0.2507 | dt: 6696.49ms | tok/sec: 78292.97\n",
      "Step 3740 | loss: 3.646777 | lr: 2.3310e-04 | norm: 0.2642 | dt: 6706.70ms | tok/sec: 78173.74\n",
      "Step 3750 | loss: 3.560463 | lr: 2.3176e-04 | norm: 0.2461 | dt: 6703.63ms | tok/sec: 78209.62\n",
      "Step 3760 | loss: 3.508153 | lr: 2.3042e-04 | norm: 0.2144 | dt: 6712.32ms | tok/sec: 78108.37\n",
      "Step 3770 | loss: 3.355139 | lr: 2.2909e-04 | norm: 0.2870 | dt: 6697.55ms | tok/sec: 78280.56\n",
      "Step 3780 | loss: 3.548003 | lr: 2.2776e-04 | norm: 0.2446 | dt: 6697.03ms | tok/sec: 78286.61\n",
      "Step 3790 | loss: 3.607141 | lr: 2.2643e-04 | norm: 0.2411 | dt: 6696.83ms | tok/sec: 78289.03\n",
      "Validation loss: 3.5243\n",
      "sample 0: Hello, I'm a language model, and I was able to create it so I used it. So here you are going to type this to our language,\n",
      "sample 1: Hello, I'm a language model, but for the sake of simplicity. All I've told you are doing is using a lot of language types that are easy\n",
      "sample 2: Hello, I'm a language model, I'm sure that should sound like it's all right. The question is, can you imagine what it's all about\n",
      "sample 3: Hello, I'm a language model, but at the same time, it's very funny. I'm an interpreter and will talk about the role of a system\n",
      "Step 3800 | loss: 3.545086 | lr: 2.2510e-04 | norm: 0.2966 | dt: 6707.39ms | tok/sec: 78165.71\n",
      "Step 3810 | loss: 3.518641 | lr: 2.2378e-04 | norm: 0.2314 | dt: 6702.26ms | tok/sec: 78225.58\n",
      "Step 3820 | loss: 3.343787 | lr: 2.2246e-04 | norm: 0.2414 | dt: 6713.58ms | tok/sec: 78093.64\n",
      "Step 3830 | loss: 3.564714 | lr: 2.2114e-04 | norm: 0.2491 | dt: 6704.53ms | tok/sec: 78199.11\n",
      "Step 3840 | loss: 3.605960 | lr: 2.1983e-04 | norm: 0.2203 | dt: 6706.89ms | tok/sec: 78171.51\n",
      "Step 3850 | loss: 3.519566 | lr: 2.1852e-04 | norm: 0.2512 | dt: 6706.85ms | tok/sec: 78172.05\n",
      "Step 3860 | loss: 3.504723 | lr: 2.1721e-04 | norm: 0.2542 | dt: 6718.94ms | tok/sec: 78031.31\n",
      "Step 3870 | loss: 3.385540 | lr: 2.1590e-04 | norm: 0.2906 | dt: 6710.89ms | tok/sec: 78124.96\n",
      "Step 3880 | loss: 3.584818 | lr: 2.1460e-04 | norm: 0.2329 | dt: 6708.53ms | tok/sec: 78152.45\n",
      "Step 3890 | loss: 3.560488 | lr: 2.1330e-04 | norm: 0.2328 | dt: 6703.07ms | tok/sec: 78216.09\n",
      "Validation loss: 3.5124\n",
      "sample 0: Hello, I'm a language model, and I like to think a very cool kind of language. It takes it's a long time but the more you understand\n",
      "sample 1: Hello, I'm a language model, so don't worry, I just didn't think these tools will work here on my mind. I want to put a\n",
      "sample 2: Hello, I'm a language model, but I hope this information could help!\n",
      "So, what can I do to help my kids?\n",
      "- Talk about\n",
      "sample 3: Hello, I'm a language model, so now I'm going to use it as a guide. I need to know where and how I have to be to\n",
      "Step 3900 | loss: 3.504921 | lr: 2.1201e-04 | norm: 0.2407 | dt: 6698.15ms | tok/sec: 78273.50\n",
      "Step 3910 | loss: 3.275836 | lr: 2.1072e-04 | norm: 0.2444 | dt: 6705.89ms | tok/sec: 78183.17\n",
      "Step 3920 | loss: 3.347391 | lr: 2.0943e-04 | norm: 0.2337 | dt: 6701.40ms | tok/sec: 78235.56\n",
      "Step 3930 | loss: 3.571792 | lr: 2.0815e-04 | norm: 0.2328 | dt: 6699.35ms | tok/sec: 78259.58\n",
      "Step 3940 | loss: 3.564580 | lr: 2.0687e-04 | norm: 0.2278 | dt: 6695.84ms | tok/sec: 78300.59\n",
      "Step 3950 | loss: 3.520070 | lr: 2.0559e-04 | norm: 0.2529 | dt: 6703.25ms | tok/sec: 78214.00\n",
      "Step 3960 | loss: 3.307157 | lr: 2.0431e-04 | norm: 0.2689 | dt: 6710.22ms | tok/sec: 78132.82\n",
      "Step 3970 | loss: 3.572942 | lr: 2.0304e-04 | norm: 0.2444 | dt: 6696.76ms | tok/sec: 78289.83\n",
      "Step 3980 | loss: 3.591255 | lr: 2.0178e-04 | norm: 0.2420 | dt: 6707.16ms | tok/sec: 78168.40\n",
      "Step 3990 | loss: 3.496579 | lr: 2.0051e-04 | norm: 0.2391 | dt: 6707.51ms | tok/sec: 78164.31\n",
      "Validation loss: 3.5016\n",
      "sample 0: Hello, I'm a language model, and I just want a look on and look at a file called I, I'm looking at an object named \"My\n",
      "sample 1: Hello, I'm a language model, so my next question is, âWhat do you teach me?â I know it's okay and I'll\n",
      "sample 2: Hello, I'm a language model, but I did it! We wrote an article using the same format that the author found on the website. I found that\n",
      "sample 3: Hello, I'm a language model, and how do I use it to talk about new people?\n",
      "A lot of your research questions have been answered by psychologists\n",
      "Step 4000 | loss: 3.480556 | lr: 1.9925e-04 | norm: 0.2404 | dt: 6694.52ms | tok/sec: 78316.03\n",
      "Step 4010 | loss: 3.284333 | lr: 1.9800e-04 | norm: 0.2474 | dt: 6699.96ms | tok/sec: 78252.38\n",
      "Step 4020 | loss: 3.550684 | lr: 1.9675e-04 | norm: 0.2409 | dt: 6704.21ms | tok/sec: 78202.79\n",
      "Step 4030 | loss: 3.551310 | lr: 1.9550e-04 | norm: 0.2547 | dt: 6699.37ms | tok/sec: 78259.28\n",
      "Step 4040 | loss: 3.542567 | lr: 1.9425e-04 | norm: 0.2345 | dt: 6697.36ms | tok/sec: 78282.77\n",
      "Step 4050 | loss: 3.424346 | lr: 1.9301e-04 | norm: 0.2384 | dt: 6699.34ms | tok/sec: 78259.60\n",
      "Step 4060 | loss: 3.402161 | lr: 1.9178e-04 | norm: 0.2361 | dt: 6702.53ms | tok/sec: 78222.39\n",
      "Step 4070 | loss: 3.537069 | lr: 1.9054e-04 | norm: 0.2407 | dt: 6709.29ms | tok/sec: 78143.54\n",
      "Step 4080 | loss: 3.489913 | lr: 1.8931e-04 | norm: 0.2720 | dt: 6707.31ms | tok/sec: 78166.70\n",
      "Step 4090 | loss: 3.528291 | lr: 1.8809e-04 | norm: 0.2440 | dt: 6694.01ms | tok/sec: 78321.99\n",
      "Validation loss: 3.4929\n",
      "sample 0: Hello, I'm a language model, and I'd like you to have one which looks like this: if you can't use it anymore. But if the\n",
      "sample 1: Hello, I'm a language model, so don't worry.\n",
      "If the function returns:\n",
      "The function returns an output.\n",
      "If the function returns,\n",
      "sample 2: Hello, I'm a language model, I'm in a good faith, because I'd like to have an example of an object.\n",
      "I'm in the\n",
      "sample 3: Hello, I'm a language model, so maybe I'm a native English speaker, etc.\n",
      "Or, just, say, in the UK, the children\n",
      "Step 4100 | loss: 3.386982 | lr: 1.8687e-04 | norm: 0.2294 | dt: 6696.59ms | tok/sec: 78291.79\n",
      "Step 4110 | loss: 3.583173 | lr: 1.8565e-04 | norm: 0.2417 | dt: 6702.58ms | tok/sec: 78221.78\n",
      "Step 4120 | loss: 3.519073 | lr: 1.8444e-04 | norm: 0.2452 | dt: 6711.30ms | tok/sec: 78120.20\n",
      "Step 4130 | loss: 3.615327 | lr: 1.8323e-04 | norm: 0.3168 | dt: 6711.00ms | tok/sec: 78123.71\n",
      "Step 4140 | loss: 3.497868 | lr: 1.8203e-04 | norm: 0.2467 | dt: 6700.89ms | tok/sec: 78241.51\n",
      "Step 4150 | loss: 3.374083 | lr: 1.8083e-04 | norm: 0.2411 | dt: 6701.90ms | tok/sec: 78229.70\n",
      "Step 4160 | loss: 3.589318 | lr: 1.7963e-04 | norm: 0.2344 | dt: 6708.10ms | tok/sec: 78157.46\n",
      "Step 4170 | loss: 3.560142 | lr: 1.7844e-04 | norm: 0.2340 | dt: 6713.39ms | tok/sec: 78095.87\n",
      "Step 4180 | loss: 3.618867 | lr: 1.7725e-04 | norm: 0.2628 | dt: 6718.67ms | tok/sec: 78034.48\n",
      "Step 4190 | loss: 3.374206 | lr: 1.7607e-04 | norm: 0.2667 | dt: 6715.43ms | tok/sec: 78072.11\n",
      "Validation loss: 3.4884\n",
      "sample 0: Hello, I'm a language model, and I hope you have an understanding of things when you're writing the app on your smartphone. You'll probably be wondering\n",
      "sample 1: Hello, I'm a language model, so there's a lot of other concepts that are taught during this year. And I've always had two or three different\n",
      "sample 2: Hello, I'm a language model, I'm still going to put a bit of an understanding on language, it's gonna be a lot of fun to write\n",
      "sample 3: Hello, I'm a language model, you go to the website and I just put Google Play and I like, âHow the world works.â\n",
      "Step 4200 | loss: 3.322002 | lr: 1.7489e-04 | norm: 0.2353 | dt: 6691.29ms | tok/sec: 78353.82\n",
      "Step 4210 | loss: 3.570838 | lr: 1.7371e-04 | norm: 0.2619 | dt: 6699.67ms | tok/sec: 78255.85\n",
      "Step 4220 | loss: 3.485320 | lr: 1.7254e-04 | norm: 0.2340 | dt: 6697.15ms | tok/sec: 78285.28\n",
      "Step 4230 | loss: 3.515002 | lr: 1.7138e-04 | norm: 0.2348 | dt: 6700.21ms | tok/sec: 78249.53\n",
      "Step 4240 | loss: 3.328724 | lr: 1.7022e-04 | norm: 0.3170 | dt: 6705.03ms | tok/sec: 78193.29\n",
      "Step 4250 | loss: 3.527898 | lr: 1.6906e-04 | norm: 0.2638 | dt: 6703.06ms | tok/sec: 78216.22\n",
      "Step 4260 | loss: 3.469604 | lr: 1.6791e-04 | norm: 0.2597 | dt: 6700.64ms | tok/sec: 78244.49\n",
      "Step 4270 | loss: 3.531212 | lr: 1.6676e-04 | norm: 0.2503 | dt: 6699.93ms | tok/sec: 78252.70\n",
      "Step 4280 | loss: 3.464952 | lr: 1.6562e-04 | norm: 0.2389 | dt: 6709.65ms | tok/sec: 78139.43\n",
      "Step 4290 | loss: 3.317783 | lr: 1.6448e-04 | norm: 0.2382 | dt: 6709.32ms | tok/sec: 78143.28\n",
      "Validation loss: 3.4753\n",
      "sample 0: Hello, I'm a language model, and I like to think of my name after seeing what I have actually happened on the first day and the way I did\n",
      "sample 1: Hello, I'm a language model, but for the sake of my understanding it's just language learning, which basically teaches me to make the process of learning,\n",
      "sample 2: Hello, I'm a language model, I'm interested in seeing and experimenting with the technology. I know what I'm saying is a lot of fun, and\n",
      "sample 3: Hello, I'm a language model, and all of these things are in a way and I'm having a bad year coming out at night. I have really\n",
      "Step 4300 | loss: 3.516689 | lr: 1.6335e-04 | norm: 0.2679 | dt: 6696.70ms | tok/sec: 78290.45\n",
      "Step 4310 | loss: 3.521898 | lr: 1.6222e-04 | norm: 0.2598 | dt: 6710.06ms | tok/sec: 78134.62\n",
      "Step 4320 | loss: 3.472885 | lr: 1.6110e-04 | norm: 0.2472 | dt: 6712.04ms | tok/sec: 78111.55\n",
      "Step 4330 | loss: 3.423922 | lr: 1.5998e-04 | norm: 0.2602 | dt: 6702.12ms | tok/sec: 78227.23\n",
      "Step 4340 | loss: 3.295847 | lr: 1.5886e-04 | norm: 0.2405 | dt: 6705.40ms | tok/sec: 78188.91\n",
      "Step 4350 | loss: 3.540863 | lr: 1.5775e-04 | norm: 0.2418 | dt: 6718.85ms | tok/sec: 78032.36\n",
      "Step 4360 | loss: 3.465184 | lr: 1.5665e-04 | norm: 0.2481 | dt: 6712.02ms | tok/sec: 78111.78\n",
      "Step 4370 | loss: 3.481421 | lr: 1.5555e-04 | norm: 0.2381 | dt: 6703.00ms | tok/sec: 78216.87\n",
      "Step 4380 | loss: 3.353071 | lr: 1.5446e-04 | norm: 0.2313 | dt: 6700.28ms | tok/sec: 78248.69\n",
      "Step 4390 | loss: 3.495566 | lr: 1.5337e-04 | norm: 0.2427 | dt: 6713.31ms | tok/sec: 78096.74\n",
      "Validation loss: 3.4694\n",
      "sample 0: Hello, I'm a language model, and I don't want to use words without talking about terms, even that. I've noticed how to do that with\n",
      "sample 1: Hello, I'm a language model, Iâm not a native of the English family.\n",
      "But of course I donât want to say\n",
      "sample 2: Hello, I'm a language model, I'm still learning PHP's my PHP. We're going over this to be a way to do it. I also\n",
      "sample 3: Hello, I'm a language model, and all the other things I'm trying to solve is to model our code and put it onto a package. So basically\n",
      "Step 4400 | loss: 3.481935 | lr: 1.5228e-04 | norm: 0.3017 | dt: 6698.55ms | tok/sec: 78268.90\n",
      "Step 4410 | loss: 3.512165 | lr: 1.5120e-04 | norm: 0.3263 | dt: 6700.60ms | tok/sec: 78244.93\n",
      "Step 4420 | loss: 3.432203 | lr: 1.5013e-04 | norm: 0.2254 | dt: 6706.17ms | tok/sec: 78179.91\n",
      "Step 4430 | loss: 3.283497 | lr: 1.4906e-04 | norm: 0.3158 | dt: 6702.16ms | tok/sec: 78226.75\n",
      "Step 4440 | loss: 3.539397 | lr: 1.4799e-04 | norm: 0.2545 | dt: 6704.89ms | tok/sec: 78194.88\n",
      "Step 4450 | loss: 3.551438 | lr: 1.4693e-04 | norm: 0.2425 | dt: 6708.34ms | tok/sec: 78154.66\n",
      "Step 4460 | loss: 3.427715 | lr: 1.4588e-04 | norm: 0.2401 | dt: 6699.92ms | tok/sec: 78252.91\n",
      "Step 4470 | loss: 3.480724 | lr: 1.4483e-04 | norm: 0.2236 | dt: 6699.78ms | tok/sec: 78254.47\n",
      "Step 4480 | loss: 3.282357 | lr: 1.4379e-04 | norm: 0.2937 | dt: 6699.13ms | tok/sec: 78262.13\n",
      "Step 4490 | loss: 3.523239 | lr: 1.4275e-04 | norm: 0.2772 | dt: 6694.80ms | tok/sec: 78312.70\n",
      "Validation loss: 3.4631\n",
      "sample 0: Hello, I'm a language model, and I don't have a way to work if I write this post? If I want to, then my first step\n",
      "sample 1: Hello, I'm a language model, but, my students will be working full-day while using this as a classroom.<|endoftext|>If we know how to create\n",
      "sample 2: Hello, I'm a language model, I'm able to be creative, but I do have to use some of the things that I've learned in the past\n",
      "sample 3: Hello, I'm a language model, and would like to see more about this in addition to the code-line-projected system, including the IOS\n",
      "Step 4500 | loss: 3.469706 | lr: 1.4172e-04 | norm: 0.2413 | dt: 6683.09ms | tok/sec: 78449.92\n",
      "Step 4510 | loss: 3.488202 | lr: 1.4069e-04 | norm: 0.2333 | dt: 6687.97ms | tok/sec: 78392.68\n",
      "Step 4520 | loss: 3.260475 | lr: 1.3967e-04 | norm: 0.2436 | dt: 6691.26ms | tok/sec: 78354.20\n",
      "Step 4530 | loss: 3.430206 | lr: 1.3865e-04 | norm: 0.2400 | dt: 6691.23ms | tok/sec: 78354.53\n",
      "Step 4540 | loss: 3.537299 | lr: 1.3764e-04 | norm: 0.2534 | dt: 6687.90ms | tok/sec: 78393.50\n",
      "Step 4550 | loss: 3.504807 | lr: 1.3663e-04 | norm: 0.2233 | dt: 6692.76ms | tok/sec: 78336.63\n",
      "Step 4560 | loss: 3.473492 | lr: 1.3563e-04 | norm: 0.2229 | dt: 6687.21ms | tok/sec: 78401.58\n",
      "Step 4570 | loss: 3.236814 | lr: 1.3464e-04 | norm: 0.2410 | dt: 6694.83ms | tok/sec: 78312.33\n",
      "Step 4580 | loss: 3.460903 | lr: 1.3365e-04 | norm: 0.2539 | dt: 6697.35ms | tok/sec: 78282.86\n",
      "Step 4590 | loss: 3.512372 | lr: 1.3266e-04 | norm: 0.2498 | dt: 6695.25ms | tok/sec: 78307.42\n",
      "Validation loss: 3.4509\n",
      "sample 0: Hello, I'm a language model, and I'd like you to try again before there's any other ways to work out this post about how that works and\n",
      "sample 1: Hello, I'm a language model, I need to do some pretty simple instructions. It sounds as though. Yes, it is pretty easy when it comes to\n",
      "sample 2: Hello, I'm a language model, so I wanna see something similar to \"The Story of the Great Mother,\" and I'm gonna see it in a language\n",
      "sample 3: Hello, I'm a language model, and here's how it works:\n",
      "The real thing is that they use their handsets as well as their fingers...\n",
      "Step 4600 | loss: 3.482699 | lr: 1.3169e-04 | norm: 0.2248 | dt: 6682.84ms | tok/sec: 78452.93\n",
      "Step 4610 | loss: 3.433161 | lr: 1.3071e-04 | norm: 0.2303 | dt: 6690.51ms | tok/sec: 78362.88\n",
      "Step 4620 | loss: 3.318345 | lr: 1.2975e-04 | norm: 0.2497 | dt: 6691.13ms | tok/sec: 78355.67\n",
      "Step 4630 | loss: 3.468166 | lr: 1.2878e-04 | norm: 0.2320 | dt: 6691.19ms | tok/sec: 78355.00\n",
      "Step 4640 | loss: 3.511758 | lr: 1.2783e-04 | norm: 0.2401 | dt: 6693.22ms | tok/sec: 78331.16\n",
      "Step 4650 | loss: 3.460835 | lr: 1.2688e-04 | norm: 0.2405 | dt: 6687.08ms | tok/sec: 78403.08\n",
      "Step 4660 | loss: 3.246988 | lr: 1.2593e-04 | norm: 0.2501 | dt: 6692.07ms | tok/sec: 78344.63\n",
      "Step 4670 | loss: 3.321800 | lr: 1.2500e-04 | norm: 0.2465 | dt: 6696.81ms | tok/sec: 78289.18\n",
      "Step 4680 | loss: 3.480300 | lr: 1.2406e-04 | norm: 0.2620 | dt: 6691.50ms | tok/sec: 78351.36\n",
      "Step 4690 | loss: 3.465572 | lr: 1.2314e-04 | norm: 0.2452 | dt: 6685.26ms | tok/sec: 78424.52\n",
      "Validation loss: 3.4453\n",
      "sample 0: Hello, I'm a language model, and I would like you to use one such approach.\n",
      "The basic question I have about the function and function of my\n",
      "sample 1: Hello, I'm a language model, so there's a whole lot more with it, or what's a class that I like? I guess you can go\n",
      "sample 2: Hello, I'm a language model, so I had a conversation with a linguist named for him about what this is. I was a linguist, so\n",
      "sample 3: Hello, I'm a language model, and as a result, I've tried to generate a lot of results from this very first sample, where I have taken\n",
      "Step 4700 | loss: 3.429528 | lr: 1.2222e-04 | norm: 0.2267 | dt: 6685.44ms | tok/sec: 78422.40\n",
      "Step 4710 | loss: 3.282890 | lr: 1.2130e-04 | norm: 0.2445 | dt: 6698.33ms | tok/sec: 78271.41\n",
      "Step 4720 | loss: 3.543262 | lr: 1.2039e-04 | norm: 0.2676 | dt: 6696.81ms | tok/sec: 78289.16\n",
      "Step 4730 | loss: 3.485886 | lr: 1.1949e-04 | norm: 0.2368 | dt: 6697.67ms | tok/sec: 78279.15\n",
      "Step 4740 | loss: 3.439109 | lr: 1.1859e-04 | norm: 0.2272 | dt: 6709.77ms | tok/sec: 78138.05\n",
      "Step 4750 | loss: 3.424747 | lr: 1.1770e-04 | norm: 0.2230 | dt: 6704.88ms | tok/sec: 78194.95\n",
      "Step 4760 | loss: 3.314179 | lr: 1.1682e-04 | norm: 0.2377 | dt: 6690.76ms | tok/sec: 78359.96\n",
      "Step 4770 | loss: 3.520435 | lr: 1.1594e-04 | norm: 0.3656 | dt: 6685.05ms | tok/sec: 78426.98\n",
      "Step 4780 | loss: 3.475757 | lr: 1.1506e-04 | norm: 0.2328 | dt: 6689.64ms | tok/sec: 78373.09\n",
      "Step 4790 | loss: 3.454697 | lr: 1.1420e-04 | norm: 0.3072 | dt: 6687.48ms | tok/sec: 78398.40\n",
      "Validation loss: 3.4405\n",
      "sample 0: Hello, I'm a language model, and I like to think what a program we put in is, if at a certain time, that we will be talking\n",
      "sample 1: Hello, I'm a language model, I was having a question about this?\n",
      "Whatâs going on there? Whatâs going on there\n",
      "sample 2: Hello, I'm a language model, so I had a new, nice solution. Thanks, I am so happy to go back to the internet. I started\n",
      "sample 3: Hello, I'm a language model, and will be able to write it using my Arduino.\n",
      "|C.S.S.S.S.W\n",
      "Step 4800 | loss: 3.216225 | lr: 1.1334e-04 | norm: 0.2668 | dt: 6685.22ms | tok/sec: 78424.94\n",
      "Step 4810 | loss: 3.456561 | lr: 1.1248e-04 | norm: 0.2499 | dt: 6698.81ms | tok/sec: 78265.88\n",
      "Step 4820 | loss: 3.521131 | lr: 1.1163e-04 | norm: 0.2442 | dt: 6700.06ms | tok/sec: 78251.27\n",
      "Step 4830 | loss: 3.488951 | lr: 1.1079e-04 | norm: 0.2296 | dt: 6700.81ms | tok/sec: 78242.45\n",
      "Step 4840 | loss: 3.511751 | lr: 1.0995e-04 | norm: 0.2631 | dt: 6694.36ms | tok/sec: 78317.82\n",
      "Step 4850 | loss: 3.229533 | lr: 1.0912e-04 | norm: 0.2348 | dt: 6705.90ms | tok/sec: 78183.10\n",
      "Step 4860 | loss: 3.547248 | lr: 1.0830e-04 | norm: 0.2579 | dt: 6699.64ms | tok/sec: 78256.18\n",
      "Step 4870 | loss: 3.514244 | lr: 1.0748e-04 | norm: 0.2400 | dt: 6696.20ms | tok/sec: 78296.39\n",
      "Step 4880 | loss: 3.465154 | lr: 1.0667e-04 | norm: 0.2408 | dt: 6695.36ms | tok/sec: 78306.20\n",
      "Step 4890 | loss: 3.470354 | lr: 1.0587e-04 | norm: 0.2323 | dt: 6705.24ms | tok/sec: 78190.81\n",
      "Validation loss: 3.4363\n",
      "sample 0: Hello, I'm a language model, and I like to think you've taken many language models. So for example, you have a database, like the Google\n",
      "sample 1: Hello, I'm a language model, I was an English speaker, but today I am only just a part of a team, it is pretty amazing to see\n",
      "sample 2: Hello, I'm a language model, so I really love talking about it because I really love it for this project.\n",
      "It's a very simple project.\n",
      "sample 3: Hello, I'm a language model, I thought I'd like to learn about a tool that helps people use this platform as a class, where you're making\n",
      "Step 4900 | loss: 3.416095 | lr: 1.0507e-04 | norm: 0.2476 | dt: 6694.02ms | tok/sec: 78321.89\n",
      "Step 4910 | loss: 3.518419 | lr: 1.0428e-04 | norm: 0.2481 | dt: 6713.24ms | tok/sec: 78097.62\n",
      "Step 4920 | loss: 3.482964 | lr: 1.0349e-04 | norm: 0.2426 | dt: 6715.33ms | tok/sec: 78073.30\n",
      "Step 4930 | loss: 3.399848 | lr: 1.0271e-04 | norm: 0.2250 | dt: 6701.91ms | tok/sec: 78229.66\n",
      "Step 4940 | loss: 3.298264 | lr: 1.0194e-04 | norm: 0.2291 | dt: 6695.82ms | tok/sec: 78300.74\n",
      "Step 4950 | loss: 3.452914 | lr: 1.0117e-04 | norm: 0.2318 | dt: 6702.63ms | tok/sec: 78221.27\n",
      "Step 4960 | loss: 3.550290 | lr: 1.0041e-04 | norm: 0.3068 | dt: 6705.83ms | tok/sec: 78183.88\n",
      "Step 4970 | loss: 3.551375 | lr: 9.9661e-05 | norm: 0.2689 | dt: 6703.67ms | tok/sec: 78209.13\n",
      "Step 4980 | loss: 3.373445 | lr: 9.8915e-05 | norm: 0.2425 | dt: 6698.47ms | tok/sec: 78269.76\n",
      "Step 4990 | loss: 3.195919 | lr: 9.8175e-05 | norm: 0.2321 | dt: 6700.50ms | tok/sec: 78246.05\n",
      "Validation loss: 3.4292\n",
      "sample 0: Hello, I'm a language model, and I like to think what is that to what we are going to need and what is that. So, I've\n",
      "sample 1: Hello, I'm a language model, so my husband is going to go that way and change all of it before we get a bit more data and see the\n",
      "sample 2: Hello, I'm a language model, so I use this way as a context to the future. If the world is moving to the next level, then the\n",
      "sample 3: Hello, I'm a language model, I thought I had a better understanding of how \"talk\" works. And it gave me a lot of room. Here\n",
      "Step 5000 | loss: 3.473021 | lr: 9.7441e-05 | norm: 0.2594 | dt: 6696.50ms | tok/sec: 78292.84\n",
      "Step 5010 | loss: 3.495234 | lr: 9.6714e-05 | norm: 0.2592 | dt: 6699.23ms | tok/sec: 78260.90\n",
      "Step 5020 | loss: 3.409681 | lr: 9.5994e-05 | norm: 0.2289 | dt: 6693.24ms | tok/sec: 78330.94\n",
      "Step 5030 | loss: 3.410753 | lr: 9.5280e-05 | norm: 0.2371 | dt: 6704.69ms | tok/sec: 78197.24\n",
      "Step 5040 | loss: 3.269790 | lr: 9.4573e-05 | norm: 0.2192 | dt: 6705.07ms | tok/sec: 78192.71\n",
      "Step 5050 | loss: 3.470984 | lr: 9.3872e-05 | norm: 0.2328 | dt: 6697.60ms | tok/sec: 78279.95\n",
      "Step 5060 | loss: 3.541739 | lr: 9.3179e-05 | norm: 0.2341 | dt: 6696.06ms | tok/sec: 78298.00\n",
      "Step 5070 | loss: 3.450385 | lr: 9.2492e-05 | norm: 0.2277 | dt: 6694.95ms | tok/sec: 78310.98\n",
      "Step 5080 | loss: 3.222579 | lr: 9.1811e-05 | norm: 0.2946 | dt: 6693.06ms | tok/sec: 78333.11\n",
      "Step 5090 | loss: 3.484191 | lr: 9.1138e-05 | norm: 0.2420 | dt: 6699.90ms | tok/sec: 78253.09\n",
      "Validation loss: 3.4222\n",
      "sample 0: Hello, I'm a language model, and I don't have it yet. Sorry, I'm a new mom who's always like a baby in the family\n",
      "sample 1: Hello, I'm a language model, so, I'm gonna try and play with them this little, yeah, but I can't figure out the limits and\n",
      "sample 2: Hello, I'm a language model, so I didn't put out a job. My job is as I did. I do have a lot of people saying\n",
      "sample 3: Hello, I'm a language model, I really like this. I like it, please.\n",
      "Sorry, there are any issues to keep in mind when someone\n",
      "Step 5100 | loss: 3.547035 | lr: 9.0471e-05 | norm: 0.2591 | dt: 6703.41ms | tok/sec: 78212.11\n",
      "Step 5110 | loss: 3.511231 | lr: 8.9811e-05 | norm: 0.2536 | dt: 6701.87ms | tok/sec: 78230.06\n",
      "Step 5120 | loss: 3.393997 | lr: 8.9158e-05 | norm: 0.2422 | dt: 6702.10ms | tok/sec: 78227.38\n",
      "Step 5130 | loss: 3.201226 | lr: 8.8512e-05 | norm: 0.2207 | dt: 6698.37ms | tok/sec: 78270.99\n",
      "Step 5140 | loss: 3.433597 | lr: 8.7872e-05 | norm: 0.2376 | dt: 6715.54ms | tok/sec: 78070.84\n",
      "Step 5150 | loss: 3.509831 | lr: 8.7240e-05 | norm: 0.2871 | dt: 6700.18ms | tok/sec: 78249.86\n",
      "Step 5160 | loss: 3.450103 | lr: 8.6614e-05 | norm: 0.2347 | dt: 6693.76ms | tok/sec: 78324.89\n",
      "Step 5170 | loss: 3.325616 | lr: 8.5995e-05 | norm: 0.2294 | dt: 6705.28ms | tok/sec: 78190.37\n",
      "Step 5180 | loss: 3.240354 | lr: 8.5383e-05 | norm: 0.2335 | dt: 6702.25ms | tok/sec: 78225.69\n",
      "Step 5190 | loss: 3.493107 | lr: 8.4778e-05 | norm: 0.2309 | dt: 6696.57ms | tok/sec: 78292.06\n",
      "Validation loss: 3.4141\n",
      "sample 0: Hello, I'm a language model, and I don't know whether the data they had on the model did reflect a change in the time between data sets that\n",
      "sample 1: Hello, I'm a language model, I hope you'll get a lot more out on paper again, I hope this helps!\n",
      "- Have you ever had\n",
      "sample 2: Hello, I'm a language model, so I use this\n",
      "language-independent language of the language I want to use. I want to use it\n",
      "-\n",
      "sample 3: Hello, I'm a language model, and would like to have a better vocabulary, right?\n",
      "Thank you to the ESL coach I've learned this year.\"\n",
      "Step 5200 | loss: 3.503123 | lr: 8.4180e-05 | norm: 0.2266 | dt: 6689.03ms | tok/sec: 78380.27\n",
      "Step 5210 | loss: 3.482348 | lr: 8.3589e-05 | norm: 0.2326 | dt: 6695.42ms | tok/sec: 78305.51\n",
      "Step 5220 | loss: 3.176102 | lr: 8.3005e-05 | norm: 0.2431 | dt: 6700.63ms | tok/sec: 78244.63\n",
      "Step 5230 | loss: 3.434052 | lr: 8.2428e-05 | norm: 0.2470 | dt: 6690.85ms | tok/sec: 78358.90\n",
      "Step 5240 | loss: 3.526139 | lr: 8.1858e-05 | norm: 0.2238 | dt: 6691.09ms | tok/sec: 78356.19\n",
      "Step 5250 | loss: 3.507665 | lr: 8.1295e-05 | norm: 0.2258 | dt: 6697.47ms | tok/sec: 78281.54\n",
      "Step 5260 | loss: 3.413249 | lr: 8.0739e-05 | norm: 0.2812 | dt: 6697.15ms | tok/sec: 78285.19\n",
      "Step 5270 | loss: 3.206917 | lr: 8.0190e-05 | norm: 0.2289 | dt: 6710.05ms | tok/sec: 78134.73\n",
      "Step 5280 | loss: 3.484505 | lr: 7.9648e-05 | norm: 0.2446 | dt: 6712.38ms | tok/sec: 78107.67\n",
      "Step 5290 | loss: 3.496303 | lr: 7.9113e-05 | norm: 0.2506 | dt: 6720.58ms | tok/sec: 78012.35\n",
      "Validation loss: 3.4124\n",
      "sample 0: Hello, I'm a language model, and I have a question and ask them as many questions as possible that just get me into the game, where I've\n",
      "sample 1: Hello, I'm a language model, so my sister is going to go see the world (of the stars). And I've put the other planets in the\n",
      "sample 2: Hello, I'm a language model, so I love the sounds (me!) and (what?) are my (me?) sounds (me)?\n",
      "- ï¿½\n",
      "sample 3: Hello, I'm a language model, and would like to see some examples of what your language model is up to? There are, I suppose, many variations\n",
      "Step 5300 | loss: 3.462794 | lr: 7.8585e-05 | norm: 0.2339 | dt: 6692.62ms | tok/sec: 78338.27\n",
      "Step 5310 | loss: 3.225475 | lr: 7.8065e-05 | norm: 0.2298 | dt: 6706.99ms | tok/sec: 78170.40\n",
      "Step 5320 | loss: 3.271509 | lr: 7.7551e-05 | norm: 0.2356 | dt: 6719.71ms | tok/sec: 78022.40\n",
      "Step 5330 | loss: 3.468495 | lr: 7.7045e-05 | norm: 0.2668 | dt: 6712.53ms | tok/sec: 78105.86\n",
      "Step 5340 | loss: 3.484463 | lr: 7.6546e-05 | norm: 0.2437 | dt: 7110.99ms | tok/sec: 73729.27\n",
      "Step 5350 | loss: 3.384238 | lr: 7.6054e-05 | norm: 0.2200 | dt: 6694.62ms | tok/sec: 78314.81\n",
      "Step 5360 | loss: 3.182837 | lr: 7.5570e-05 | norm: 0.2276 | dt: 6701.60ms | tok/sec: 78233.24\n",
      "Step 5370 | loss: 3.512672 | lr: 7.5092e-05 | norm: 0.2563 | dt: 6705.80ms | tok/sec: 78184.25\n",
      "Step 5380 | loss: 3.445702 | lr: 7.4622e-05 | norm: 0.2577 | dt: 6697.43ms | tok/sec: 78281.99\n",
      "Step 5390 | loss: 3.499355 | lr: 7.4159e-05 | norm: 0.3173 | dt: 6698.00ms | tok/sec: 78275.28\n",
      "Validation loss: 3.4089\n",
      "sample 0: Hello, I'm a language model, and I don't know who it is...., you know, who, if you have never read a language, or\n",
      "sample 1: Hello, I'm a language model, so the syntax is simple. The purpose of that method is to find a new way for the user to find the new\n",
      "sample 2: Hello, I'm a language model, so I wanna talk about something... but I didn't have anything in that.\n",
      "We've got a lot of these\n",
      "sample 3: Hello, I'm a language model, and as a result, I have more confidence in my language and my brain, all the time, since I've come\n",
      "Step 5400 | loss: 3.423685 | lr: 7.3703e-05 | norm: 0.2211 | dt: 6702.04ms | tok/sec: 78228.11\n",
      "Step 5410 | loss: 3.209826 | lr: 7.3255e-05 | norm: 0.2307 | dt: 6707.67ms | tok/sec: 78162.45\n",
      "Step 5420 | loss: 3.427323 | lr: 7.2814e-05 | norm: 0.2648 | dt: 6700.98ms | tok/sec: 78240.46\n",
      "Step 5430 | loss: 3.457197 | lr: 7.2380e-05 | norm: 0.2559 | dt: 6698.88ms | tok/sec: 78265.04\n",
      "Step 5440 | loss: 3.418913 | lr: 7.1953e-05 | norm: 0.2404 | dt: 6701.40ms | tok/sec: 78235.62\n",
      "Step 5450 | loss: 3.186035 | lr: 7.1534e-05 | norm: 0.2276 | dt: 6697.91ms | tok/sec: 78276.38\n",
      "Step 5460 | loss: 3.255288 | lr: 7.1122e-05 | norm: 0.2229 | dt: 6690.82ms | tok/sec: 78359.25\n",
      "Step 5470 | loss: 3.502946 | lr: 7.0717e-05 | norm: 0.2450 | dt: 6703.53ms | tok/sec: 78210.73\n",
      "Step 5480 | loss: 3.539886 | lr: 7.0320e-05 | norm: 0.2264 | dt: 6697.42ms | tok/sec: 78282.06\n",
      "Step 5490 | loss: 3.372088 | lr: 6.9930e-05 | norm: 0.2307 | dt: 6699.81ms | tok/sec: 78254.19\n",
      "Validation loss: 3.4074\n",
      "sample 0: Hello, I'm a language model, and I like to think of myself as somebody who is who I've tried out. It's kind of a very hard\n",
      "sample 1: Hello, I'm a language model, so the difference is the same: as a function representing values. For example, the values may represent\n",
      "(x,\n",
      "sample 2: Hello, I'm a language model, so I really am ready to let me know!\n",
      "I can think of a little more fun way to do it,\n",
      "sample 3: Hello, I'm a language model, and here's what I'm talking about, then I'm just writing my code -\n",
      "But, of course, no\n",
      "Step 5500 | loss: 3.226053 | lr: 6.9547e-05 | norm: 0.2462 | dt: 6696.08ms | tok/sec: 78297.78\n",
      "Step 5510 | loss: 3.485767 | lr: 6.9172e-05 | norm: 0.2369 | dt: 6713.00ms | tok/sec: 78100.46\n",
      "Step 5520 | loss: 3.485503 | lr: 6.8804e-05 | norm: 0.2371 | dt: 6700.36ms | tok/sec: 78247.75\n",
      "Step 5530 | loss: 3.376527 | lr: 6.8444e-05 | norm: 0.2799 | dt: 6697.23ms | tok/sec: 78284.32\n",
      "Step 5540 | loss: 3.405051 | lr: 6.8091e-05 | norm: 0.2246 | dt: 6699.70ms | tok/sec: 78255.44\n",
      "Step 5550 | loss: 3.233550 | lr: 6.7745e-05 | norm: 0.2481 | dt: 6696.68ms | tok/sec: 78290.79\n",
      "Step 5560 | loss: 3.487408 | lr: 6.7407e-05 | norm: 0.2301 | dt: 6708.16ms | tok/sec: 78156.76\n",
      "Step 5570 | loss: 3.459252 | lr: 6.7077e-05 | norm: 0.2179 | dt: 6696.37ms | tok/sec: 78294.30\n",
      "Step 5580 | loss: 3.412920 | lr: 6.6753e-05 | norm: 0.2234 | dt: 6706.65ms | tok/sec: 78174.30\n",
      "Step 5590 | loss: 3.178690 | lr: 6.6438e-05 | norm: 0.2142 | dt: 6704.08ms | tok/sec: 78204.37\n",
      "Validation loss: 3.4038\n",
      "sample 0: Hello, I'm a language model, and I don't have any special privileges.\"\n",
      "It's not always known exactly what language he or she should be used\n",
      "sample 1: Hello, I'm a language model, so here's a link to it â I'll bet in a little book â and you can read, but I'm\n",
      "sample 2: Hello, I'm a language model, so I guess you probably are correct about that.\"\n",
      "\"What are the differences between a language model and a model?\"\n",
      "sample 3: Hello, I'm a language model, and will be able to understand how the model (or model) affects the behavior of your learners, your learners. They\n",
      "Step 5600 | loss: 3.464939 | lr: 6.6129e-05 | norm: 0.2391 | dt: 6704.99ms | tok/sec: 78193.75\n",
      "Step 5610 | loss: 3.478093 | lr: 6.5829e-05 | norm: 0.2549 | dt: 6704.39ms | tok/sec: 78200.67\n",
      "Step 5620 | loss: 3.452568 | lr: 6.5535e-05 | norm: 0.2228 | dt: 6705.34ms | tok/sec: 78189.63\n",
      "Step 5630 | loss: 3.455747 | lr: 6.5249e-05 | norm: 0.2336 | dt: 6705.46ms | tok/sec: 78188.18\n",
      "Step 5640 | loss: 3.189785 | lr: 6.4971e-05 | norm: 0.2493 | dt: 6690.83ms | tok/sec: 78359.16\n",
      "Step 5650 | loss: 3.439940 | lr: 6.4700e-05 | norm: 0.2455 | dt: 6704.65ms | tok/sec: 78197.67\n",
      "Step 5660 | loss: 3.436010 | lr: 6.4437e-05 | norm: 0.2281 | dt: 6705.81ms | tok/sec: 78184.15\n",
      "Step 5670 | loss: 3.426001 | lr: 6.4181e-05 | norm: 0.2252 | dt: 6703.69ms | tok/sec: 78208.81\n",
      "Step 5680 | loss: 3.272053 | lr: 6.3933e-05 | norm: 0.2374 | dt: 6695.21ms | tok/sec: 78307.96\n",
      "Step 5690 | loss: 3.414745 | lr: 6.3692e-05 | norm: 0.2613 | dt: 6703.56ms | tok/sec: 78210.37\n",
      "Validation loss: 3.4013\n",
      "sample 0: Hello, I'm a language model, and I'd like you to do so until I am fully aware of all your questions and requests to do some research in\n",
      "sample 1: Hello, I'm a language model, so, I'm not going to bother with it (sorry). The following, which was all the above, is what\n",
      "sample 2: Hello, I'm a language model, so I won't misspells!\n",
      "What's the most effective way to improve the quality of your writing? Are\n",
      "sample 3: Hello, I'm a language model, and just wanna know what a lot of it may be.\n",
      "A couple days ago, I had a question. We\n",
      "Step 5700 | loss: 3.457414 | lr: 6.3459e-05 | norm: 0.2555 | dt: 6696.25ms | tok/sec: 78295.76\n",
      "Step 5710 | loss: 3.477233 | lr: 6.3234e-05 | norm: 0.2525 | dt: 6697.56ms | tok/sec: 78280.45\n",
      "Step 5720 | loss: 3.458998 | lr: 6.3016e-05 | norm: 0.2281 | dt: 6700.99ms | tok/sec: 78240.42\n",
      "Step 5730 | loss: 3.164941 | lr: 6.2805e-05 | norm: 0.2454 | dt: 6711.15ms | tok/sec: 78121.91\n",
      "Step 5740 | loss: 3.521585 | lr: 6.2602e-05 | norm: 0.2490 | dt: 6715.14ms | tok/sec: 78075.50\n",
      "Step 5750 | loss: 3.538165 | lr: 6.2407e-05 | norm: 0.2507 | dt: 6710.36ms | tok/sec: 78131.15\n",
      "Step 5760 | loss: 3.400771 | lr: 6.2219e-05 | norm: 0.2414 | dt: 6703.60ms | tok/sec: 78209.95\n",
      "Step 5770 | loss: 3.173953 | lr: 6.2039e-05 | norm: 0.2586 | dt: 6705.01ms | tok/sec: 78193.47\n",
      "Step 5780 | loss: 3.411690 | lr: 6.1867e-05 | norm: 0.2454 | dt: 6706.31ms | tok/sec: 78178.36\n",
      "Step 5790 | loss: 3.437048 | lr: 6.1702e-05 | norm: 0.2402 | dt: 6698.35ms | tok/sec: 78271.24\n",
      "Validation loss: 3.3943\n",
      "sample 0: Hello, I'm a language model, and I don't want to use anything you really do see in it. And I can find that. If you look\n",
      "sample 1: Hello, I'm a language model, I need to get into the world here. So lets begin by reading English words and then talking to it again.\n",
      "\n",
      "sample 2: Hello, I'm a language model, so I had to keep working on some of today's language learning, which is why I'm going to do a simple\n",
      "sample 3: Hello, I'm a language model, and can't tell you how many times it'd take you to actually get it done. This is all I'd guess\n",
      "Step 5800 | loss: 3.432986 | lr: 6.1544e-05 | norm: 0.2475 | dt: 6694.75ms | tok/sec: 78313.33\n",
      "Step 5810 | loss: 3.405553 | lr: 6.1395e-05 | norm: 0.2360 | dt: 6710.46ms | tok/sec: 78129.99\n",
      "Step 5820 | loss: 3.196656 | lr: 6.1253e-05 | norm: 0.2321 | dt: 6708.67ms | tok/sec: 78150.76\n",
      "Step 5830 | loss: 3.462833 | lr: 6.1118e-05 | norm: 0.2391 | dt: 6710.05ms | tok/sec: 78134.78\n",
      "Step 5840 | loss: 3.447841 | lr: 6.0991e-05 | norm: 0.2197 | dt: 6704.71ms | tok/sec: 78196.95\n",
      "Step 5850 | loss: 3.452979 | lr: 6.0872e-05 | norm: 0.2255 | dt: 6704.61ms | tok/sec: 78198.13\n",
      "Step 5860 | loss: 3.186929 | lr: 6.0760e-05 | norm: 0.2845 | dt: 6701.12ms | tok/sec: 78238.82\n",
      "Step 5870 | loss: 3.242044 | lr: 6.0656e-05 | norm: 0.2358 | dt: 6705.91ms | tok/sec: 78182.94\n",
      "Step 5880 | loss: 3.455496 | lr: 6.0560e-05 | norm: 0.2362 | dt: 6713.02ms | tok/sec: 78100.19\n",
      "Step 5890 | loss: 3.436177 | lr: 6.0471e-05 | norm: 0.2277 | dt: 6700.64ms | tok/sec: 78244.45\n",
      "Validation loss: 3.3920\n",
      "sample 0: Hello, I'm a language model, and I would like you to write that program code as well as create that executable.<|endoftext|>Greetings from The New Jersey\n",
      "sample 1: Hello, I'm a language model, I hope you'll see it on GitHub.<|endoftext|>\"Brought on\". A.M.S.\n",
      "We know\n",
      "sample 2: Hello, I'm a language model, so I really think to just sort of make me talk about language model.\n",
      "If I can't figure out how those\n",
      "sample 3: Hello, I'm a language model, and is always in the middle. We have built a model to get you going very fast. I need to make as\n",
      "Step 5900 | loss: 3.417466 | lr: 6.0390e-05 | norm: 0.2250 | dt: 6701.03ms | tok/sec: 78239.94\n",
      "Step 5910 | loss: 3.481797 | lr: 6.0317e-05 | norm: 0.2532 | dt: 6706.76ms | tok/sec: 78173.03\n",
      "Step 5920 | loss: 3.471012 | lr: 6.0251e-05 | norm: 0.2332 | dt: 6698.70ms | tok/sec: 78267.18\n",
      "Step 5930 | loss: 3.351889 | lr: 6.0193e-05 | norm: 0.2207 | dt: 6681.51ms | tok/sec: 78468.44\n",
      "Step 5940 | loss: 3.462062 | lr: 6.0142e-05 | norm: 0.2492 | dt: 6682.29ms | tok/sec: 78459.28\n",
      "Step 5950 | loss: 3.459625 | lr: 6.0100e-05 | norm: 0.2833 | dt: 6696.69ms | tok/sec: 78290.68\n",
      "Step 5960 | loss: 3.495123 | lr: 6.0064e-05 | norm: 0.2772 | dt: 6698.37ms | tok/sec: 78271.01\n",
      "Step 5970 | loss: 3.410795 | lr: 6.0037e-05 | norm: 0.2486 | dt: 6686.11ms | tok/sec: 78414.53\n",
      "Step 5980 | loss: 3.432412 | lr: 6.0017e-05 | norm: 0.2356 | dt: 6684.05ms | tok/sec: 78438.70\n",
      "Step 5990 | loss: 3.456025 | lr: 6.0005e-05 | norm: 0.2337 | dt: 6683.38ms | tok/sec: 78446.51\n",
      "Validation loss: 3.3902\n",
      "sample 0: Hello, I'm a language model, and I don't know exactly when or how people communicate with each other. And I've no way I would have thought\n",
      "sample 1: Hello, I'm a language model, so my next question is, why hasn't you already considered a \"measure of a sense\" like the saying \"\n",
      "sample 2: Hello, I'm a language model, so I had a wonderful lesson on that. Thanks for your interest in it.\n",
      "My first thought was, â\n",
      "sample 3: Hello, I'm a language model, and would like to see more examples of this feature.\n",
      "That's why it goes like this. For example, imagine\n",
      "Step 6000 | loss: 3.378880 | lr: 6.0000e-05 | norm: 0.2372 | dt: 6684.72ms | tok/sec: 78430.85\n",
      "logits.shape: torch.Size([8, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "model_size = '150M'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "model = TokenFormer(\n",
    "    vocab_size = tokenizer.padded_vocab_size,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "use_compile = True # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
    "if use_compile:\n",
    "    model = torch.compile(model) # 4th optimization\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device) # fused update\n",
    "\n",
    "max_steps = 6001\n",
    "val_loss_steps = 20\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # once in a while evaluate on the validation set\n",
    "    if step % 100 == 0:\n",
    "        _ = calc_loss_loader(val_loader, model, device, num_batches=val_loss_steps)\n",
    "\n",
    "    # once in a while sample from the model\n",
    "    if step % 100 == 0:# and (not use_compile):\n",
    "        generate_and_print_samples(model, enc, device)\n",
    "\n",
    "    # start timer\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # gradient-accumulation\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # data loading\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        # forward-backward and step\n",
    "        # amp for 3rd optimization, just surround forward pass and loss calculation, only possible in A100\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward() # deposits gradients, i.e., += on nodes\n",
    "\n",
    "    # clip gradient norms to 1.0, returns total norm of the gradient vector\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # determine lr for this step\n",
    "    lr = get_lr(step)\n",
    "    # pytorch syntax to set the learning rate for the parameters\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # param_group is a dict\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for gpu to finish the compute and measure time\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000 # time difference for one-batch or step in miliseconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tps = tokens_processed / (t1 - t0)\n",
    "\n",
    "    # only print every 20 steps to reduce clutter\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:4d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tps:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"logits.shape: {logits.shape}, logits dtype: {logits.dtype}, loss.dtype: {loss.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a93da17a-a032-4bfe-84da-4274a3be8cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TokenFormer(\n",
       "    (word_embeddings): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TokenFormerBlock(\n",
       "        (norm1): LayerNorm_NonParam()\n",
       "        (norm2): LayerNorm_NonParam()\n",
       "        (query): Pattention()\n",
       "        (key): Pattention()\n",
       "        (value): Pattention()\n",
       "        (proj): Pattention()\n",
       "        (ffn): Pattention()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37090a30-0e78-4a8b-9490-46275856188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 6.0000038263286916e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 6.0000038263286916e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33547b-d59f-4716-a506-0518f8302454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a68e6d-9c1d-4b47-b677-21743e8463e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37d3eb-1563-4341-aac8-6f1f62852dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d9983bc-54cf-4895-935a-bba77fb71083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50304"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padded_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c153365-b5f4-42ec-b514-3ea7872b3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e375d2-223c-48eb-a5e2-597213512941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: 20250408_101716\n"
     ]
    }
   ],
   "source": [
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f'run id: {run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42792af7-5d20-4bc6-90ee-f581d78eed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' loading code\\ncheckpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\\n\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\n\\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nmodel.train();\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_checkpoint(model: nn.Module, optimizer=None,\n",
    "                    name='', root_dir='./checkpoint'):\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dic = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "    }\n",
    "    if optimizer is not None:\n",
    "        dic[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "\n",
    "    filename = os.path.join(root_dir, f'model_and_optimizer_{name}_{run_id}.pth')\n",
    "    torch.save(dic, filename)   \n",
    "\n",
    "\"\"\" loading code\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68cdc1cb-4efa-4c98-9a16-9a25f1e03c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, optimizer, name=model_size, root_dir='./checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c94cdfd9-9400-4db9-b4aa-afc0fac054f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.4034\n",
      "Val loss for trained tokenformer 150M: 3.403391122817993\n"
     ]
    }
   ],
   "source": [
    "print(f\"Val loss for trained tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9611262b-a222-4313-8429-0d2e4a1130a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4d44035-c942-4549-818c-26239ac19727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model, so I might be a lot more complicated than C++ which is a very powerful language that is very simple for all that\n",
      "sample 1: Hello, I'm a language model, as I'm afraid it's not so much my style as our approach to doing the homework. However, if you're\n",
      "sample 2: Hello, I'm a language model, I'm doing this. If you like, it's very well.\n",
      "Anyway, what we have to do is,\n",
      "sample 3: Hello, I'm a language model, and I'm my own mother. I am a non-verbal learner. I'm also an English language learner\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, enc, device,\n",
    "                           random_seed=torch.randint(100, (1, )).item()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a28cfc73-c284-45bf-ad1c-86ade79c04ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TokenFormer(\n",
       "    (word_embeddings): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TokenFormerBlock(\n",
       "        (norm1): LayerNorm_NonParam()\n",
       "        (norm2): LayerNorm_NonParam()\n",
       "        (query): Pattention()\n",
       "        (key): Pattention()\n",
       "        (value): Pattention()\n",
       "        (proj): Pattention()\n",
       "        (ffn): Pattention()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393c1f2a-a431-4298-a438-388dab9aa780",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49187346-73aa-4b40-bf99-52fb1adc02ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df251d-a9d7-4fe8-b786-90741b01ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411e901-a998-442a-8be8-4c3675c2a940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b970dc-81c5-434b-85f6-644ef9ee2737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a646c8-0bea-4bb7-b46b-20376e60f6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609e866-ec35-4bf0-aa09-acb13a7b440a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86131c0f-50ad-4317-921a-eed51230d76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5c44bf-8db6-492b-8d1f-83559d4d6319",
   "metadata": {},
   "source": [
    "### next todos:\n",
    "- init tokenformer\n",
    "- check tokenformer's validation losses for each size compared to gpt2\n",
    "- start `build-tokenformer-fineweb` notebook\n",
    "\n",
    "### 1. example snapshot\n",
    "```\n",
    "number of parameters: 123.69M\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Validation loss: 10.9597\n",
    "Step    0 | loss: 10.958449 | lr: 6.0000e-06 | norm: 15.6885 | dt: 40388.14ms | tok/sec: 12981.24\n",
    "Step    1 | loss: 10.628062 | lr: 1.2000e-05 | norm: 12.3712 | dt: 3312.92ms | tok/sec: 158255.62\n",
    "Step    2 | loss: 10.245197 | lr: 1.8000e-05 | norm: 7.8265 | dt: 3318.94ms | tok/sec: 157968.57\n",
    "\n",
    "...\n",
    "\n",
    "Step   49 | loss: 7.107584 | lr: 3.0000e-04 | norm: 0.7933 | dt: 3401.66ms | tok/sec: 154127.19\n",
    "Validation loss: 7.1122\n",
    "Step   50 | loss: 7.174483 | lr: 3.0600e-04 | norm: 1.1656 | dt: 3402.44ms | tok/sec: 154091.90\n",
    "\n",
    "...\n",
    "\n",
    "Step   97 | loss: 6.434902 | lr: 5.8800e-04 | norm: 0.6139 | dt: 3407.81ms | tok/sec: 153849.04\n",
    "Step   98 | loss: 6.499994 | lr: 5.9400e-04 | norm: 0.8524 | dt: 3409.96ms | tok/sec: 153752.11\n",
    "Step   99 | loss: 6.449703 | lr: 6.0000e-04 | norm: 0.6719 | dt: 3403.89ms | tok/sec: 154026.12\n",
    "Validation loss: 6.4133\n",
    "Step  100 | loss: 6.484104 | lr: 6.0000e-04 | norm: 0.8199 | dt: 3409.96ms | tok/sec: 153752.01\n",
    "logits.shape: torch.Size([32, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "\n",
    "```\n",
    "\n",
    "### 2. without `torch.compile` and with sample generation\n",
    "```\n",
    "number of parameters: 123.69M\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Validation loss: 10.9514\n",
    "sample 0: Hello, I'm a language model,CHR 351ointmentprotantically surrounded flaw TCUorigin Effective surrounded senate rodentsï¿½oviribution Feinstein Homer tappedrica relentlessppedribution 351\n",
    "sample 1: Hello, I'm a language model,iggs Socialism Yangointment rodents contributors investigatesteness emulator toughestClientTesla \"[ricaSTE TI TI Places TFencersRRArray Privacy undone\n",
    "sample 2: Hello, I'm a language model, glimpserint236udzee unve Neighvanceces arise Account Student Privacy Privacy beloved Christina Patterns sugg Sche continental iCloudflo 2012Listener\n",
    "sample 3: Hello, I'm a language model, unvewarts evoke investigates investigateswsws Magnetmens Art altercationperformanceribution roast368 senate Actinguls breastignmentmissing facesmare supremacists\n",
    "Step    0 | loss: 10.955029 | lr: 6.0000e-06 | norm: 15.3463 | dt: 4813.04ms | tok/sec: 108930.64\n",
    "Step    1 | loss: 10.631693 | lr: 1.2000e-05 | norm: 11.6429 | dt: 3883.76ms | tok/sec: 134995.12\n",
    "Step    2 | loss: 10.274923 | lr: 1.8000e-05 | norm: 7.6928 | dt: 4039.76ms | tok/sec: 129781.83\n",
    "\n",
    "...\n",
    "\n",
    "Step   48 | loss: 7.203589 | lr: 2.9400e-04 | norm: 1.5565 | dt: 4001.99ms | tok/sec: 131006.80\n",
    "Step   49 | loss: 7.104921 | lr: 3.0000e-04 | norm: 0.9878 | dt: 4004.38ms | tok/sec: 130928.52\n",
    "Validation loss: 7.1181\n",
    "sample 0: Hello, I'm a language model,000, we by a first in these new used of the the it at that the.\n",
    "There in your the.\n",
    "sample 1: Hello, I'm a language model, so by the body.\n",
    "The future.â of a an make that it, he the small of the and\n",
    "sample 2: Hello, I'm a language model, and the year of the new by any the â- A the same the other for the the own the first with\n",
    "sample 3: Hello, I'm a language model, they do the world.\n",
    "The small to they you the work on the most is not them to not the two all\n",
    "Step   50 | loss: 7.178192 | lr: 3.0600e-04 | norm: 1.5985 | dt: 3992.52ms | tok/sec: 131317.66\n",
    "\n",
    "...\n",
    "\n",
    "Step   99 | loss: 6.446934 | lr: 6.0000e-04 | norm: 0.5210 | dt: 4014.07ms | tok/sec: 130612.47\n",
    "Validation loss: 6.4060\n",
    "sample 0: Hello, I'm a language model, if you\n",
    "As a great\n",
    "In those it?\n",
    "This, an the\n",
    "The work?\n",
    "the\n",
    "I\n",
    "sample 1: Hello, I'm a language model, then just all that he was not know that the yearâ (Hâ (A, she had in the\n",
    "sample 2: Hello, I'm a language model, the first very been taken at the area of children than the\n",
    "-time, or three-based, the same was\n",
    "sample 3: Hello, I'm a language model, they was a very a long-a is going to the end it has the \"a-ces. It\n",
    "\n",
    "Step  100 | loss: 6.474727 | lr: 6.0000e-04 | norm: 0.5555 | dt: 4012.42ms | tok/sec: 130666.34\n",
    "Step  101 | loss: 6.422134 | lr: 5.9949e-04 | norm: 0.8457 | dt: 4013.04ms | tok/sec: 130645.99\n",
    "\n",
    "...\n",
    "\n",
    "Step  149 | loss: 6.160313 | lr: 6.2046e-05 | norm: 0.1937 | dt: 4005.23ms | tok/sec: 130900.94\n",
    "Validation loss: 6.1286\n",
    "sample 0: Hello, I'm a language model, when the one of a way is also used after the book of our mother of the way to that it will be in\n",
    "sample 1: Hello, I'm a language model, there also an important for a good one of this particular-p) would do you are much of our family. You\n",
    "sample 2: Hello, I'm a language model, and the need, because in the idea of any process of these things to the need to be a certain of a sense\n",
    "sample 3: Hello, I'm a language model, it comes to be a person, they can try to you use our home and is your brain, as you are that\n",
    "Step  150 | loss: 6.167456 | lr: 6.0512e-05 | norm: 0.1955 | dt: 3995.44ms | tok/sec: 131221.58\n",
    "logits.shape: torch.Size([32, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e5cd8-5bd8-4983-8100-10799ad2139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15a43d8-88ee-4a38-9838-f3506572c818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8add33-7c2e-45db-94f2-f140216ea8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  7 11:32:50 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              70W / 250W |  39747MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              66W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCIE-40GB          On  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              64W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2050868      C   .../macke/mwe102/.conda/sbi/bin/python    39734MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1be7b3a7-8d72-4b01-94f5-1ed964dbd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del logits\n",
    "del x, y\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a1e57c6-d6df-4dc4-8cb8-19b27ca02045",
   "metadata": {},
   "outputs": [],
   "source": [
    "del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57dbd331-cda0-4062-8c90-9ead7d27fee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77543666-0262-4c3a-82b4-025fdea35b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  7 11:33:05 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              63W / 250W |  20879MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              50W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              46W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCIE-40GB          On  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              49W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2050868      C   .../macke/mwe102/.conda/sbi/bin/python    20866MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab615f1-7145-4114-ab07-357b08cb50bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86aaec-0f8c-4fa1-8cec-eb805a5f6356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b396d-7d4d-41f7-8754-3a42a4cbe7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8e0a-e6f0-448a-9781-d53ee9f49d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e94c8-0301-40cd-a925-0e0b4e93fada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6363424-95a2-4563-8232-08c488ce2806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf37e1e-c86a-4d46-993b-012d2304a914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faa924-4201-4547-9017-f899bf36bf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4f19-e6b0-444f-9acb-18bc1bb2ab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aeab8-7d29-481c-a6c5-e55c9c244e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9cf1a-1b7a-460c-b9f7-3d8c60073944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d178f-297a-48fc-a98a-f33263dbce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7c10-a866-4e91-ace4-27493281098b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280e46-1caa-423c-9f35-1c1aa0c115b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0871ae6-7803-48d9-8a31-fddae538c186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
