# build-gpt2-moe

This repository houses some of my experiments to understand how to train small (~GPT2 scale) LLMs and their Mixture-of-Experts (MoEs) variants.

<p align="center">
  <img src="https://github.com/swag2198/build-gpt2/blob/main/results/figures/val_all_seeds.png?raw=true" alt="alt text"/>
</p>

<!-- TABLE OF CONTENTS -->
<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#dataset-used">Dataset used</a></li>
    <li><a href="#model-description">Repository structure</a></li>
    <li><a href="#hyperparameters">Hyperparameters</a></li>
    <li><a href="#compute-resources">Compute resources</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#references">References</a></li>
  </ol>
</details>


## Dataset used

## Model description

## Hyperparameters

## Compute resources

## Results

## References
The training codes are largely taken from [nanoGPT](https://github.com/karpathy/build-nanogpt), MoE modules are taken from [nanoMoE](https://github.com/wolfecameron/nanoMoE).
