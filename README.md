# build-gpt2-moe

This repository houses some of my experiments to understand how to train small (~GPT2 scale) LLMs and their Mixture-of-Experts (MoEs) variants. 

<!-- TABLE OF CONTENTS -->
<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#dataset-used">Dataset used</a></li>
    <li><a href="#model-description">Repository structure</a></li>
    <li><a href="#hyperparameters">Hyperparameters</a></li>
    <li><a href="#compute-resources">Compute resources</a></li>
    <li><a href="#results">Results</a></li>
    <li><a href="#references">References</a></li>
  </ol>
</details>


## Dataset used

## Model description

## Hyperparameters

## Compute resources

## Results

## References
