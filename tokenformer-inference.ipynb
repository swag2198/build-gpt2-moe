{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f837c-2897-4a0a-855a-d4c76978d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.2.4\n",
      "matplotlib version: 3.10.1\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0\n",
      "transformers version: 4.51.1\n",
      "tokenizers version: 0.21.1\n",
      "/mnt/lustre/work/bethge/mwe102/.conda/llm/bin/python\n",
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = ['numpy', 'matplotlib', 'tiktoken', 'torch', 'transformers', 'tokenizers']\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "!which python; python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047f9445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.2.4\n",
      "matplotlib version: 3.10.1\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.6.0\n",
      "transformers version: 4.51.1\n",
      "tokenizers version: 0.21.1\n",
      "/mnt/lustre/work/bethge/mwe102/.conda/llm/bin/python\n",
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = ['numpy', 'matplotlib', 'tiktoken', 'torch', 'transformers', 'tokenizers']\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "!which python; python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8066db-c024-43a4-b09a-16943e44ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os, math\n",
    "import time, inspect\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eada1bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 11 20:44:06 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   71C    P0             251W / 250W |  34209MiB / 40960MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3170917      C   ...bethge/mwe102/.conda/llm/bin/python    34196MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7166e8-aecd-4ac6-9d13-83e1544a527a",
   "metadata": {},
   "source": [
    "Need the following:\n",
    "```python\n",
    "# from norms import get_norm, get_final_norm\n",
    "# from positional_embeddings import RotaryEmbedding, apply_rotary_pos_emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c74b95-2750-4193-9917-1bf29a518907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(config):\n",
    "    eps = config.get('layernorm_epsilon', 1e-5)\n",
    "    if config['norm'] == \"layernorm_nonparam\":\n",
    "        norm = LayerNorm_NonParam\n",
    "    elif config['norm'] == \"layernorm\":\n",
    "        norm = nn.LayerNorm\n",
    "    else:\n",
    "        raise ValueError(f\"norm {config['norm']} not recognized\")\n",
    "    return norm, eps\n",
    "\n",
    "\n",
    "def get_final_norm(config):\n",
    "    eps = config.get('layernorm_epsilon', 1e-5)\n",
    "    if config['final_norm'] == \"layernorm_nonparam\":\n",
    "        norm = LayerNorm_NonParam\n",
    "    elif config['final_norm'] == \"layernorm\":\n",
    "        norm = nn.LayerNorm\n",
    "    else:\n",
    "        raise ValueError(f\"norm {config['final_norm']} not recognized\")\n",
    "    return norm, eps\n",
    "\n",
    "\n",
    "class LayerNorm_NonParam(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_channels = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, normalized_shape=(self.num_channels,), eps=self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a242811-758e-4993-b789-73046e86fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, max_seq_len, base=10000, precision=torch.half, save_inv_freqs=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "        self.precision = precision\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.dim = dim\n",
    "\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        cos_cached, sin_cached, inv_freq = self._prepare_cache(\n",
    "            max_seq_len, precision, base\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=save_inv_freqs)\n",
    "        self.cos_cached = cos_cached\n",
    "        self.sin_cached = sin_cached\n",
    "\n",
    "    def _prepare_cache(self, seq_len, precision, base):\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "\n",
    "        t = torch.arange(seq_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        cos_cached = emb.cos()[:, None, None, :]\n",
    "        sin_cached = emb.sin()[:, None, None, :]\n",
    "\n",
    "        return (\n",
    "            cos_cached.to(precision),\n",
    "            sin_cached.to(precision),\n",
    "            inv_freq.to(precision),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_dim=0, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[seq_dim]\n",
    "\n",
    "        assert seq_len <= self.max_seq_len\n",
    "\n",
    "        if seq_len != self.max_seq_len:\n",
    "            # y, z, _ = self._prepare_cache(seq_len, self.precision, self.base)\n",
    "            return (\n",
    "                self.cos_cached[:seq_len, ...].to(x.device),\n",
    "                self.sin_cached[:seq_len, ...].to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            return self.cos_cached.to(x.device), self.sin_cached.to(x.device)\n",
    "\n",
    "\n",
    "# rotary pos emb helpers:\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat(\n",
    "        (-x2, x1), dim=x1.ndim - 1\n",
    "    )  # dim=-1 triggers a bug in earlier torch versions\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n",
    "    cos, sin = (\n",
    "        cos[offset : q.shape[0] + offset, ...],\n",
    "        sin[offset : q.shape[0] + offset, ...],\n",
    "    )\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb_torch(\n",
    "    q, k, cos, sin, offset: int = 0\n",
    "):  # jitting fails with bf16\n",
    "    cos, sin = (\n",
    "        cos[offset : q.shape[0] + offset, ...],\n",
    "        sin[offset : q.shape[0] + offset, ...],\n",
    "    )\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a188905-177d-4b6c-8b4a-6e6fd1d91bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db572b9d-3703-4339-9c9e-98ad784ef866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_normalization(inputs, normalization_type, dim=-1):\n",
    "    if normalization_type == 'softmax': \n",
    "        # NOTE: softmax = exp_l1_norm\n",
    "        # outputs = F.softmax(inputs, dim=dim) * inputs.shape[dim]\n",
    "        nonlinear_outputs = torch.exp(inputs)\n",
    "        norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=1, dim=dim, keepdim=True) * inputs.shape[dim]\n",
    "        outputs = norm_outputs\n",
    "    elif normalization_type == 'gelu_l2_norm':\n",
    "        nonlinear_outputs = F.gelu(inputs)\n",
    "        norm_outputs = nonlinear_outputs / torch.norm(nonlinear_outputs, p=2, dim=dim, keepdim=True) * math.sqrt(nonlinear_outputs.shape[dim])\n",
    "        outputs = norm_outputs\n",
    "    elif normalization_type == 'l2_norm_gelu':\n",
    "        # first normalize then apply gelu\n",
    "        norm_outputs = inputs / torch.norm(inputs, p=2, dim=dim, keepdim=True) * math.sqrt(inputs.shape[dim])\n",
    "        nonlinear_outputs = F.gelu(norm_outputs)\n",
    "        outputs = nonlinear_outputs\n",
    "    else:\n",
    "        raise ValueError(\"normalization_type should be in {'softmax', 'gelu_l2_norm', 'l2_norm_gelu'}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9eb77a6-157c-4e1d-9ac3-9c7fd59f5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Pattention(nn.Module):\n",
    "#     \"\"\"Parameter-based attention layer\"\"\"\n",
    "#     def __init__(self, input_channels, output_channels, param_token_num, normalization_type):\n",
    "#         super().__init__()\n",
    "#         self.key_param_tokens = nn.Parameter(torch.empty(param_token_num, input_channels))\n",
    "#         self.value_param_tokens = nn.Parameter(torch.empty(param_token_num, output_channels))\n",
    "#         self.normalization_type = normalization_type\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # Compute attention weights using dot product between inputs and key parameters\n",
    "#         attn_weights = inputs @ self.key_param_tokens.transpose(-2, -1)\n",
    "\n",
    "#         # Apply nonlinear normalization\n",
    "#         attn_weights = nonlinear_normalization(attn_weights, self.normalization_type)\n",
    "    \n",
    "#         # Compute weighted sum with value parameters\n",
    "#         output = attn_weights @ self.value_param_tokens\n",
    "#         return output\n",
    "    \n",
    "class Pattention(nn.Module):\n",
    "    \"\"\"Parameter-based attention layer with (optional) hardened input-token attention\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 output_channels,\n",
    "                 param_token_num,\n",
    "                 normalization_type=\"l2_norm_gelu\",\n",
    "\n",
    "                 active_param_frac=None,\n",
    "                 is_ffn=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.key_param_tokens = nn.Parameter(torch.empty(param_token_num, input_channels))\n",
    "        self.value_param_tokens = nn.Parameter(torch.empty(param_token_num, output_channels))\n",
    "        self.normalization_type = normalization_type\n",
    "        self.active_param_frac = active_param_frac\n",
    "        self.is_ffn = is_ffn # is this pattention implementing the FFN block?\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute attention weights using dot product between inputs and key parameters\n",
    "        attn_weights = inputs @ self.key_param_tokens.transpose(-2, -1) # routing: selecting the right experts\n",
    "        # TODO: do this selection of right experts in a lower dimensional space assuming the vanilla method works\n",
    "        # print(f\"attn_weights shape: {attn_weights.shape}\") # BxTxd @ dxn -> BxTxn\n",
    "\n",
    "        # Apply nonlinear normalization\n",
    "        attn_weights = nonlinear_normalization(attn_weights, self.normalization_type)\n",
    "\n",
    "        if self.is_ffn and self.active_param_frac:\n",
    "            # this is now only done in the MLP layer, keep only topk per row\n",
    "            k = int(self.active_param_frac * self.value_param_tokens.shape[0])\n",
    "            before = attn_weights.count_nonzero()\n",
    "            topk, indices = torch.topk(attn_weights, k=k, dim=-1)\n",
    "            attn_weights = torch.zeros_like(attn_weights).scatter_(dim=-1, index=indices, src=topk)\n",
    "\n",
    "            # print(f\"#value parameter tokens selected: {k} out of {self.value_param_tokens.shape[0]}\")\n",
    "            # print(f\"before nonzero: {before} after nonzero: {attn_weights.count_nonzero()}, reduction: {100*((before-after)/before):.4f}\")\n",
    "\n",
    "        # Compute weighted sum with value parameters\n",
    "        output = attn_weights @ self.value_param_tokens\n",
    "        return output\n",
    "\n",
    "\n",
    "class TokenFormerBlock(nn.Module):\n",
    "    \"\"\"Main TokenFormer layer block\"\"\"\n",
    "    def __init__(self, hidden_size, qkv_slot_num, ffn_slot_num, proj_slot_num, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalization_type = config['norm_activation_type']\n",
    "        self.num_attention_heads = config['num_attention_heads']\n",
    "        self.hidden_size_per_attention_head = hidden_size // self.num_attention_heads\n",
    "        \n",
    "        # Get norm layers based on config\n",
    "        norm, eps = get_norm(config)\n",
    "        self.norm1 = norm(hidden_size, eps=eps)\n",
    "        self.norm2 = norm(hidden_size, eps=eps)\n",
    "        \n",
    "        # Self-attention with parameter tokens\n",
    "        self.query = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.key = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.value = Pattention(hidden_size, hidden_size, qkv_slot_num, self.normalization_type)\n",
    "        self.proj = Pattention(hidden_size, hidden_size, proj_slot_num, self.normalization_type)\n",
    "        \n",
    "        # FFN, optionally with hardened value parameter aggregation\n",
    "        self.ffn = Pattention(hidden_size, hidden_size, ffn_slot_num, self.normalization_type, active_param_frac=config[\"active_param_frac\"], is_ffn=True)\n",
    "\n",
    "        # Initialize rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=int(self.hidden_size_per_attention_head * 0.25),  # 25% of dimensions\n",
    "            base=10000,  # default base\n",
    "            max_seq_len=config.get('max_position_embeddings', 2048),\n",
    "            precision=torch.float16 if config.get('fp16', False) else torch.float32\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self attention\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Get Q,K,V through parameter attention\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        # Compute self-attention\n",
    "        x = self.attention(q, k, v)\n",
    "        \n",
    "        # Project and add residual connection\n",
    "        x = self.proj(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        # FFN\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ffn(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def attention(self, q, k, v):\n",
    "\n",
    "        # Reshape for attention heads\n",
    "        b, s, h = q.size()\n",
    "        q = q.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "        k = k.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "        v = v.view(b, s, self.num_attention_heads, self.hidden_size_per_attention_head)\n",
    "\n",
    "        # At this point, dimensions are:\n",
    "        # q, k: [batch_size, seq_len, num_heads, head_size]\n",
    " \n",
    "        # Apply rotary embeddings to first 25% of dimensions\n",
    "        rotary_ndims = int(self.hidden_size_per_attention_head * 0.25)\n",
    "        \n",
    "        # Split the dimensions for partial rotary\n",
    "        q_rot, q_pass = q[..., :rotary_ndims], q[..., rotary_ndims:]\n",
    "        k_rot, k_pass = k[..., :rotary_ndims], k[..., rotary_ndims:]\n",
    "        \n",
    "        # After split:\n",
    "        # q_rot, k_rot: [batch_size, seq_len, num_heads, rotary_ndims]\n",
    "        # q_pass, k_pass: [batch_size, seq_len, num_heads, (head_size - rotary_ndims)]\n",
    "\n",
    "        # Get and apply rotary embeddings\n",
    "        seq_len = q.size(1)\n",
    "        cos, sin = self.rotary_emb(q, seq_len=seq_len)\n",
    "        q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin, offset=0)\n",
    "        \n",
    "        # Recombine rotary and pass-through dimensions\n",
    "        q = torch.cat((q_rot, q_pass), dim=-1)\n",
    "        k = torch.cat((k_rot, k_pass), dim=-1)\n",
    "        \n",
    "        # Transpose for attention calculation\n",
    "        q = q.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        k = k.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        v = v.transpose(1, 2)  # [b, nh, s, hs]\n",
    "        \n",
    "        # Create causal mask with proper dimensions\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones((1, 1, seq_len, seq_len), dtype=torch.bool, device=q.device), \n",
    "            diagonal=1\n",
    "        )\n",
    "        attn_mask = torch.where(causal_mask, float('-inf'), 0.0)\n",
    "        \n",
    "        # Compute attention scores with proper scaling and normalization\n",
    "        scale = 1 / math.sqrt(self.hidden_size_per_attention_head)\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) * scale  # [b, nh, s, s]\n",
    "        attn_weights = attn_weights + attn_mask\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        x = attn_weights @ v  # [b, nh, s, hs]\n",
    "\n",
    "        # Reshape and return\n",
    "        x = x.transpose(1, 2).contiguous().view(b, s, h)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TokenFormer(nn.Module):\n",
    "    \"\"\"TokenFormer model for inference\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        qkv_slot_num=768,\n",
    "        ffn_slot_num=3072,\n",
    "        proj_slot_num=768,\n",
    "        max_position_embeddings=2048,\n",
    "        config=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        # TokenFormer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TokenFormerBlock(\n",
    "                hidden_size=hidden_size,\n",
    "                qkv_slot_num=qkv_slot_num,\n",
    "                ffn_slot_num=ffn_slot_num,\n",
    "                proj_slot_num=proj_slot_num,\n",
    "                config=config\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Get final norm based on config\n",
    "        final_norm, final_eps = get_final_norm(config)\n",
    "        self.norm_f = final_norm(hidden_size, eps=final_eps)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "\n",
    "        # initialize and report number of parameters\n",
    "        self.initialize_parameters()\n",
    "        print(\"init and number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        since all trainable tokens are in nn.Parameter containers, they need\n",
    "        to be initialized by hand as pytorch won't do it\n",
    "        \"\"\"\n",
    "        std = 0.02\n",
    "        for name, param in self.named_parameters():\n",
    "            if ('weight' in name) or ('param_tokens' in name):\n",
    "                if param.dim() >= 2:\n",
    "                    torch.nn.init.normal_(param, mean=0.0, std=0.02)\n",
    "                else:\n",
    "                    torch.nn.init.zeros_(param) # for 1d bias like params (if any)\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param)\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        Note that here they are not doing weight tying for some reason, i.e.,\n",
    "        `self.word_embeddings.weight` is not same as `self.output.weight`\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.word_embeddings.weight.numel()\n",
    "        return n_params\n",
    "        \n",
    "    def forward(self, input_ids, targets=None):\n",
    "        # Embeddings\n",
    "        hidden_states = self.word_embeddings(input_ids)\n",
    "        \n",
    "        # Forward through layers\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states)\n",
    "            \n",
    "        # Final norm and output projection\n",
    "        hidden_states = self.norm_f(hidden_states)\n",
    "        logits = self.output(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_size):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # just pick out params that require grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups -- all 2d params will be weight decayed, biases and layernorms no decay\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        \n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd5a38c5-572d-457d-b709-4afccd951974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from tokenizer import build_tokenizer\n",
    "\n",
    "def load_config(model_type='150M'):\n",
    "    assert model_type in {'150M', '450M', '900M', '1-5B'}\n",
    "    config_path = f'../tokenformer-minimal/config/{model_type}_eval.yml'\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "model_size = '150M'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "# Initialize tokenizer\n",
    "# tokenizer = build_tokenizer(\n",
    "#     tokenizer_type=config['tokenizer-type'],\n",
    "#     vocab_file=config['vocab-file'],\n",
    "#     padding_multiple=128\n",
    "# )\n",
    "\n",
    "# regular gpt2 tokenizer used in nanoGPT\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e206c7a-f83a-41a6-80d2-8edc748c1f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([15496, 11, 314, 1101, 257, 3303, 2746, 11],\n",
       " \"Hello, I'm a language model,\",\n",
       " ' coup. T top�pri until.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"Hello, I'm a language model,\"), enc.decode([15496, 11, 314, 1101, 257, 3303, 2746, 11]), enc.decode([12092, 13, 309, 1353, 247, 3448, 1566, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dca92f-e407-4b23-bbfe-fac98e8d3b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e03bd3d-7b05-459b-b6ee-09e7eba25eb4",
   "metadata": {},
   "source": [
    "## Model container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059a5d90-4b87-4225-97ae-76c7a20a13c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "init and number of parameters: 1587.22M\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "# device = \"cpu\" # OVERRIDE\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model_size = '1-5B'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "config['active_param_frac'] = None\n",
    "model = TokenFormer(\n",
    "    vocab_size = 50304,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01300cc0-2319-4e36-b59e-3dd11dcf2b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenFormer(\n",
       "  (word_embeddings): Embedding(50304, 1536)\n",
       "  (layers): ModuleList(\n",
       "    (0-39): 40 x TokenFormerBlock(\n",
       "      (norm1): LayerNorm_NonParam()\n",
       "      (norm2): LayerNorm_NonParam()\n",
       "      (query): Pattention()\n",
       "      (key): Pattention()\n",
       "      (value): Pattention()\n",
       "      (proj): Pattention()\n",
       "      (ffn): Pattention()\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (output): Linear(in_features=1536, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f4c7955-1f4b-48eb-8153-ea6645739733",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1704f1a-888e-4a02-b2cd-255a90e4be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeddings.weight: torch.Size([50304, 1536])\n",
      "layers.0.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.0.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.0.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.1.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.1.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.1.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.2.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.2.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.2.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.3.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.3.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.3.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.4.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.4.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.4.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.5.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.5.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.5.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.6.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.6.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.6.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.7.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.7.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.7.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.8.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.8.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.8.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.9.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.9.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.9.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.10.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.10.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.10.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.11.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.11.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.11.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.12.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.12.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.12.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.13.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.13.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.13.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.14.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.14.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.14.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.15.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.15.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.15.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.16.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.16.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.16.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.17.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.17.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.17.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.18.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.18.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.18.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.19.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.19.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.19.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.20.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.20.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.20.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.21.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.21.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.21.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.22.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.22.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.22.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.23.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.23.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.23.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.24.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.24.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.24.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.25.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.25.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.25.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.26.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.26.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.26.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.27.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.27.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.27.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.28.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.28.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.28.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.29.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.29.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.29.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.30.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.30.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.30.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.31.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.31.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.31.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.32.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.32.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.32.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.33.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.33.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.33.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.34.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.34.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.34.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.35.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.35.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.35.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.36.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.36.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.36.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.37.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.37.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.37.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.38.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.38.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.38.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "layers.39.query.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.query.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.key.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.key.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.value.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.value.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.proj.key_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.proj.value_param_tokens: torch.Size([1536, 1536])\n",
      "layers.39.ffn.key_param_tokens: torch.Size([6144, 1536])\n",
      "layers.39.ffn.value_param_tokens: torch.Size([6144, 1536])\n",
      "norm_f.weight: torch.Size([1536])\n",
      "norm_f.bias: torch.Size([1536])\n",
      "output.weight: torch.Size([50304, 1536])\n",
      "init and number of parameters: 1304.10M\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_size = '1-5B'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "config['active_param_frac'] = None\n",
    "model = TokenFormer(\n",
    "    vocab_size = 50304,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "# model = model.to(device)\n",
    "\n",
    "param_count = 0\n",
    "active_frac = 0.25\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "    if 'ffn.value_param_tokens' in name:\n",
    "        param_count += param.numel() * active_frac\n",
    "    elif 'word_embeddings'in name:\n",
    "        pass\n",
    "    else:\n",
    "        param_count += param.numel()\n",
    "\n",
    "\n",
    "\n",
    "print(\"init and number of parameters: %.2fM\" % (param_count/1e6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bab4868c-8ce9-4933-b356-0863e7650935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80ed56-af06-422f-b42f-41e232d72032",
   "metadata": {},
   "source": [
    "## 1. check fineweb val-loss on randomly initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d60f505-4b1f-453f-a359-63826b6cc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with data root: edu_fineweb10B found: 99 shards for: train split and num processes: 1\n",
      "with data root: edu_fineweb10B found: 1 shards for: val split and num processes: 1\n"
     ]
    }
   ],
   "source": [
    "## test on val data\n",
    "def load_tokens(filename: str):\n",
    "    # expects .npy file\n",
    "    npt = np.load(filename)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank=0, num_processes=1, split='train', data_root='edu_fineweb10B'):\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filename\n",
    "        data_root = data_root\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f\"with data root: {data_root} found: {len(shards)} shards for: {split} split and num processes: {self.num_processes}\")\n",
    "\n",
    "        # state management\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # useful in val_loader.reset()\n",
    "        # state, initialize at shard 0 or first file\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        # prepare inputs and targets for a single *step* of the optimization\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "        x = buf[:-1].view(B, T) # remove last token\n",
    "        y = buf[1:].view(B, T)  # remove first token\n",
    "        # advance to the next chunk of the array\n",
    "        self.current_position += (B * T * self.num_processes)\n",
    "        # check for next batch loading for all processes\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B*T*self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y\n",
    "\n",
    "# initialize the dataloader\n",
    "B = 32    # micro batch size: how many rows we are processing in a single forward-backward step (16 fits nicht in one A100 40GB)\n",
    "T = 1024 # sequence length\n",
    "train_loader = DataLoaderLite(B=B, T=T, split='train', data_root='edu_fineweb10B')\n",
    "val_loader = DataLoaderLite(B=B, T=T, split='val', data_root='edu_fineweb10B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b744b642-cba3-42d1-828c-73c70ca0919d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>The Independent Jane\\nFor all the love, romance and scandal in Jane Austen’s books, what'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader.reset()\n",
    "x, y = val_loader.next_batch()\n",
    "enc.decode(x.flatten().tolist())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65faa6be-b3f0-42ec-b085-c5310791f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches, print_loss=True) -> float:\n",
    "    model.eval()\n",
    "    data_loader.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_accum = 0.0\n",
    "        loss_steps = num_batches\n",
    "        for _ in range(loss_steps):\n",
    "            x, y = data_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "            loss = loss / loss_steps\n",
    "            # print(f'loss at step {_}: {loss.detach().item():.4f}')\n",
    "            loss_accum += loss.detach()\n",
    "    \n",
    "    if print_loss:\n",
    "        # averaged per-step loss, averaged over `num_batches` batches or steps\n",
    "        print(f\"Validation loss: {loss_accum.item():.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    return loss_accum.item()\n",
    "\n",
    "def generate_and_print_samples(model, tokenizer, device,\n",
    "                               num_return_sequences = 4,\n",
    "                               max_length = 32,\n",
    "                               start_context = \"Hello, I'm a language model,\",\n",
    "                               random_seed = 42\n",
    "                               ):\n",
    "    \n",
    "    model.eval()\n",
    "    encoder = None\n",
    "    decoder = None\n",
    "    if hasattr(tokenizer, 'encode'):\n",
    "        encoder = tokenizer.encode\n",
    "        decoder = tokenizer.decode\n",
    "    elif hasattr(tokenizer, 'tokenize'):\n",
    "        encoder = tokenizer.tokenize\n",
    "        decoder = tokenizer.detokenize\n",
    "    else:\n",
    "        raise ValueError(f\"Please pass a tokenizer with either encode/decode or tokenize/detokenize methods\")\n",
    "\n",
    "    \n",
    "    tokens = encoder(start_context)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "    xgen = tokens.to(device)\n",
    "    # don't interfere with other seeds\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(random_seed)\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            # print(logits.shape, logits.dtype)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()\n",
    "        decoded = decoder(tokens)\n",
    "        print(f\"sample {i}: {decoded}\")\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52f97447-4c76-4a85-8bcd-a7831bb586af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model,K3*5\"+0,?7EI;1=&01A(#3H&\n",
      "sample 1: Hello, I'm a language model,97B*#5OHM#A5/LB%Q#4:1QI<\n",
      "sample 2: Hello, I'm a language model,/N:5=/B<M.=O)%%A:DJCP*&9\n",
      "sample 3: Hello, I'm a language model,'7J%8($/+K;'D.L3FA/K4E87\n",
      "Validation loss: 10.8125\n",
      "Val loss for randomly initialized tokenformer 150M: 10.812499046325684\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, enc, device, random_seed=42)\n",
    "print(f\"Val loss for randomly initialized tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb5191c-4060-45b0-9dbd-e17e15cf6717",
   "metadata": {},
   "source": [
    "Observation: It is comparable to GPT2 loss of around 10, so everything going good so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "649af681-9d84-4234-ac6b-7c08ef00dcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenFormer(\n",
       "  (word_embeddings): Embedding(50304, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TokenFormerBlock(\n",
       "      (norm1): LayerNorm_NonParam()\n",
       "      (norm2): LayerNorm_NonParam()\n",
       "      (query): Pattention()\n",
       "      (key): Pattention()\n",
       "      (value): Pattention()\n",
       "      (proj): Pattention()\n",
       "      (ffn): Pattention()\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f523ad9-596f-4ce1-846f-ed2e894b3eb4",
   "metadata": {},
   "source": [
    "## 2. load my pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cff6925-6815-41ec-897a-e37448ff742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./checkpoints/model_and_optimizer_150M_20250408_102234.pth\", weights_only=True)\n",
    "old_state_dict = checkpoint[\"model_state_dict\"]\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Strip the '_orig_mod.' prefix from keys because I saved optimized model!\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in old_state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_key = k[len(\"_orig_mod.\"):]\n",
    "    else:\n",
    "        new_key = k\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "# new_state_dict\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9a7b618-6756-43ae-bf5c-aff14bc72637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenFormer(\n",
       "  (word_embeddings): Embedding(50304, 768)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TokenFormerBlock(\n",
       "      (norm1): LayerNorm_NonParam()\n",
       "      (norm2): LayerNorm_NonParam()\n",
       "      (query): Pattention()\n",
       "      (key): Pattention()\n",
       "      (value): Pattention()\n",
       "      (proj): Pattention()\n",
       "      (ffn): Pattention()\n",
       "      (rotary_emb): RotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6134cb06-9d4f-431c-924a-288a22c247b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model, and I would like you to write more text books than I would a second class. But once they have completed the book\n",
      "sample 1: Hello, I'm a language model, so my only question is, “What does translation involve,” or, in fact, as we'll see\n",
      "sample 2: Hello, I'm a language model, so I use this way since I was a lot of people are not comfortable with it, and I'm not going to\n",
      "sample 3: Hello, I'm a language model, and would like to see more examples of this if you have any problems. I have had these problems but I think maybe\n",
      "Validation loss: 3.4512\n",
      "Val loss for pretrained tokenformer 150M: 3.4512038230895996\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, enc, device, random_seed=42)\n",
    "print(f\"Val loss for pretrained tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5cfaa45-2d1a-4131-aa5d-693286b0bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model, you can go from there, and find out that you're in there if you want to know about the function\n",
      "for\n",
      "sample 1: Hello, I'm a language model, I'm a programmer - but how can you help them out of the box? [email protected]\n",
      "When I'm\n",
      "sample 2: Hello, I'm a language model, but I only use it with language models that don't have this. But I'll be having a good explanation. If\n",
      "sample 3: Hello, I'm a language model, this is my new game, It's like this:\n",
      "I'd see this game work as 'a puzzle' for\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, enc, device, random_seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54683a71-77ea-423d-b838-3a61da045711",
   "metadata": {},
   "source": [
    "## TODO: Now I need to replace all the MLP layer by sparse/hard aggregation of value tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c07f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 9.3448\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.1: 9.34477424621582\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 8.9835\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.2: 8.98345947265625\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 8.5225\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.30000000000000004: 8.522541999816895\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 8.2211\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.4: 8.221060752868652\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 7.9623\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.5: 7.96226167678833\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 7.5799\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.6000000000000001: 7.579946994781494\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 6.5625\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.7000000000000001: 6.5625128746032715\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 4.3612\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.8: 4.361184120178223\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 3.5761\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 0.9: 3.5760817527770996\n",
      "init and number of parameters: 151.88M\n",
      "Validation loss: 3.4512\n",
      "Val loss for pretrained tokenformer 150M at ffn active frac: 1.0: 3.4512038230895996\n"
     ]
    }
   ],
   "source": [
    "fracs = np.linspace(0, 1, 11)[1:]\n",
    "print(fracs)\n",
    "\n",
    "for frac in fracs:\n",
    "    config['active_param_frac'] = frac\n",
    "    model = TokenFormer(\n",
    "        vocab_size = 50304,\n",
    "        hidden_size = config['hidden_size'],\n",
    "        num_layers = config['num_layers'],\n",
    "        qkv_slot_num = config['qkv_slot_num'],\n",
    "        ffn_slot_num = config['ffn_slot_num'],\n",
    "        proj_slot_num = config['proj_slot_num'],\n",
    "        max_position_embeddings = config['max_position_embeddings'],\n",
    "        config=config\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.train();\n",
    "\n",
    "    print(f\"Val loss for pretrained tokenformer {model_size} at ffn active frac: {frac}: {calc_loss_loader(val_loader, model, device, num_batches=50)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cff8a3",
   "metadata": {},
   "source": [
    "## TODO: count active parameters when doing the sparse value token aggregation\n",
    "- if the number of active parameters is close to `150M`, the performance of our `150M` parameter model would be the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfda5a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init and number of parameters: 151.88M\n",
      "word_embeddings.weight: torch.Size([50304, 768])\n",
      "layers.0.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.0.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.0.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.0.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.0.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.0.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.0.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.0.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.0.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.0.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.1.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.1.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.1.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.1.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.1.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.1.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.1.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.1.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.1.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.1.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.2.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.2.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.2.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.2.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.2.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.2.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.2.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.2.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.2.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.2.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.3.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.3.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.3.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.3.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.3.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.3.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.3.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.3.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.3.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.3.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.4.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.4.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.4.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.4.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.4.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.4.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.4.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.4.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.4.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.4.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.5.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.5.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.5.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.5.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.5.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.5.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.5.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.5.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.5.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.5.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.6.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.6.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.6.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.6.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.6.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.6.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.6.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.6.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.6.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.6.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.7.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.7.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.7.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.7.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.7.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.7.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.7.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.7.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.7.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.7.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.8.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.8.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.8.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.8.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.8.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.8.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.8.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.8.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.8.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.8.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.9.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.9.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.9.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.9.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.9.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.9.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.9.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.9.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.9.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.9.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.10.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.10.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.10.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.10.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.10.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.10.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.10.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.10.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.10.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.10.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "layers.11.query.key_param_tokens: torch.Size([768, 768])\n",
      "layers.11.query.value_param_tokens: torch.Size([768, 768])\n",
      "layers.11.key.key_param_tokens: torch.Size([768, 768])\n",
      "layers.11.key.value_param_tokens: torch.Size([768, 768])\n",
      "layers.11.value.key_param_tokens: torch.Size([768, 768])\n",
      "layers.11.value.value_param_tokens: torch.Size([768, 768])\n",
      "layers.11.proj.key_param_tokens: torch.Size([768, 768])\n",
      "layers.11.proj.value_param_tokens: torch.Size([768, 768])\n",
      "layers.11.ffn.key_param_tokens: torch.Size([3072, 768])\n",
      "layers.11.ffn.value_param_tokens: torch.Size([3072, 768])\n",
      "norm_f.weight: torch.Size([768])\n",
      "norm_f.bias: torch.Size([768])\n",
      "output.weight: torch.Size([50304, 768])\n",
      "init and number of parameters: 130.65M\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_size = '150M'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "config['active_param_frac'] = None\n",
    "model = TokenFormer(\n",
    "    vocab_size = 50304,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "# model = model.to(device)\n",
    "\n",
    "param_count = 0\n",
    "active_frac = 0.25\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n",
    "    if 'ffn.value_param_tokens' in name:\n",
    "        param_count += param.numel() * active_frac\n",
    "    elif 'word_embeddings'in name:\n",
    "        pass\n",
    "    else:\n",
    "        param_count += param.numel()\n",
    "\n",
    "\n",
    "print(\"init and number of parameters: %.2fM\" % (param_count/1e6,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106146dc",
   "metadata": {},
   "source": [
    "## TODOs next:\n",
    "- setup ddp script so that we could train atleast 150M and 450M models on atleast 10B tokens\n",
    "- setup hellaswag eval script (tokenformer paper also mentions lambada, arc-e, arc-c, winograde)\n",
    "- also train a small (150M) version of tokenformer at different sparsity levels (0.25, 0.5, 0.75) to see if that helps us achieve anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e019f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab0d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be31a80-5fe8-40fe-861b-41fa9176a96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b873da-d83d-4ace-be65-b1a984fc4942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56047a61-8d21-407b-af1d-0ff4687d221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c625f7-3bbe-4da6-ae24-33b47f5e1535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31bd7-5899-4ed8-8b9e-b207639b241e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5f12c-2868-463a-93a4-d313bc4cdb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56ee52-f962-4536-b83e-044fa3a938a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339e5ff-027b-4cec-9ace-452d9917fc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180b8ea-4c3c-4797-a295-a1f748f19657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869d3d1-6c34-47ec-b86a-05dca7158df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203db96-45c2-4ccd-a032-9c9af97ec8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50a810-0a29-4fd4-a910-a6c5cb6a440d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6795657-7be5-407f-afb0-e44407720fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336250a-31cc-4efd-8eb7-e6f24df201b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ce877-a0f7-4b0f-8532-c5da19cc895d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931ed4f-cc49-4f9a-8b1b-bef6e4762744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d8ee8c-8660-4409-8c99-39194e1ecf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb6446-7dac-4a5d-a8bf-3c6b634e7dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea17ef-dc21-477a-b4db-f40123a0dca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba3adf-c2f1-4049-8ad1-454ae95a1ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96d53c56-3540-4d59-a4ae-b6b9769f27f6",
   "metadata": {},
   "source": [
    "Observation: It is comparable to GPT2 loss of around 10, so everything going good so far!\n",
    "\n",
    "## 2. load official pre-trained weights and check its val-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed4760f3-8f30-41ae-af46-870f27ee0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(num_layers):\n",
    "    # Create mapping for model weights\n",
    "    remap_dict = {\n",
    "        'sequential.0.word_embeddings': 'word_embeddings'\n",
    "    }\n",
    "    \n",
    "    # Map layers\n",
    "    for i in range(2, num_layers + 2):\n",
    "        new_idx = i - 2\n",
    "        remap_dict[f'sequential.{i}.attention'] = f'layers.{new_idx}'\n",
    "        remap_dict[f'sequential.{i}.mlp'] = f'layers.{new_idx}.ffn'\n",
    "    \n",
    "    # Map final norm\n",
    "    remap_dict[f'sequential.{num_layers + 3}.norm'] = 'norm_f'\n",
    "    \n",
    "    return remap_dict\n",
    "\n",
    "\n",
    "def load_model(model, weights_path, num_layers):\n",
    "    \"\"\"Load model weights from state dict\"\"\"\n",
    "    state_dict = torch.load(weights_path, weights_only=True)\n",
    "    \n",
    "    remap_dict = create_mapping(num_layers)\n",
    "    \n",
    "    # Copy output weight from word embeddings if missing\n",
    "    if 'output.weight' not in state_dict:\n",
    "        state_dict['output.weight'] = state_dict['sequential.0.word_embeddings.weight']\n",
    "    \n",
    "    # Rename keys in-place by replacing prefixes\n",
    "    for old_prefix, new_prefix in remap_dict.items():\n",
    "        for key in list(state_dict.keys()):\n",
    "            if key.startswith(old_prefix):\n",
    "                new_key = key.replace(old_prefix, new_prefix, 1)\n",
    "                state_dict[new_key] = state_dict.pop(key)\n",
    "    \n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa3bda43-a791-4a1b-b0a6-d7eb3c9d51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init and number of parameters: 151.88M\n",
      "Loading pre-trained tokenformer 150M weights on cuda...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = TokenFormer(\n",
    "    vocab_size = tokenizer.padded_vocab_size,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Loading pre-trained tokenformer {model_size} weights on {device}...\")\n",
    "model_bin_file = f'../tokenformer-minimal/pytorch_model_{model_size}.bin'\n",
    "load_model(model, model_bin_file, config['num_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "281ff13f-a7c0-494c-adfb-b52f32be5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7785ab7e-b53f-4e99-9b6f-7dafd6866e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 6.4403\n",
      "Val loss for pre-trained tokenformer 150M: 6.440320014953613\n"
     ]
    }
   ],
   "source": [
    "# generate_and_print_samples(model, tokenizer, device)\n",
    "print(f\"Val loss for pre-trained tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=20)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58195d53-b469-4715-bb56-cec96eb220fe",
   "metadata": {},
   "source": [
    "Observation: Hmm, this looks a bit suspicious! I think GPT2 124M had a validation loss of around $3.24$ on the same set. Not sure if everythin is working correctly.\n",
    "- I made sure to compute the validation loss on a tokenformer tokenized version of fineweb, and not the GPT2 tiktoken one.\n",
    "- I need to check what they report in the paper, may be I need to check on the owt dataset instead of fineweb!\n",
    "- \n",
    "### Using pre-trained weights from Tokenformer is a pain!\n",
    "- It is using a different tokenizer, not the tiktoken gpt2 tokenizer!\n",
    "- I created another version of the fineweb tokenized data using this tokenizer to test this model!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73708d92-ae6c-461f-89df-032b25c625a3",
   "metadata": {},
   "source": [
    "# Pre-training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec775e-6a35-43e2-9725-cb8cdbf66e06",
   "metadata": {},
   "source": [
    "## Dataloader for the fineweb dataset (assuming shards are already available)\n",
    "- since I am pretraining from scratch, I will just use the original GPT2 fwt data tokenized with tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d21ae3ad-cff7-4f72-83c2-5e38099dbd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "# device = \"cpu\" # OVERRIDE\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "589da6b0-a8d9-4724-9dcb-4338cd5432b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288 tokens\n",
      "=> calculated gradient accumulation steps: 64\n",
      "with data root: edu_fineweb10B found: 99 shards for: train split and num processes: 1\n",
      "with data root: edu_fineweb10B found: 1 shards for: val split and num processes: 1\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524_288 # 2**19, closest power of 2 to ~0.5M\n",
    "B = 8    # micro batch size: how many rows we are processing in a single forward-backward step (16 fits nicht in one A100 40GB)\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T) == 0, \"total batch size in number of tokens should be divisible by B*T\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size: {total_batch_size} tokens\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "# initialize the dataloader\n",
    "train_loader = DataLoaderLite(B=B, T=T, split='train', data_root='edu_fineweb10B')\n",
    "val_loader = DataLoaderLite(B=B, T=T, split='val', data_root='edu_fineweb10B')\n",
    "\n",
    "# initialize tokenizer for sampling through the model\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c03a1fc4-7ee6-442e-8067-67e607f5c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tf32, now matmuls will use tf32 (tensor cores from A100)\n",
    "torch.set_float32_matmul_precision('high') # default is highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b41c5d-1ace-4a85-af2d-1ca291aaeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # prev constant lr that we were using\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 100\n",
    "max_steps = 6001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cad1eba-c4c4-49fe-8096-e2e2373081e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # linear warmup\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # if it > lr decay iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1, goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f09aed-869e-460f-8c82-1887a7db6a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAGdCAYAAAASUnlxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVlElEQVR4nO3de1zUVf4/8NfcRxBGBLl5QVBT8Q4ogqLZBS9dtJvYBbXSopui1Zq6fW377S66u7XVqpjX1EzdliwrLbAUMUZURPKCdxRFEPEygyjMDHN+fxhjBCFDMB9gXs/HYx67fTjz+bw/B+v99pzP+RyZEEKAiIiIyAnIpQ6AiIiIyFFY+BAREZHTYOFDREREToOFDxERETkNFj5ERETkNFj4EBERkdNg4UNEREROg4UPEREROQ2l1AE0JVarFRcuXICbmxtkMpnU4RAREVEdCCFQUlICf39/yOW1j+mw8PmVCxcuoGPHjlKHQURERPVw7tw5dOjQodY2LHx+xc3NDcCtjnN3d5c4GiIiIqoLo9GIjh072vJ4bVj4/Erl9Ja7uzsLHyIiomamLo+p8OFmIiIichosfIiIiMhpsPAhIiIip8HCh4iIiJwGCx8iIiJyGix8iIiIyGmw8CEiIiKnwcKHiIiInAYLHyIiInIa9Sp8Fi9ejMDAQGi1WoSGhiItLa3W9qmpqQgNDYVWq0VQUBCWLFlSrU1SUhKCg4Oh0WgQHByMTZs21eu6OTk5ePjhh6HT6eDm5obBgwcjLy+vPrdJRERELYzdhc/GjRsRHx+PuXPnIisrC1FRURg9evTvFhe5ubkYM2YMoqKikJWVhTlz5mDatGlISkqytdHr9YiJiUFsbCyys7MRGxuL8ePHIyMjw67rnjp1CkOHDkWPHj2wY8cOZGdn4+2334ZWq7X3NomIiKgFkgkhhD1fCA8PR0hICBITE23HevbsiXHjxiEhIaFa+1mzZmHz5s3IycmxHYuLi0N2djb0ej0AICYmBkajEVu3brW1GTVqFDw8PLB+/fo6X3fChAlQqVRYu3atPbdkYzQaodPpYDAYuFcXERFRM2FP/rZrk1KTyYTMzEy89dZbVY5HR0cjPT29xu/o9XpER0dXOTZy5EisWLECZrMZKpUKer0eM2bMqNbmgw8+qPN1rVYrvv32W/zpT3/CyJEjkZWVhcDAQMyePRvjxo2rMbby8nKUl5fb/tloNN6xDxrDlVITPkk/g5smC1QKOZQKOVRyGVRKOVqpFHBvpYS7VgX3Viq4a1XQtVLBq7UaSgUf0SIiIrKHXYVPcXExKioq4OPjU+W4j48PCgsLa/xOYWFhje0tFguKi4vh5+f3u20qz1mX6xYVFeH69euYP38+/vrXv2LBggX47rvv8Oijj2L79u0YPnx4tdgSEhLwl7/8xZ4uaBQb9ubhox9O2PUdmQzwaq2Br7sWPu4a+Lhr0cHDBYFergj0ckWApwu0KkUjRUxERNQ82VX4VPrttu9CiFq3gq+p/W+P1+WctbWxWq0AgLFjx9pGj/r374/09HQsWbKkxsJn9uzZmDlzpu2fjUYjOnbs+Lv30VgMN80AgL4ddAgLaAuL1QpzhRUmi8BNswXGmxYYy8ww3jSjpMyCazfNqLAKXCopx6WSchzMr/m8/jotgtq1Rk8/NwT7u6Onnzu6tGsNFUeKiIjISdlV+Hh5eUGhUFQb3SkqKqo2GlPJ19e3xvZKpRKenp61tqk8Z12u6+XlBaVSieDg4CptevbsiV27dtUYm0ajgUajqe2WHcJScasQHNLVC7NG9bhje6tV4HKpCReNZbhoLEOhsQwXDWU4e+UGzhSXIre4FMYyCy4YynDBUIZdJ4tt31Ur5LjLtzX6dWiD0AAPhAZ4oFNbl1oLVyIiopbCrsJHrVYjNDQUKSkpeOSRR2zHU1JSMHbs2Bq/ExERga+//rrKseTkZISFhUGlUtnapKSkVHnOJzk5GZGRkXW+rlqtxsCBA3Hs2LEq1zp+/DgCAgLsuU2HM1fcGq2q60iMXC5DOzcN2rlp0Lu9rtrPhRC4esOM3OJSnLhYgpwCI3IKSnCkwIjr5RYcyjfiUL4R6zJurYjzaq1GSCcPDOzcFkO6eqGHrxvkchZCRETU8tg91TVz5kzExsYiLCwMERERWLp0KfLy8hAXFwfg1vRRfn4+1qxZA+DWCq6FCxdi5syZmDp1KvR6PVasWGFbrQUA06dPx7Bhw7BgwQKMHTsWX331FbZt21ZlpOZO1wWAN998EzExMRg2bBhGjBiB7777Dl9//TV27NhR3/5xCFvh00DFhkwmQ1tXNdq6qhEa4GE7brUKnL96E4cvGLA/7yoyz17FoXwjiq+bkHzkIpKPXARwqxCK7OKFod28ENXNC366Vg0SFxERkdTsLnxiYmJw+fJlvPvuuygoKEDv3r2xZcsW26hKQUFBlXfrBAYGYsuWLZgxYwYWLVoEf39/fPTRR3jsscdsbSIjI7Fhwwb8+c9/xttvv40uXbpg48aNCA8Pr/N1AeCRRx7BkiVLkJCQgGnTpqF79+5ISkrC0KFD69U5jmKy3JrqUikb99kbuVyGTp4u6OTpgtF9/AAAZeYKHL5gwL4zV7H79GXsPn0FxddN2Jx9AZuzLwAAevi64f5gH0QH+6J3e3dOixERUbNl93t8WjKp3uMzfUMWvjpwAW8/GIznhwY67Lo1MVms2J93FbtOFCPtZDEOnr8G66/+hPjptLivpw+ie/kgIsiTS+qJiEhyjfYeH2oct5/xkX4kRa2UY3CQJwYHeeKNkd1xtdSE7ceKkHz4InaeuIQCQxnW7j6LtbvPwqu1Gg/08cPD/dsjpFMbjgQREVGTx8KnCbBNdTXB0RMPVzUeDemAR0M6oMxcgfRTxUg+fOt5oOLrJqzWn8Vq/Vl08GiFsf39Ma5/e3TzcZM6bCIiohqx8GkCLFb7VnVJRatS4J4ePrinhw/+3zgrdp0sxuYDF5B8uBDnr97Eou2nsGj7KYR0aoMJAzvhwX5+cFHzjxgRETUdzEpNQFOa6qorlUKOEd29MaK7N26aKvDD0Yv4MusCth8rwv68a9ifdw3vfnMED/Xzx5ODOqJPex2nwoiISHIsfJoAcxOe6qqLVmoFHuzrjwf7+qOopAz/yzyPjXvP4ezlG1i/Jw/r9+Shd3t3PBsZiAf7+UGj5FYaREQkjeaZaVsYk50vMGzKvN20ePnurtj++t34bGo4xvb3h1opx6F8I17/PBtD5m/Hv1OOo6ikTOpQiYjICXHEpwm4/YxPy5kKkstliOzihcguXphXasL6PXlYqz+LQmMZPvzhBBJ3nMKDff0wJSoIwf6Oe3UAERE5t+Y/xNACVE51qVvAiE9N2rqq8cqIrkibNQL/eXIAQjq1ganCii+y8jHmozQ8u2oP9p65InWYRETkBDji0wRUPtzc0l8GqFLI8VA/fzzUzx/Z565h+a5cfPvzBWw/dgnbj13CoM5t8fKILhh+Vzs+CE1ERI2iZWfaZsLUDFd1/VH9OrbBf54cgB9fvxtPDuoItUKOPWeuYPKqvXjwP7vw3aFC8KXiRETU0Fj4NAGWiua9quuP6OzlioRH+2Lnn0ZgytBAuKgVOHzBiLhPM/Hwwp+w/VgRCyAiImowzpdpmyBzC1rVVV++Oi3+/GAwfpp1D14d0RWuagUO5hvw7Kq9eHyJHumniqUOkYiIWgDnzbRNiDNOdf0eD1c13hjZHTv/NAIvDAuCRilH5tmreGpZBp5athtZeVelDpGIiJoxFj5NgDNPdf0ez9YazBnTE2l/GoFJEQFQKWRIP3UZjyxOx6uf7ce5KzekDpGIiJohZtomoHKqS63kr+O3vN21+MvY3tj+xt14PLQDZDLgm58LcO97qfj7lhwYbpqlDpGIiJoRZlqJWa0CFuutER+lnFNdv6eDhwv+9UQ/fPPaUAzp6glThRVLd57G8H9ux6qfcmGyWKUOkYiImgEWPhIzW28nbBVHfO6ol78Onz4fjlWTB6Kbd2tcu2HGX74+glEf7MTO45ekDo+IiJo4ZlqJVT7fA7TcNzc3NJlMhhE9vLF1ehT+9khveLVW43RxKSau3IO4tZnIv3ZT6hCJiKiJYqaVWOXzPQAfbraXUiHH0+EB+PGNu/HskM5QyGX47nAh7n1vBxb+eALllgqpQyQioiaGmVZilUvZZTJAwWd86sVdq8K8h3rh22lDMSiwLcrMVvwr+ThG/nsnth8rkjo8IiJqQlj4SMzMpewNpoevOza+MBgfTugPbzcNzly+gWdX7cUrn+1HUUmZ1OEREVETwGwrMUvlUnYWPg1CJpNhbP/2+OH14ZgyNBAKuQzf/lyA+95Lxca9edz+gojIyTHbSuz2zuyc5mpIbloV/vxgML56ZQh6t3eHscyCWUkH8eSy3cgtLpU6PCIikggLH4mZLJzqaky92+vw5ctDMHdMT2hVcuw+fQUjP9iJRdtPVnmwnIiInAOzrcQsVk51NTalQo6pw4KQHD8cUd28YLJY8c/vj2Hswp9wtNAodXhERORAzLYSM3ODUofp5OmCNc8Nwvvj+6GNiwpHCox46D+7sGj7SduzVkRE1LKx8JFY5VSXkiM+DiGTyfBoSAckzxiG+3r6wFwh8M/vj+GxJXqcLLoudXhERNTImG0ldnvEh78KR/J202LZxFD864l+cNMqkX3uGh74KA3L007DauXKLyKilorZVmK3n/HhVJejyWQyPB56a/QnqpsXyi1W/PXbHExYuhvnrtyQOjwiImoELHwkxlVd0vPTtcKa5wbh74/0gatagT1nrmDMh2n46kC+1KEREVEDY7aVGN/j0zTIZDI8Fd4J38UPQ0inNigpt2D6hgOY+d8DuF5ukTo8IiJqICx8JMZnfJqWjm1d8N8XIzDt3m6Qy4Av9udjzIdpyMq7KnVoRETUAJhtJWb5Za8uvsen6VAq5Jh5/13Y+GIE2rdphbwrN/D4Ej0W/ngCFXzwmYioWWO2lZiJU11N1sDObbFlehQe6uePCqvAv5KP46llu1Fo4IanRETNFQsfiXGqq2nTtVLhown98d4T/eCqViAj9woe+CgNaScuSR0aERHVA7OtxDjV1fTJZDI8FtoB30yLQrCfOy6XmjBx5R68n3KcU19ERM0Ms63ETBzxaTYCvVzxxcuReCq8E4QAPvrhBGJXZKCohFNfRETNBbOtxLicvXnRqhT4+yN98EFMf7ioFUg/dRkPfLQL6aeKpQ6NiIjqgIWPxPiMT/M0bkB7bH51KO7yaY1LJeV4ZnkG/vPDCW53QUTUxDHbSsz2jI+Sv4rmpqt3a3z1ylA8EdoBVgG8l3IcL6zdB2OZWerQiIjodzDbSuz2Mz6c6mqOWqkV+OcT/fCPx/tCrZRjW04Rxi78CSculkgdGhER1YCFj8Rsz/jI+atozsaHdcT/4iLgr9Mit7gUYxf9hC0HC6QOi4iIfoPZVmKc6mo5+nZog69fG4qIIE/cMFXg5XX7MX/rUS55JyJqQphtJcaprpbFs7UGa58fhKlRgQCAJamnMHnVHlwtNUkcGRERASx8JGf+ZcSHU10th1Ihx9wHgvHhhP7QquRIO1GMhxbuQk6BUerQiIicHrOtxMyWX0Z8ONXV4ozt3x6bXh6CTm1dcP7qTTyWmI7kw4VSh0VE5NTqlW0XL16MwMBAaLVahIaGIi0trdb2qampCA0NhVarRVBQEJYsWVKtTVJSEoKDg6HRaBAcHIxNmzbZfd3JkydDJpNV+QwePLg+t+gwFuutwkfNqa4WqaefOza/OgRDut567ufFTzOxaPtJCMHnfoiIpGB34bNx40bEx8dj7ty5yMrKQlRUFEaPHo28vLwa2+fm5mLMmDGIiopCVlYW5syZg2nTpiEpKcnWRq/XIyYmBrGxscjOzkZsbCzGjx+PjIwMu687atQoFBQU2D5btmyx9xYdyvTLVBdfYNhytXFR45NnByF2cACEAP75/THM/G82yswVUodGROR0ZMLOv3qGh4cjJCQEiYmJtmM9e/bEuHHjkJCQUK39rFmzsHnzZuTk5NiOxcXFITs7G3q9HgAQExMDo9GIrVu32tqMGjUKHh4eWL9+fZ2vO3nyZFy7dg1ffvmlPbdkYzQaodPpYDAY4O7uXq9z2OvJpbuhP30ZHz05AA/383fINUk6a/Vn8M7XR1BhFRjQqQ0+jg2Ft5tW6rCIiJo1e/K3XcMMJpMJmZmZiI6OrnI8Ojoa6enpNX5Hr9dXaz9y5Ejs27cPZrO51jaV57Tnujt27IC3tzfuuusuTJ06FUVFRb97P+Xl5TAajVU+jlb5Hh9OdTmH2IjOWPPcIOhaqZCVdw3jFv6EQ/kGqcMiInIadhU+xcXFqKiogI+PT5XjPj4+KCys+aHNwsLCGttbLBYUFxfX2qbynHW97ujRo7Fu3Tr8+OOPeO+997B3717cc889KC8vrzG2hIQE6HQ626djx4516IWGZbZyqsvZDOnqhS9fGYKgdq64YCjDE0v0+O4QX3ZIROQI9cq2MlnV0QkhRLVjd2r/2+N1Oeed2sTExOCBBx5A79698dBDD2Hr1q04fvw4vv322xrjmj17NgwGg+1z7ty5372HxmJb1cXCx6kEerli08tDENXNCzfNFYj7dD+W7jzFh56JiBqZXdnWy8sLCoWi2uhOUVFRtdGYSr6+vjW2VyqV8PT0rLVN5Tnrc10A8PPzQ0BAAE6cOFHjzzUaDdzd3at8HM22ZQWnupyOrpUKqyYPxMSIAADA37ccxdtfHYLllz8TRETU8OwqfNRqNUJDQ5GSklLleEpKCiIjI2v8TkRERLX2ycnJCAsLg0qlqrVN5Tnrc10AuHz5Ms6dOwc/P7+63aAELL9Mdak54uOUlAo5/vJwL7z9YDBkMuDT3Xl4YW0mSsstUodGRNQi2Z1tZ86cieXLl2PlypXIycnBjBkzkJeXh7i4OAC3po8mTpxoax8XF4ezZ89i5syZyMnJwcqVK7FixQq88cYbtjbTp09HcnIyFixYgKNHj2LBggXYtm0b4uPj63zd69ev44033oBer8eZM2ewY8cOPPTQQ/Dy8sIjjzxS3/5pdCZOdTk9mUyG54cGIvHpEGiUcvx4tAjjP9bjorFM6tCIiFoeUQ+LFi0SAQEBQq1Wi5CQEJGammr72aRJk8Tw4cOrtN+xY4cYMGCAUKvVonPnziIxMbHaOT///HPRvXt3oVKpRI8ePURSUpJd171x44aIjo4W7dq1EyqVSnTq1ElMmjRJ5OXl1fm+DAaDACAMBkOdv/NHDfxrigiY9Y04lH/NYdekpivz7BUR8m6yCJj1jYj4+zZxtMAodUhERE2ePfnb7vf4tGRSvMdnwLvJuHrDjJQZw9DNx80h16Sm7ezlUjy7ai9OF5fCTaNE4jOhGNrNS+qwiIiarEZ7jw81PAvf3Ey/EeDpii9ejsSgzm1RUm7B5FV78Pk+x684JCJqiZhtJWaq4CalVF0bFzXWThmEh/v5w2IVePN/P2Phjye43J2I6A9itpVY5XJ2lZzL2akqjVKBD2L646W7uwAA/pV8HO9sPowKK4sfIqL6YuEjoQqrQGUO41QX1UQul2HWqB6Y99Ct5e6r9WcxbX0Wyi3c4JSIqD6YbSVk/tWL6jjVRbV5dkggPpowACqFDN8eLMCzq/aipMwsdVhERM0Os62EqhQ+fHMz3cFD/fyxavIguKoVSD91GTEf70ZRCd/1Q0RkDxY+EjJX3H5WQyXnr4LubGg3L2x8MQJerdU4UmDEY4npOFNcKnVYRETNBrOthCr3ZFLIZZDz4Waqo97tdUh6KRIBni44d+UmHktMx8HzBqnDIiJqFlj4SMi2lJ3TXGSnAE9X/C8uEr3bu+NyqQkTlurx08liqcMiImryWPhIqHKqi9NcVB/t3DTY8EIEhnT1RKmpAs+u2ovkw4VSh0VE1KQx40rIzJcX0h/UWqPEyskDMaqXL0wVVry0bj++2H9e6rCIiJosZlwJmTnVRQ1Ao1Rg4VMD8HhoB1RYBWb+Nxtr9GekDouIqEli4SMhM/fpogaiVMjxj8f6YnJkZwDA/311GIu2n+QWF0REv8GMK6HbIz78NdAfJ5fLMO+hYEy7txsA4J/fH8P8rUdZ/BAR/QozroTMFk51UcOSyWSYef9d+PMDPQEAH+88jTmbDnJ/LyKiX7DwkZDZyqkuahxTooKw4LE+kMuA9XvOYfqGLJgs1jt/kYiohWPGldDtER/+GqjhxQzshP88GQKVQoZvfi7Ai2v3oczMzU2JyLkx40qIq7qosT3Q1w9LJ4ZBq5Jj+7FLeH71XtwwWaQOi4hIMix8JMSpLnKEEd29sfrZW5ub/nTyMiav3Ivr5Sx+iMg5MeNKiFNd5CjhQZ5Y83w43DRK7DlzBbErMmC4aZY6LCIih2PGlRCnusiRQgM8sG5qOHStVMjKu4Znlmfg2g2T1GERETkUCx8J8T0+5Gh9O7TBZ1PD0dZVjYP5Bjy5LAOXr5dLHRYRkcMw40qIb24mKfTy12HDC4Ph1VqDnAIjJizdjaKSMqnDIiJyCGZcCXHEh6Ryl48bNr44GD7uGpwouo4JH+9GoYHFDxG1fMy4EuIzPiSlLu1a478vRqB9m1Y4XVyK8R/rcf7qDanDIiJqVCx8JGTiVBdJLMDTFRtfHIxObV2Qd+UGYj7ejbOXS6UOi4io0TDjSsjCqS5qAjp4uGDji4MR5OWK/Gs38eRSFj9E1HIx40qIU13UVPjpWmHDi4PRpZ0rLhjK8OTS3ci7zGkvImp5WPhIiKu6qCnxdtNi/Qu3i58JS/UsfoioxWHGlRBXdVFTw+KHiFo6ZlwJ2QofJae6qOnwdtNi/dRfTXst47QXEbUcLHwkZJvqkvPXQE2Lt/ut4ieo3S8PPC/bjXNXWPwQUfPHjCshEx9upibM212LDb8qfiYsZfFDRM0fCx8J2ZazK/lroKaJxQ8RtTTMuBLiqi5qDmzFjxeLHyJq/phxJcT3+FBz4e2uxYYXWPwQUfPHwkdCJguXs1Pz4e1+a6n7r4uf/Gs3pQ6LiMguzLgSslg51UXNi88vxU/gL8XPU8t246KRu7oTUfPBjCshTnVRc+TjrsVnU8PRsW0rnL18A08t241LJeVSh0VEVCcsfCTEqS5qrvx0rfDZlMHw12lx6lIpnlmegSulJqnDIiK6I2ZcCXGqi5qzjm1d8NnUwfB20+DYxRI8szwDhhtmqcMiIqoVM66EuFcXNXedvVzx2dTB8GqtxpECIyauzEBJGYsfImq6mHElZLbwGR9q/rp6t8a6KYPh4aJC9nkDJq/ai9Jyi9RhERHViIWPhEx8gSG1EN193bD2+XC4a5XIPHsVz6/ei5umCqnDIiKqhhlXQhYrp7qo5ejdXoc1z4ejtUaJ3aev4IW1+1BmZvFDRE1LvTLu4sWLERgYCK1Wi9DQUKSlpdXaPjU1FaGhodBqtQgKCsKSJUuqtUlKSkJwcDA0Gg2Cg4OxadOmP3TdF198ETKZDB988IHd9+colVNdahY+1EL079gGnzw7EC5qBdJOFOPldfttqxeJiJoCuzPuxo0bER8fj7lz5yIrKwtRUVEYPXo08vLyamyfm5uLMWPGICoqCllZWZgzZw6mTZuGpKQkWxu9Xo+YmBjExsYiOzsbsbGxGD9+PDIyMup13S+//BIZGRnw9/e39/YcqnKvLiWf8aEWJKxzW6yYNBAapRw/Hi3Ca+v32x7kJyKSmkwIIez5Qnh4OEJCQpCYmGg71rNnT4wbNw4JCQnV2s+aNQubN29GTk6O7VhcXByys7Oh1+sBADExMTAajdi6dautzahRo+Dh4YH169fbdd38/HyEh4fj+++/xwMPPID4+HjEx8fX6d6MRiN0Oh0MBgPc3d3r1iH1JIRA4OwtAIC9c+9DOzdNo16PyNF2Hr+EKav3wVRhxYN9/fBBTH8oObpJRI3Anvxt13+FTCYTMjMzER0dXeV4dHQ00tPTa/yOXq+v1n7kyJHYt28fzGZzrW0qz1nX61qtVsTGxuLNN99Er1697Lk1h6uw3q43OdVFLdGwu9oh8ZkQqBQyfPNzAd764iCsVrv+nkVE1ODsyrjFxcWoqKiAj49PleM+Pj4oLCys8TuFhYU1trdYLCguLq61TeU563rdBQsWQKlUYtq0aXW6n/LychiNxiofR6mc5gI41UUt1709ffCfJwdALgP+l3ke735zBHYOMhMRNah6DTXIZFUTtRCi2rE7tf/t8bqcs7Y2mZmZ+PDDD/HJJ5/UGsuvJSQkQKfT2T4dO3as0/cagulXzzxwVRe1ZKN6++Gfj/cDAHySfgb/Sj4mcURE5MzsyrheXl5QKBTVRneKioqqjcZU8vX1rbG9UqmEp6dnrW0qz1mX66alpaGoqAidOnWCUqmEUqnE2bNn8frrr6Nz5841xjZ79mwYDAbb59y5c3XriAZgqVL4cMSHWrbHQjvg/429Nf28aPspLN5xUuKIiMhZ2VX4qNVqhIaGIiUlpcrxlJQUREZG1vidiIiIau2Tk5MRFhYGlUpVa5vKc9blurGxsfj5559x4MAB28ff3x9vvvkmvv/++xpj02g0cHd3r/JxFLPt5YWyOo9QETVnsRGd8dboHgCAf3x3DGv0Z6QNiIicktLeL8ycOROxsbEICwtDREQEli5diry8PMTFxQG4NYqSn5+PNWvWALi1gmvhwoWYOXMmpk6dCr1ejxUrVthWawHA9OnTMWzYMCxYsABjx47FV199hW3btmHXrl11vq6np6dtBKmSSqWCr68vunfvbn/PNLLK5b1KOae5yHnEDe+C62UWLNx+Ev/31WG4qJV4PLSD1GERkROxu/CJiYnB5cuX8e6776KgoAC9e/fGli1bEBAQAAAoKCio8m6dwMBAbNmyBTNmzMCiRYvg7++Pjz76CI899pitTWRkJDZs2IA///nPePvtt9GlSxds3LgR4eHhdb5uc2Oq4D5d5Jxej74L18st+CT9DP70v2y4qhUY3cdP6rCIyEnY/R6flsyR7/E5VliCkR/shFdrNfb9+f5GvRZRU2O1CsxK+hmfZ56HSiHD0olhGNHdW+qwiKiZarT3+FDDMVdwny5yXnK5DPMf64sH+vrBXCEQtzYTu09fljosInICzLoSqZzq4jt8yFkp5DL8e3x/3NPDG+UWK6as3ofsc9ekDouIWjgWPhKp3KCUIz7kzNRKORY/HYKIIE9cL7dg4so9OFrouBeJEpHzYdaViOWXV/dzuwpydlqVAssmhaF/xzYw3DTjmeV7kFtcKnVYRNRCMetKhFNdRLe11iix+tlB6OHrhuLr5Xh62W7kX7spdVhE1AKx8JEIp7qIqtK5qLD2+XAEebnigqEMTy/bjaKSMqnDIqIWhllXIpVTXSx8iG5r56bBp1PC0b5NK5y5fAMTV+yB4YZZ6rCIqAVh1pVI5XJ2PuNDVJV/m1ZYNyUc7dw0OFpYgmc/2YMbJovUYRFRC8GsKxGThc/4EP2ezl6uWPPcILhrldifdw0vrs1EuaVC6rCIqAVg4SOR25uU8ldAVJOefu5Y9ewgtFIpkHaiGDM3ZqPCyhfNE9Efw6wrEYuVU11EdxIa4IGPY0OhUsjw7cECzN10ENxlh4j+CGZdiZgs3KSUqC6G3dUOH04YALkM2LD3HOZ/d1TqkIioGWPhI5HKqS4lR3yI7mhMHz/8/ZE+AICPU08jcccpiSMiouaKWVciFm5SSmSXCYM6Yc6YHgCABd8dxWcZeRJHRETNEbOuRG4vZ+dUF1FdvTCsC16+uwsAYO6XB/F19gWJIyKi5oaFj0RMnOoiqpc3R3bH0+GdIAQwY+MB7DhWJHVIRNSMMOtKxMypLqJ6kclkeHdsbzzUzx8Wq0Dcp5nYd+aK1GERUTPBrCsRC6e6iOpNIZfhvSf64e7u7VBmtuLZT/biyAWj1GERUTPAwkciJr7AkOgPUSvlSHw6FGEBHigps2Diyj3ILS6VOiwiauKYdSVSOdXFZ3yI6q+VWoEVkwci2M8dxdfL8czyDBQYbkodFhE1Ycy6Ern9jA+nuoj+CF0rFVY/NwiBXq7Iv3YTsSv24EqpSeqwiKiJYuEjEcsvU11qJX8FRH9UOzcN1j4/CL7uWpwsuo7Jq/bgejl3dCei6ph1JWLiqi6iBtXBwwWfThkEDxcVfj5vwNTV+1Bm5o7uRFQVs65EbM/4yDnVRdRQunq7YfVzg9Bao4T+9GW8tj7LtoKSiAhg4SMZTnURNY6+Hdpg2cQwqJVypBy5iD8l/QyrlTu6E9EtzLoS4VQXUeOJ6OKJRU+FQCGX4Yv9+Xj3myMQgsUPEbHwkQynuoga1/3BPvjn430BAJ+kn8FHP5yUOCIiagpY+EjEtpydU11EjebRkA6Y91AwAODf247jk59yJY6IiKTGrCsR2zM+nOoialTPDglE/H3dAADvfH0Em7LOSxwREUmJWVcifMaHyHGm39sNkyM7AwDe+PxnbDtyUdqAiEgyzLoSub1lBZ/xIWpsMpkM//dgMB4d0B4VVoGXP9uP3acvSx0WEUmAhY9EzBZOdRE5klwuw4LH++K+nj4wWayYsnofDuUbpA6LiByMWVciFiunuogcTaWQY+FTAzA4qC2ul9/a0f1k0XWpwyIiB2LWlYjJwk1KiaSgVSmwbGIY+rTX4UqpCRNXZCD/Gnd0J3IWLHwkYv5lVRdHfIgcz02rwifPDkSXdq64YChD7PIMFF8vlzosInIAZl2JcKqLSFqerTVY+3w42rdphdPFpZi0cg+MZWapwyKiRsasKwEhxK9GfDjVRSQV/zatsPb5QfB0VePwBSOmcEd3ohaPhY8EKoseAFByxIdIUkHtWmP1c4PgplFiT+4VvLJuv+11E0TU8jDrSuDX/1HlcnYi6fVur8PySWHQKOX44WgR3vw8mzu6E7VQzLoSsPxqxIdTXURNQ3iQJxKfCYFSLsOXBy7gL18f5o7uRC0QCx8JVG5XIZMBCu7OTtRk3NPDB++N7weZDFitP4t/bzshdUhE1MBY+EjAtjO7XA6ZjIUPUVMytn97/OXhXgCAj344gRW7uKM7UUvCwkcCtsKH01xETdLEiM54/f67AAD/75sjSMrkju5ELQULHwnYlrIr2f1ETdWr93TF80MDAQB/SvoZyYcLJY6IiBoCM68EbDuzy9n9RE2VTCbD3DE98XhoB1RYBV5dn4X0U8VSh0VEfxAzrwQqCx81p7qImjS5XIb5j/ZBdPCtHd2nrt6Hn89fkzosIvoD6lX4LF68GIGBgdBqtQgNDUVaWlqt7VNTUxEaGgqtVougoCAsWbKkWpukpCQEBwdDo9EgODgYmzZtsvu677zzDnr06AFXV1d4eHjgvvvuQ0ZGRn1usVFxqouo+VAq5PjoyQGICPJEqakCk1buwcmiEqnDIqJ6sjvzbty4EfHx8Zg7dy6ysrIQFRWF0aNHIy8vr8b2ubm5GDNmDKKiopCVlYU5c+Zg2rRpSEpKsrXR6/WIiYlBbGwssrOzERsbi/Hjx1cpWupy3bvuugsLFy7EwYMHsWvXLnTu3BnR0dG4dOmSvbfZqG4/3MzCh6g50KoUWDYpDP066HD1hhnPLN+D81dvSB0WEdWDTNj5hq7w8HCEhIQgMTHRdqxnz54YN24cEhISqrWfNWsWNm/ejJycHNuxuLg4ZGdnQ6/XAwBiYmJgNBqxdetWW5tRo0bBw8MD69evr9d1AcBoNEKn02Hbtm24995773hvle0NBgPc3d3v2L6+0k5cQuyKPejh64bv4oc12nWIqGFdKTVh/Md6nCy6jkAvV/z3xQi0c9NIHRaR07Mnf9s15GAymZCZmYno6Ogqx6Ojo5Genl7jd/R6fbX2I0eOxL59+2A2m2ttU3nO+lzXZDJh6dKl0Ol06NevX41tysvLYTQaq3wcwfaMD6e6iJqVtq5qrH1+ENq3aYXcX3Z0N9zkju5EzYldmbe4uBgVFRXw8fGpctzHxweFhTUv9SwsLKyxvcViQXFxca1tKs9pz3W/+eYbtG7dGlqtFv/+97+RkpICLy+vGmNLSEiATqezfTp27HiHHmgYt3dmZ+FD1Nz46Vrh0ynh8GqtxpECI6as3oubJu7oTtRc1Cvz/vZtw0KIWt9AXFP73x6vyznr0mbEiBE4cOAA0tPTMWrUKIwfPx5FRUU1xjV79mwYDAbb59y5c797Dw2JLzAkat4CvVxv7eiuVWLvmat4aV0mTBbu6E7UHNhV+Hh5eUGhUFQbZSkqKqo2GlPJ19e3xvZKpRKenp61tqk8pz3XdXV1RdeuXTF48GCsWLECSqUSK1asqDE2jUYDd3f3Kh9H4MPNRM1fL38dVk4eCK1Kjh3HLuH1z7NRwR3diZo8uzKvWq1GaGgoUlJSqhxPSUlBZGRkjd+JiIio1j45ORlhYWFQqVS1tqk8Z32uW0kIgfLy8jvfnAOZLZzqImoJBnZui8RnQqGUy/B19gXM23yIO7oTNXF2Z96ZM2di+fLlWLlyJXJycjBjxgzk5eUhLi4OwK3po4kTJ9rax8XF4ezZs5g5cyZycnKwcuVKrFixAm+88YatzfTp05GcnIwFCxbg6NGjWLBgAbZt24b4+Pg6X7e0tBRz5szB7t27cfbsWezfvx9TpkzB+fPn8cQTT9S3fxqF2cqpLqKWYkR3b7wf0x8yGfDp7jy8l3xc6pCIqBZKe78QExODy5cv491330VBQQF69+6NLVu2ICAgAABQUFBQ5d06gYGB2LJlC2bMmIFFixbB398fH330ER577DFbm8jISGzYsAF//vOf8fbbb6NLly7YuHEjwsPD63xdhUKBo0ePYvXq1SguLoanpycGDhyItLQ09OrVq94d1BjMvzwLoOSID1GL8HA/fxhumvH2l4ewcPtJtHFRYUpUkNRhEVEN7H6PT0vmqPf4LNt5Gn/bkoNHBrTHv2P6N9p1iMixFm0/iX9+fwwA8I/H+2J8mGNWihI5u0Z7jw81DE51EbVML9/dBVOjbu3o/lbSz/juEHd0J2pqWPhIgA83E7VMMpkMc8b0xPiwDrAKYNr6LPx0kju6EzUlzLwS4HJ2opZLJpPh74/0wahevjBVWDF1zT5k5V2VOiwi+gUzrwT4AkOilk2pkOPDJ/tjSFdP3DBV4NlP9uL4Re7oTtQUsPCRALesIGr5NEoFlsaGoX/HNrh2w4zYFRk4d4U7uhNJjZlXApzqInIOrholVk0eiLt8WuOisRzPrMhAUUmZ1GEROTVmXglwqovIeXi4qrH2+XB08GiFs5dvIHb5HlwtNUkdFpHTYuEjARNHfIicio+7FuumhMPbTYNjF0swceUeGMvMUodF5JSYeSVg4TM+RE4nwNMV66aEo62rGgfzDXhu1V7cMFmkDovI6TDzSoBTXUTOqZuPG9Y8NwhuWiX2nb2KF9ZkosxcIXVYRE6FhY8E+HAzkfPq3V6HT54dBBe1ArtOFuPVz/bb/ptARI2PmVcCXM5O5NxCAzywfFIYNEo5tuUUYcbGA6iwcttEIkdg5pWAbcRHye4nclaRXbyw5JlQqBQyfPNzAd5K+hlWFj9EjY6ZVwK2wkfOZ3yInNmIHt74cMIAyGXA55nn8e43RyAEix+ixsTCRwImTnUR0S/G9PHDv57oBwD4JP0M/vn9MYkjImrZmHklYOFUFxH9yqMhHfDXcb0BAIt3nMKi7Scljoio5WLmlQCXsxPRbz0zOABzx/QEAPzz+2NYuStX4oiIWiYWPhLgqi4iqsnUYUGIv68bAODdb45gw548iSMianmYeSVgsvA9PkRUs+n3dsMLw4IAALM3HcRXB/IljoioZWHmlYDFyqkuIqqZTCbD7NE98MzgThACmPnfbHx/uFDqsIhaDBY+EuBUFxHVRiaT4d2He+PRkPaosAq89lkWth8rkjosohaBmVcCZk51EdEdyOUy/OOxvnigjx9MFVa8uDYTO49fkjosomaPmVcCZk51EVEdKBVyfDChP6KDfWCyWDF1zT6knyyWOiyiZo2FjwQqp7rUHPEhojtQKeRY+FQI7u3hjXKLFc+v3oeM05elDouo2WLmdbAKq7BtRqhk4UNEdaBWyrH4mRAMv6sdbpor8Owne5F59orUYRE1S8y8Dlb58kKAU11EVHcapQIfx4ZiaFcv3DBVYNLKvcjKuyp1WETNDgsfB7P8avdlPtxMRPbQqhRYNjEMg4Pa4nq5BRNX7sHB8wapwyJqVph5HaxyRRfAwoeI7NdKrcCKSQMxsLMHSsoseGZFBg5fYPFDVFfMvA5WOdUllwEKOae6iMh+rholVj07CCGd2sBw04xnlmfgaKFR6rCImgUWPg5mquA7fIjoj2utUeKT5wahXwcdrt4w4+llGThxsUTqsIiaPGZfB7NwKTsRNRB3rQprngtH7/buuFxqwpPLMnDq0nWpwyJq0ph9HaxyqkvJFV1E1AB0LiqsfS4cPf3cUXy9HE8t240zxaVSh0XUZLHwcTBOdRFRQ/NwVePT5wfhLp/WuGgsx4Slu5HL4oeoRsy+DmbhBqVE1Ag8W2uwbspgdPNujUJjGWI+1nPai6gGzL4OVjnVpVay64moYbVz02D9C4PR3ccNRSW3Rn5OFrH4Ifo1Zl8Hq5zqUnIpOxE1Aq/WGnw2NRw9fN1wyVb8cLUXUSUWPg5m5lQXETUyz9YafDZ1sO2B5wlLd+M4l7oTAWDh43CWyoebOdVFRI2orasan00JR7CfO4qvm/Dk0t04Vsjih4jZ18Fsz/hwOTsRNTIPVzU+m/rr9/zs5hueyemx8HEw0y9TXUo5u56IGl8bFzXWPT8YfdrrcKX01sjPkQssfsh5Mfs6WOUmpZzqIiJH0bmo8OmUcNv2Fk8t382NTclpMfs6mMXKqS4icjxdKxXWTglH/45tcO2GGU8ty8ChfBY/5HxY+DgYp7qISCruWhXWPD8IA37Z1f3JZbuxP++q1GERORSzr4NxqouIpHRrY9NBGNjZAyVlFsQuz4D+1GWpwyJyGGZfB6uc6lJxqouIJOKmVWH1c4MwtKsXSk0VmLxqD3YcK5I6LCKHYOHjYJUvMFTzBYZEJCEXtRLLJ4Xh3h7eKLdYMXXNPnx/uFDqsIgaXb2y7+LFixEYGAitVovQ0FCkpaXV2j41NRWhoaHQarUICgrCkiVLqrVJSkpCcHAwNBoNgoODsWnTJruuazabMWvWLPTp0weurq7w9/fHxIkTceHChfrcYqMx/TLVpeSIDxFJTKtSYElsKB7o4wdzhcDL6/bjqwP5UodF1KjsLnw2btyI+Ph4zJ07F1lZWYiKisLo0aORl5dXY/vc3FyMGTMGUVFRyMrKwpw5czBt2jQkJSXZ2uj1esTExCA2NhbZ2dmIjY3F+PHjkZGRUefr3rhxA/v378fbb7+N/fv344svvsDx48fx8MMP23uLjaryBYbcsoKImgKVQo4PJ/THoyHtUWEViN94AP/de07qsIgajUwIIez5Qnh4OEJCQpCYmGg71rNnT4wbNw4JCQnV2s+aNQubN29GTk6O7VhcXByys7Oh1+sBADExMTAajdi6dautzahRo+Dh4YH169fX67oAsHfvXgwaNAhnz55Fp06d7nhvRqMROp0OBoMB7u7ud2xfH3/fkoOlO0/jxWFBmD2mZ6Ncg4jIXlarwNtfHcK6jFt/mfzLw70wKbKztEER1ZE9+duuYQeTyYTMzExER0dXOR4dHY309PQav6PX66u1HzlyJPbt2wez2Vxrm8pz1ue6AGAwGCCTydCmTZsaf15eXg6j0Vjl09gqp7o44kNETYlcLsNfx/XGlKGBAIB5mw9jSeopiaMianh2Zd/i4mJUVFTAx8enynEfHx8UFtb8UFxhYWGN7S0WC4qLi2ttU3nO+ly3rKwMb731Fp566qnfrf4SEhKg0+lsn44dO/7OnTecyqkuPuNDRE2NTCbD3Ad6Yto9XQEA87cexfspx2HnxABRk1avYQeZrGrSFkJUO3an9r89Xpdz1vW6ZrMZEyZMgNVqxeLFi383rtmzZ8NgMNg+5841/ry25ZdVXRzxIaKmSCaTYWZ0d8wa1QMA8NEPJ/CXr4/AamXxQy2D0p7GXl5eUCgU1UZZioqKqo3GVPL19a2xvVKphKenZ61tKs9pz3XNZjPGjx+P3Nxc/Pjjj7XO9Wk0Gmg0mlruuOHd3p2dhQ8RNV0v3d0FLmoF5m0+jE/Sz8Bw04x/PN6Xf2mjZs+uP8FqtRqhoaFISUmpcjwlJQWRkZE1ficiIqJa++TkZISFhUGlUtXapvKcdb1uZdFz4sQJbNu2zVZYNSUmTnURUTMxKbIzPojpD6Vchk1Z+XhxbSZumiqkDovoD7G7dJ85cyaWL1+OlStXIicnBzNmzEBeXh7i4uIA3Jo+mjhxoq19XFwczp49i5kzZyInJwcrV67EihUr8MYbb9jaTJ8+HcnJyViwYAGOHj2KBQsWYNu2bYiPj6/zdS0WCx5//HHs27cP69atQ0VFBQoLC1FYWAiTyVTf/mlwXM5ORM3JuAHtsWxiGLQqOX48WoSJKzNguGmWOiyi+hP1sGjRIhEQECDUarUICQkRqamptp9NmjRJDB8+vEr7HTt2iAEDBgi1Wi06d+4sEhMTq53z888/F927dxcqlUr06NFDJCUl2XXd3NxcAaDGz/bt2+t0XwaDQQAQBoOhbh1RD8+t2iMCZn0jNu7Ja7RrEBE1tD25l0Xved+JgFnfiFEf7BQXjTelDonIxp78bfd7fFoyR7zHJ3ZFBtJOFOPfMf3wyIAOjXINIqLGkFNgROyKPSi+Xo4ATxd8+nw4OrZ1kTososZ7jw/9cbbl7HJ2PRE1Lz393JH0UgQ6tm2Fs5dv4LHEdBwtbPz3nxE1JGZfBzNzOTsRNWMBnq5IiotED183FJWUY/wSPTLPXpE6LKI6Y/Z1MEvlcnYlV3URUfPk7a7FxhciEBrgAWOZBU8vz8C2IxelDouoTlj4OJiJIz5E1ALoXFRY+/wgjOjeDmVmK15Yuw/rMs5KHRbRHTH7Ohif8SGilsJFrcTSiWEYH9YBVgHM3XQI7yUf4xYX1KQx+zoYp7qIqCVRKeRY8FhfTL+3GwDgPz+exJv/+9n2lzyipoaFj4Px4WYiamlkMhlm3H8X5j/aBwq5DP/LPI/nV+9DablF6tCIqmH2dTATp7qIqIWaMKgTlk0MRSuVAjuPX0LMUj2KSsqkDouoCmZfBzNzqouIWrB7evhgwwuD4emqxqF8Ix5dnI5Tl65LHRaRDQsfB7NwqouIWrh+Hdsg6aVIBHi64PzVm3g8MR37zvBdP9Q0MPs6mImblBKRE+js5YqklyLRr2MbXL1hxlPLMvBlVr7UYRGx8HEkIcTt5ewKTnURUcvm1VqD9VPDMbKXD0wVVsRvPIB/pxzncneSFAsfB6qwClT++67miA8ROQEXtRKJT4fixeFBAIAPfziB6RsOoMxcIXFk5KyYfR3IYr39txxOdRGRs5DLZZg9uicWPNYHSrkMm7Mv4OnlGSi+Xi51aOSEmH0dyPSrF3pxqouInE3MwE5Y89wguGuVyDx7FeMW/YQTF0ukDoucDAsfBzJbbhc+Kr7Hh4icUGRXL2x6ZYhtxdeji9ORduKS1GGRE2H2daDKqS6lXAa5nCM+ROScurRrjU0vD8Ggzm1RUm7B5FV7sTr9DB96Jodg4eNAJguXshMRAUBbVzXWThmERwe0R4VVYN7mw3gr6SDKLXzomRoXM7ADcSk7EdFtGqUC743vhzljekAuAzbuO4cnl+7mNhfUqFj4OFDlBqVcyk5EdItMJsMLw7pg5eSBcNMqsT/vGh7+z0/IPndN6tCohWIGdiAz39pMRFSju7t746tXhqBLO1cUGsvwxMd6bMo6L3VY1AIxAzuQrfDhBqVERNUEtWuNL18Zgnt7eMNksWLGxmz8fUsOKqx86JkaDgsfB6qc6uJSdiKimrlpVVg2MQyvjugKAFi68zQmr9qDK6UmiSOjloIZ2IE41UVEdGdyuQxvjOyORU+FoJVKgbQTxXjwozQc4HM/1ACYgR2IU11ERHX3QF8/bHolEoFerrhgKMMTS9KxdvdZvu+H/hAWPg5UOdWl5FQXEVGd9PB1x1evDsGoXr4wVwi8/eUhzPxvNm6YLFKHRs0UM7ADVY74cDk7EVHduWtVSHwmBHPH9IRCLsOmrHw8sigdpy9dlzo0aoaYgR2IU11ERPUjk8kwdVgQPpsSjnZuGhy7WIKHF/6E7w4VSB0aNTMsfBzItqqLIz5ERPUSHuSJb18bikGd2+J6uQVxn+7HO5sPo8zMrS6obpiBHci2ZQWf8SEiqjdvdy3WTQ3Hi8OCAACfpJ/Bo4vTcYpTX1QHzMAOZHvGh1NdRER/iEohx+wxPbFq8kC0dVXjSIERD/1nF5Iy+bZnqh0LHwfiVBcRUcMa0cMbW6dHISLIEzdMFXj982zM3HgA18u56otqxgzsQHyBIRFRw/Nx1+LTKeF4/f67IJcBX2Tl46H/7MKhfIPUoVETxAzsQGZLZeHDqS4iooakkMvw2r3dsPHFCPjptMgtLsWji9Pxceop7vVFVbDwcSCO+BARNa6Bndti6/QoRAf7wFRhRcLWo3hy6W6cu3JD6tCoiWAGdiCzlc/4EBE1tjYuanwcG4oFj/WBq1qBPWeuYPSHafh83zlud0EsfBypcqpLyakuIqJGJZPJEDOwE7ZOH4awAA9cL7fgzf/9jLhPM3H5ernU4ZGEWPg4ELesICJyrE6eLtj4YgT+NKo7VAoZvj98ESM/SMMPORelDo0kwgzsQJzqIiJyPIVchpfv7oovXxmCu3xao/h6OZ5fvQ8zNx7A1VKT1OGRgzEDO9DtVV3sdiIiR+vlr8PmV4fihWFBtmXv9/87FVsPcr8vZ8IM7EC3V3XxGR8iIiloVQrMGdMTSS9Fopt3axRfN+Gldfvx0qeZKCopkzo8cgAWPg7ENzcTETUNAzp54JtpQ/HaPV2hlMuw9VAh7n9/J77Yf54rv1o4ZmAH4nt8iIiaDo1Sgdeju+OrV4egl787DDfNmPnfbExcuQdnikulDo8aCTOwA3Gqi4io6enlr8OXrwzBmyO7Q62UI+1EMaI/2IkPth1HmblC6vCogbHwcSBOdRERNU0qhRyvjOiK7+OHIaqbF0wWKz7YdgKjP0xD2olLUodHDaheGXjx4sUIDAyEVqtFaGgo0tLSam2fmpqK0NBQaLVaBAUFYcmSJdXaJCUlITg4GBqNBsHBwdi0aZPd1/3iiy8wcuRIeHl5QSaT4cCBA/W5vUZj4lQXEVGTFujlijXPDcJ/nhwAbzcNcotLEbtiD15bn4WLRj783BLYnYE3btyI+Ph4zJ07F1lZWYiKisLo0aORl5dXY/vc3FyMGTMGUVFRyMrKwpw5czBt2jQkJSXZ2uj1esTExCA2NhbZ2dmIjY3F+PHjkZGRYdd1S0tLMWTIEMyfP9/e23IIC6e6iIiaPJlMhof6+eOH14djcmRnyGXA19kXcO97qViSegrlFk5/NWcyYefj6+Hh4QgJCUFiYqLtWM+ePTFu3DgkJCRUaz9r1ixs3rwZOTk5tmNxcXHIzs6GXq8HAMTExMBoNGLr1q22NqNGjYKHhwfWr19v93XPnDmDwMBAZGVloX///nW+N6PRCJ1OB4PBAHd39zp/r64e+s8uHMw3YNXkgRjRw7vBz09ERA3vUL4Bc788hOxz1wAAndq6YM6YnhjZywcyGf8i2xTYk7/tGvExmUzIzMxEdHR0lePR0dFIT0+v8Tt6vb5a+5EjR2Lfvn0wm821tqk8Z32uWxfl5eUwGo1VPo2Jq7qIiJqf3u112PRSJP71RD94u2mQd+UG4j7NxFPLMnDkQuPmDWp4dmXg4uJiVFRUwMfHp8pxHx8fFBYW1vidwsLCGttbLBYUFxfX2qbynPW5bl0kJCRAp9PZPh07dqz3ueqCq7qIiJonuVyGx0M7YPsbd+PVEV2hVsqhP30ZD/4nDbO/OMiXHzYj9Rp6+O3QnhCi1uG+mtr/9nhdzmnvde9k9uzZMBgMts+5c+fqfa66sK3qUnLEh4ioOXLVKPHGyO748fXheLCvH6wCWL8nD8P/sQP/+v4YjGVmqUOkO7ArA3t5eUGhUFQbZSkqKqo2GlPJ19e3xvZKpRKenp61tqk8Z32uWxcajQbu7u5VPo3JNuIjZ+FDRNScdfBwwcKnQvB5XAT6d2yDm+YKLNx+EsP+sR1Ld57i+3+aMLsysFqtRmhoKFJSUqocT0lJQWRkZI3fiYiIqNY+OTkZYWFhUKlUtbapPGd9rtsU2QofJae6iIhagoGd22LTy5H4ODYUXb1b49oNM/6+5Sju/ucObNiTZ1vNS02H0t4vzJw5E7GxsQgLC0NERASWLl2KvLw8xMXFAbg1fZSfn481a9YAuLWCa+HChZg5cyamTp0KvV6PFStW2FZrAcD06dMxbNgwLFiwAGPHjsVXX32Fbdu2YdeuXXW+LgBcuXIFeXl5uHDhAgDg2LFjAG6NKPn6+tajexoWX2BIRNTyyGQyjOzli/t6+uCL/efxwbYTyL92E299cRCLd5zCKyO64JEBHaDmYw5Ng6iHRYsWiYCAAKFWq0VISIhITU21/WzSpEli+PDhVdrv2LFDDBgwQKjVatG5c2eRmJhY7Zyff/656N69u1CpVKJHjx4iKSnJrusKIcSqVasEgGqfefPm1em+DAaDACAMBkOd2tur59tbRcCsb0Te5dJGOT8REUnvpskilqedFiHvJouAWd+IgFnfiMiEH8Qa/RlRZrZIHV6LZE/+tvs9Pi1ZY7/Hp9vcLTBXCOhn3wM/XasGPz8RETUdN0wWfJaRh493nsalknIAgI+7Bi8O64InB3VCK7VC4ghbjkZ7jw/VnxCCU11ERE7ERa3ElKggpP1pBN55KBi+7lpcNJbj3W+OYMiCH/F+8jFbQUSOwwzsIBbr7YE1Fj5ERM5Dq1Jg8pBApP7pbvztkd7o4NEKV0pN+OjHkxiy4EfM+t/POHGxROownYbdDzdT/Zh/9WQ/X2BIROR8NEoFng4PQExYR3x/+CKWpZ3GgXPXsHHfOWzcdw53d2+HyZGdMaxbO8jlzBONhYWPg5gtHPEhIiJAqZDjgb5+GNPHF5lnr2J5Wi6+P1KIHccuYcexS+jYthWeGhSAJ8I6wKu1RupwWxwWPg5itt4e8VGykicicnoymQxhndsirHNbnL1cik/SzyAp8zzOXbmJBd8dxfspxzCqtx+eDu+E8MC23BC1gbDwcZDKqS61Qs4/vEREVEWApyvmPdQLfxrZA9/8fAHrMvJw4Nw1fJ19AV9nX0CApwvG9W+PR0PaI8DTVepwmzUWPg5SOdWl5PM9RET0O1qpFXgirCOeCOuIQ/kGfLYnD19l5ePs5Rv48IcT+PCHEwgN8MAjA9rjwb5+aOOiljrkZofv8fmVxnyPz8mi67jv/VToWqmQPS+6Qc9NREQt1w2TBSlHLiJpfz52nbiEykXCKoUMQ7p6YVQvX9wf7ANPJ34eyJ78zREfB7H88owPH2wmIiJ7uKiVGNu/Pcb2b48iYxk2Z19A0v585BQYbQ9Ez9l0EOGBnhjdxxfRwb7w1WmlDrvJYuHjIJVTXWpOdRERUT15u2sxJSoIU6KCcLLoOr47VIDvDhfiUL4R+tOXoT99Gf/31WH08HXD8LvaYfhd7RDa2QMaJd8SXYmFj4OYfnm4WckRHyIiagBdvVvj1Xu64dV7uuHclRv47lAhth4qQNa5azhaWIKjhSX4eOdpuKgViOziiYguXhjUuS16+rk5dS5i4eMglau6+PJCIiJqaB3bumDqsCBMHRaEq6UmpJ0sRuqxS0g9fgnF18uxLacI23KKAACtNUqEBHhgUGcPhHVui97tdWitcZ5ywHnuVGIW7tNFREQO4OGqxsP9/PFwP39YrQI5hUbsPF6MPbmXse/sVZSUWbDz+CXsPH4JACCTAYFerujTXoc+7XXo5a9DD183eLi2zBVjLHwc5PaIDwsfIiJyDLlchl7+t4qZl+7uggqrwNFCI/bmXsGeM1eQlXcNBYYynL5UitOXSvHVgQu277Z1VaNru9bo4u2KLu1aI9DLFf5tWsFf1wrurZTN9p10LHwcxMSpLiIikpjiV4XQ5CGBAIDi6+U4lG/AoXwDDuYbcCjfiPxrN3Gl1IQ9pbcKpN9yVSvg16YV/HRaeLqq0cZFDQ8XNTxcVWjjooabVgmNUg6tSgGtUgGNSg6NUg65TAa5TCbpqjMWPg7CqS4iImqKvFprcHd3b9zd3dt27IbJgtOXSnHq0nWc+uV/zxSXosBQhiulJpSaKnCy6DpOFl23+3pqpRzH/zq6IW/BLix8HMS2ZYWShQ8RETVtLmolerfXoXd7XbWf3TRVoMBwEwWGMly4dhNXb5hw9YYZ126YcLXUjKs3TCg1WVBmtqLcUnHrf80VKLdYIQBoJB4AYOHjILbl7NyglIiImrFWagWC2rVGULvWUodSLxx+cBA+3ExERCQ9ZmEHsT3jw6kuIiIiyTALO4jtGR+O+BAREUmGWdhB+IwPERGR9Fj4OEjlJqWc6iIiIpIOs7CDWKyc6iIiIpIas7CDcKqLiIhIeix8HIRTXURERNJjFnaQyqkuvseHiIhIOszCDnJ7OTunuoiIiKTCwsdBTL9MdSk54kNERCQZZmEH4ZYVRERE0mMWdpDby9k51UVERCQVFj4OwqkuIiIi6TELOwinuoiIiKTHLOwgt5ezc6qLiIhIKix8HKTyBYbcsoKIiEg6zMIOYtuygoUPERGRZJiFHeT2Mz6c6iIiIpIKCx8HsVRwqouIiEhqzMIOYhvx4SalREREkmEWdhDbMz5yTnURERFJhYWPg/A9PkRERNJjFnYQ2zM+nOoiIiKSDLOwg3Cqi4iISHosfByEU11ERETSYxZ2EE51ERERSY9Z2AGsVgGL9VbhwxEfIiIi6dQrCy9evBiBgYHQarUIDQ1FWlpare1TU1MRGhoKrVaLoKAgLFmypFqbpKQkBAcHQ6PRIDg4GJs2bbL7ukIIvPPOO/D390erVq1w99134/Dhw/W5xQZl/mWDUgBQ8s3NREREkrG78Nm4cSPi4+Mxd+5cZGVlISoqCqNHj0ZeXl6N7XNzczFmzBhERUUhKysLc+bMwbRp05CUlGRro9frERMTg9jYWGRnZyM2Nhbjx49HRkaGXdf9xz/+gffffx8LFy7E3r174evri/vvvx8lJSX23maDkkGGafd0RdzwLtAqFZLGQkRE5MxkQghhzxfCw8MREhKCxMRE27GePXti3LhxSEhIqNZ+1qxZ2Lx5M3JycmzH4uLikJ2dDb1eDwCIiYmB0WjE1q1bbW1GjRoFDw8PrF+/vk7XFULA398f8fHxmDVrFgCgvLwcPj4+WLBgAV588cU73pvRaIROp4PBYIC7u7s93UJEREQSsSd/2zXiYzKZkJmZiejo6CrHo6OjkZ6eXuN39Hp9tfYjR47Evn37YDaba21Tec66XDc3NxeFhYVV2mg0GgwfPvx3YysvL4fRaKzyISIiopbLrsKnuLgYFRUV8PHxqXLcx8cHhYWFNX6nsLCwxvYWiwXFxcW1tqk8Z12uW/m/9sSWkJAAnU5n+3Ts2PF3752IiIiav3o93CyTVX1AVwhR7did2v/2eF3O2VBtKs2ePRsGg8H2OXfu3O/eAxERETV/Snsae3l5QaFQVBtBKSoqqjbSUsnX17fG9kqlEp6enrW2qTxnXa7r6+sL4NbIj5+fX51i02g00Gg0td4zERERtRx2jfio1WqEhoYiJSWlyvGUlBRERkbW+J2IiIhq7ZOTkxEWFgaVSlVrm8pz1uW6gYGB8PX1rdLGZDIhNTX1d2MjIiIiJyPstGHDBqFSqcSKFSvEkSNHRHx8vHB1dRVnzpwRQgjx1ltvidjYWFv706dPCxcXFzFjxgxx5MgRsWLFCqFSqcT//vc/W5uffvpJKBQKMX/+fJGTkyPmz58vlEql2L17d52vK4QQ8+fPFzqdTnzxxRfi4MGD4sknnxR+fn7CaDTW6d4MBoMAIAwGg73dQkRERBKxJ3/bXfgIIcSiRYtEQECAUKvVIiQkRKSmptp+NmnSJDF8+PAq7Xfs2CEGDBgg1Gq16Ny5s0hMTKx2zs8//1x0795dqFQq0aNHD5GUlGTXdYUQwmq1innz5glfX1+h0WjEsGHDxMGDB+t8Xyx8iIiImh978rfd7/FpyfgeHyIiouan0d7jQ0RERNScsfAhIiIip8HCh4iIiJwGCx8iIiJyGix8iIiIyGnY9ebmlq5ygRs3KyUiImo+KvN2XRaqs/D5lZKSEgDgZqVERETNUElJCXQ6Xa1t+B6fX7Farbhw4QLc3Nxq3XS1PoxGIzp27Ihz587xHUF3wL6qO/ZV3bGv7MP+qjv2Vd01Vl8JIVBSUgJ/f3/I5bU/xcMRn1+Ry+Xo0KFDo17D3d2d/2LUEfuq7thXdce+sg/7q+7YV3XXGH11p5GeSny4mYiIiJwGCx8iIiJyGix8HESj0WDevHnQaDRSh9Lksa/qjn1Vd+wr+7C/6o59VXdNoa/4cDMRERE5DY74EBERkdNg4UNEREROg4UPEREROQ0WPkREROQ0WPg4wOLFixEYGAitVovQ0FCkpaVJHVKj27lzJx566CH4+/tDJpPhyy+/rPJzIQTeeecd+Pv7o1WrVrj77rtx+PDhKm3Ky8vx2muvwcvLC66urnj44Ydx/vz5Km2uXr2K2NhY6HQ66HQ6xMbG4tq1a418dw0rISEBAwcOhJubG7y9vTFu3DgcO3asShv21y2JiYno27ev7eVnERER2Lp1q+3n7Kffl5CQAJlMhvj4eNsx9tct77zzDmQyWZWPr6+v7efsp6ry8/PxzDPPwNPTEy4uLujfvz8yMzNtP2/y/SWoUW3YsEGoVCqxbNkyceTIETF9+nTh6uoqzp49K3VojWrLli1i7ty5IikpSQAQmzZtqvLz+fPnCzc3N5GUlCQOHjwoYmJihJ+fnzAajbY2cXFxon379iIlJUXs379fjBgxQvTr109YLBZbm1GjRonevXuL9PR0kZ6eLnr37i0efPBBR91mgxg5cqRYtWqVOHTokDhw4IB44IEHRKdOncT169dtbdhft2zevFl8++234tixY+LYsWNizpw5QqVSiUOHDgkh2E+/Z8+ePaJz586ib9++Yvr06bbj7K9b5s2bJ3r16iUKCgpsn6KiItvP2U+3XblyRQQEBIjJkyeLjIwMkZubK7Zt2yZOnjxpa9PU+4uFTyMbNGiQiIuLq3KsR48e4q233pIoIsf7beFjtVqFr6+vmD9/vu1YWVmZ0Ol0YsmSJUIIIa5duyZUKpXYsGGDrU1+fr6Qy+Xiu+++E0IIceTIEQFA7N6929ZGr9cLAOLo0aONfFeNp6ioSAAQqampQgj21514eHiI5cuXs59+R0lJiejWrZtISUkRw4cPtxU+7K/b5s2bJ/r161fjz9hPVc2aNUsMHTr0d3/eHPqLU12NyGQyITMzE9HR0VWOR0dHIz09XaKopJebm4vCwsIq/aLRaDB8+HBbv2RmZsJsNldp4+/vj969e9va6PV66HQ6hIeH29oMHjwYOp2uWfevwWAAALRt2xYA++v3VFRUYMOGDSgtLUVERAT76Xe88soreOCBB3DfffdVOc7+qurEiRPw9/dHYGAgJkyYgNOnTwNgP/3W5s2bERYWhieeeALe3t4YMGAAli1bZvt5c+gvFj6NqLi4GBUVFfDx8aly3MfHB4WFhRJFJb3Ke6+tXwoLC6FWq+Hh4VFrG29v72rn9/b2brb9K4TAzJkzMXToUPTu3RsA++u3Dh48iNatW0Oj0SAuLg6bNm1CcHAw+6kGGzZswP79+5GQkFDtZ+yv28LDw7FmzRp8//33WLZsGQoLCxEZGYnLly+zn37j9OnTSExMRLdu3fD9998jLi4O06ZNw5o1awA0jz9X3J3dAWQyWZV/FkJUO+aM6tMvv21TU/vm3L+vvvoqfv75Z+zatavaz9hft3Tv3h0HDhzAtWvXkJSUhEmTJiE1NdX2c/bTLefOncP06dORnJwMrVb7u+3YX8Do0aNt/79Pnz6IiIhAly5dsHr1agwePBgA+6mS1WpFWFgY/v73vwMABgwYgMOHDyMxMRETJ060tWvK/cURn0bk5eUFhUJRrTotKiqqVg07k8rVErX1i6+vL0wmE65evVprm4sXL1Y7/6VLl5pl/7722mvYvHkztm/fjg4dOtiOs7+qUqvV6Nq1K8LCwpCQkIB+/frhww8/ZD/9RmZmJoqKihAaGgqlUgmlUonU1FR89NFHUCqVtnthf1Xn6uqKPn364MSJE/xz9Rt+fn4IDg6ucqxnz57Iy8sD0Dz+e8XCpxGp1WqEhoYiJSWlyvGUlBRERkZKFJX0AgMD4evrW6VfTCYTUlNTbf0SGhoKlUpVpU1BQQEOHTpkaxMREQGDwYA9e/bY2mRkZMBgMDSr/hVC4NVXX8UXX3yBH3/8EYGBgVV+zv6qnRAC5eXl7KffuPfee3Hw4EEcOHDA9gkLC8PTTz+NAwcOICgoiP31O8rLy5GTkwM/Pz/+ufqNIUOGVHvdxvHjxxEQEACgmfz36g89Gk13VLmcfcWKFeLIkSMiPj5euLq6ijNnzkgdWqMqKSkRWVlZIisrSwAQ77//vsjKyrIt458/f77Q6XTiiy++EAcPHhRPPvlkjcsdO3ToILZt2yb2798v7rnnnhqXO/bt21fo9Xqh1+tFnz59mt3y0JdeeknodDqxY8eOKstpb9y4YWvD/rpl9uzZYufOnSI3N1f8/PPPYs6cOUIul4vk5GQhBPvpTn69qksI9lel119/XezYsUOcPn1a7N69Wzz44IPCzc3N9t9p9tNte/bsEUqlUvztb38TJ06cEOvWrRMuLi7i008/tbVp6v3FwscBFi1aJAICAoRarRYhISG2Zcot2fbt2wWAap9JkyYJIW4teZw3b57w9fUVGo1GDBs2TBw8eLDKOW7evCleffVV0bZtW9GqVSvx4IMPiry8vCptLl++LJ5++mnh5uYm3NzcxNNPPy2uXr3qoLtsGDX1EwCxatUqWxv21y3PPfec7d+ldu3aiXvvvddW9AjBfrqT3xY+7K9bKt8zo1KphL+/v3j00UfF4cOHbT9nP1X19ddfi969ewuNRiN69Oghli5dWuXnTb2/ZEII8cfGjIiIiIiaBz7jQ0RERE6DhQ8RERE5DRY+RERE5DRY+BAREZHTYOFDREREToOFDxERETkNFj5ERETkNFj4EBERkdNg4UNEREROg4UPEREROQ0WPkREROQ0WPgQERGR0/j//YI7Wx9DGeoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(max_steps)\n",
    "lrs = np.array([get_lr(it) for it in steps])\n",
    "\n",
    "plt.plot(steps, lrs); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6445eac-22c0-4277-a5a6-a8f33c8f74a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50304"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padded_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47cdc7d2-38a0-4608-ab86-9d9a8b436f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_size = '150M'\n",
    "config = load_config(model_type=model_size)\n",
    "\n",
    "model = TokenFormer(\n",
    "    vocab_size = tokenizer.padded_vocab_size,\n",
    "    hidden_size = config['hidden_size'],\n",
    "    num_layers = config['num_layers'],\n",
    "    qkv_slot_num = config['qkv_slot_num'],\n",
    "    ffn_slot_num = config['ffn_slot_num'],\n",
    "    proj_slot_num = config['proj_slot_num'],\n",
    "    max_position_embeddings = config['max_position_embeddings'],\n",
    "    config=config\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "use_compile = True # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
    "if use_compile:\n",
    "    model = torch.compile(model) # 4th optimization\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device) # fused update\n",
    "\n",
    "max_steps = 6001\n",
    "val_loss_steps = 20\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # once in a while evaluate on the validation set\n",
    "    if step % 100 == 0:\n",
    "        _ = calc_loss_loader(val_loader, model, device, num_batches=val_loss_steps)\n",
    "\n",
    "    # once in a while sample from the model\n",
    "    if step % 100 == 0:# and (not use_compile):\n",
    "        generate_and_print_samples(model, enc, device)\n",
    "\n",
    "    # start timer\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # gradient-accumulation\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # data loading\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        # forward-backward and step\n",
    "        # amp for 3rd optimization, just surround forward pass and loss calculation, only possible in A100\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward() # deposits gradients, i.e., += on nodes\n",
    "\n",
    "    # clip gradient norms to 1.0, returns total norm of the gradient vector\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # determine lr for this step\n",
    "    lr = get_lr(step)\n",
    "    # pytorch syntax to set the learning rate for the parameters\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # param_group is a dict\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for gpu to finish the compute and measure time\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000 # time difference for one-batch or step in miliseconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tps = tokens_processed / (t1 - t0)\n",
    "\n",
    "    # only print every 20 steps to reduce clutter\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step:4d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tps:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"logits.shape: {logits.shape}, logits dtype: {logits.dtype}, loss.dtype: {loss.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a93da17a-a032-4bfe-84da-4274a3be8cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TokenFormer(\n",
       "    (word_embeddings): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TokenFormerBlock(\n",
       "        (norm1): LayerNorm_NonParam()\n",
       "        (norm2): LayerNorm_NonParam()\n",
       "        (query): Pattention()\n",
       "        (key): Pattention()\n",
       "        (value): Pattention()\n",
       "        (proj): Pattention()\n",
       "        (ffn): Pattention()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37090a30-0e78-4a8b-9490-46275856188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 6.0000038263286916e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.1\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.95)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: True\n",
       "    lr: 6.0000038263286916e-05\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33547b-d59f-4716-a506-0518f8302454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a68e6d-9c1d-4b47-b677-21743e8463e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37d3eb-1563-4341-aac8-6f1f62852dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9983bc-54cf-4895-935a-bba77fb71083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c153365-b5f4-42ec-b514-3ea7872b3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e375d2-223c-48eb-a5e2-597213512941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run id: 20250408_101716\n"
     ]
    }
   ],
   "source": [
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f'run id: {run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42792af7-5d20-4bc6-90ee-f581d78eed3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' loading code\\ncheckpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\\n\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\n\\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nmodel.train();\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_checkpoint(model: nn.Module, optimizer=None,\n",
    "                    name='', root_dir='./checkpoint'):\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dic = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "    }\n",
    "    if optimizer is not None:\n",
    "        dic[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
    "\n",
    "    filename = os.path.join(root_dir, f'model_and_optimizer_{name}_{run_id}.pth')\n",
    "    torch.save(dic, filename)   \n",
    "\n",
    "\"\"\" loading code\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68cdc1cb-4efa-4c98-9a16-9a25f1e03c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_checkpoint(model, optimizer, name=model_size, root_dir='./checkpoints/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c94cdfd9-9400-4db9-b4aa-afc0fac054f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.4034\n",
      "Val loss for trained tokenformer 150M: 3.403391122817993\n"
     ]
    }
   ],
   "source": [
    "print(f\"Val loss for trained tokenformer {model_size}: {calc_loss_loader(val_loader, model, device, num_batches=50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9611262b-a222-4313-8429-0d2e4a1130a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'gpt2'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4d44035-c942-4549-818c-26239ac19727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: Hello, I'm a language model, so I might be a lot more complicated than C++ which is a very powerful language that is very simple for all that\n",
      "sample 1: Hello, I'm a language model, as I'm afraid it's not so much my style as our approach to doing the homework. However, if you're\n",
      "sample 2: Hello, I'm a language model, I'm doing this. If you like, it's very well.\n",
      "Anyway, what we have to do is,\n",
      "sample 3: Hello, I'm a language model, and I'm my own mother. I am a non-verbal learner. I'm also an English language learner\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_samples(model, enc, device,\n",
    "                           random_seed=torch.randint(100, (1, )).item()\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a28cfc73-c284-45bf-ad1c-86ade79c04ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): TokenFormer(\n",
       "    (word_embeddings): Embedding(50304, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TokenFormerBlock(\n",
       "        (norm1): LayerNorm_NonParam()\n",
       "        (norm2): LayerNorm_NonParam()\n",
       "        (query): Pattention()\n",
       "        (key): Pattention()\n",
       "        (value): Pattention()\n",
       "        (proj): Pattention()\n",
       "        (ffn): Pattention()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (output): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c1f2a-a431-4298-a438-388dab9aa780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49187346-73aa-4b40-bf99-52fb1adc02ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df251d-a9d7-4fe8-b786-90741b01ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411e901-a998-442a-8be8-4c3675c2a940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b970dc-81c5-434b-85f6-644ef9ee2737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a646c8-0bea-4bb7-b46b-20376e60f6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609e866-ec35-4bf0-aa09-acb13a7b440a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86131c0f-50ad-4317-921a-eed51230d76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5c44bf-8db6-492b-8d1f-83559d4d6319",
   "metadata": {},
   "source": [
    "### next todos:\n",
    "- init tokenformer\n",
    "- check tokenformer's validation losses for each size compared to gpt2\n",
    "- start `build-tokenformer-fineweb` notebook\n",
    "\n",
    "### next todos:\n",
    "- count parameters in tokenformer ffn layer\n",
    "- sparsify the ffn layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e5cd8-5bd8-4983-8100-10799ad2139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4f19-e6b0-444f-9acb-18bc1bb2ab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aeab8-7d29-481c-a6c5-e55c9c244e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9cf1a-1b7a-460c-b9f7-3d8c60073944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d178f-297a-48fc-a98a-f33263dbce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7c10-a866-4e91-ace4-27493281098b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280e46-1caa-423c-9f35-1c1aa0c115b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0871ae6-7803-48d9-8a31-fddae538c186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
