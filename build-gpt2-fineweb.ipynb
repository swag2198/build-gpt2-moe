{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f837c-2897-4a0a-855a-d4c76978d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.1.2\n",
      "matplotlib version: 3.9.2\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.4.1\n",
      "/mnt/lustre/work/macke/mwe102/.conda/sbi/bin/python\n",
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = ['numpy', 'matplotlib', 'tiktoken', 'torch']\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")\n",
    "\n",
    "!which python; python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8066db-c024-43a4-b09a-16943e44ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time, inspect\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9a30a-abee-4d66-b7b1-8cbf9c0a3188",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3512461d-4a67-4492-8875-be7119fc70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    # config for gpt2 124M model\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257 # later changed to 50304 during initialization\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "# TODO\n",
    "@dataclass\n",
    "class TokenformerConfig:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fed47cc-8635-429d-ad1c-424b1d5c96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # note that these matrices also have a bias!\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) # Wq, Wk, Wv matrices\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)     # Wo: final projection\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0  # a flag to identify this particular module\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        # note: name is misleading, it is actually the causal mask, not bias!\n",
    "        # this is the autoregressive mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x) # B, T, 3*d\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        # these 4 lines can be replaced by flash attention\n",
    "        # flash attention never materializes the TxT att matrix in GPU memory\n",
    "        # relies on online softmax calculation\n",
    "        \"\"\"\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "        \"\"\"\n",
    "        # replace by this line\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        # gaussian error linear unit, approximation by tanh is a historical quirk\n",
    "        # unlike relu, gelu always contributes a local gradient in the tail end of the flat region\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # prefer clean residual stream from outputs to all the way back to inputs\n",
    "        # no normalization in the residual streams\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict( # the main container\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # layers will be indexed by integers (0, 1, ...) instead of names (like wpe, wte)\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd), # final layer norm\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight tying scheme\n",
    "        # wte weight redirected to the lm_head weight\n",
    "        # wte weight original gets orphaned and hopefully cleaned up\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                # 2 times num layers as each layer adds 2 times to the residual path\n",
    "                # once by attn layer and another time by the MLP layer\n",
    "                std *= (2 * self.config.n_layer) ** (-0.5)\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias) # zero init bias is not pytorch default\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape [B, T]\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) #    [T, n_embd]\n",
    "        tok_emb = self.transformer.wte(idx) # [B, T, n_embd]\n",
    "        x = tok_emb + pos_emb               # broadcasting hidden\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t()) # inplace copying of a tensor\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k]) # inplace copying of a tensor\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # just pick out params that require grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # create optim groups -- all 2d params will be weight decayed, biases and layernorms no decay\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        \n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and \"cuda\" in device\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad426c2d-b352-4ac3-bfc8-9388422901ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfec775e-6a35-43e2-9725-cb8cdbf66e06",
   "metadata": {},
   "source": [
    "## Dataloader for the fineweb dataset (assuming shards are already available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "706f7ea1-8328-4c30-9597-0c5a306e230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename: str):\n",
    "    # expects .npy file\n",
    "    npt = np.load(filename)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T, process_rank=0, num_processes=1, split='train'):\n",
    "        self.B, self.T = B, T\n",
    "        self.process_rank = process_rank\n",
    "        self.num_processes = num_processes\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filename\n",
    "        data_root = 'edu_fineweb10B'\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split: {split}\"\n",
    "        master_process = process_rank == 0\n",
    "        if master_process:\n",
    "            print(f\"found: {len(shards)} shards for: {split} split and num processes: {self.num_processes}\")\n",
    "\n",
    "        # state management\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # useful in val_loader.reset()\n",
    "        # state, initialize at shard 0 or first file\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        # prepare inputs and targets for a single *step* of the optimization\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "        x = buf[:-1].view(B, T) # remove last token\n",
    "        y = buf[1:].view(B, T)  # remove first token\n",
    "        # advance to the next chunk of the array\n",
    "        self.current_position += (B * T * self.num_processes)\n",
    "        # check for next batch loading for all processes\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B*T*self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21ae3ad-cff7-4f72-83c2-5e38099dbd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "# device = \"cpu\" # OVERRIDE\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589da6b0-a8d9-4724-9dcb-4338cd5432b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288 tokens\n",
      "=> calculated gradient accumulation steps: 16\n",
      "found: 99 shards for: train split and num processes: 1\n",
      "found: 1 shards for: val split and num processes: 1\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 524_288 # 2**19, closest power of 2 to ~0.5M\n",
    "B = 32   # micro batch size: how many rows we are processing in a single forward-backward step (32 fits in one A100 40GB, was 16 earlier)\n",
    "T = 1024 # sequence length\n",
    "assert total_batch_size % (B * T) == 0, \"total batch size in number of tokens should be divisible by B*T\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size: {total_batch_size} tokens\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "# initialize the dataloader\n",
    "train_loader = DataLoaderLite(B=B, T=T, split='train')\n",
    "val_loader = DataLoaderLite(B=B, T=T, split='val')\n",
    "\n",
    "# initialize tokenizer for sampling through the model\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03a1fc4-7ee6-442e-8067-67e607f5c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable tf32, now matmuls will use tf32 (tensor cores from A100)\n",
    "torch.set_float32_matmul_precision('high') # default is highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b41c5d-1ace-4a85-af2d-1ca291aaeb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4 # prev constant lr that we were using\n",
    "min_lr = max_lr * 0.1\n",
    "warmup_steps = 100\n",
    "max_steps=151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cad1eba-c4c4-49fe-8096-e2e2373081e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # linear warmup\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it + 1) / warmup_steps\n",
    "    # if it > lr decay iters, return min_lr\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # starts at 1, goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f09aed-869e-460f-8c82-1887a7db6a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = np.arange(max_steps)\n",
    "lrs = np.array([get_lr(it) for it in steps])\n",
    "\n",
    "plt.plot(steps, lrs); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6445eac-22c0-4277-a5a6-a8f33c8f74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(optimizer.param_groups)\n",
    "def generate_and_print_samples(model, tokenizer, device,\n",
    "                               num_return_sequences = 4,\n",
    "                               max_length = 32,\n",
    "                               start_context = \"Hello, I'm a language model,\",\n",
    "                               ):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer.encode(start_context)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "    xgen = tokens.to(device)\n",
    "    # don't interfere with other seeds\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(42)\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()\n",
    "        decoded = tokenizer.decode(tokens)\n",
    "        print(f\"sample {i}: {decoded}\")\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches, print_loss=True) -> float:\n",
    "    model.eval()\n",
    "    data_loader.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_accum = 0.0\n",
    "        loss_steps = num_batches\n",
    "        for _ in range(loss_steps):\n",
    "            x, y = data_loader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "            loss = loss / loss_steps\n",
    "            loss_accum += loss.detach()\n",
    "    \n",
    "    if print_loss:\n",
    "        # averaged per-step loss, averaged over `num_batches` batches or steps\n",
    "        print(f\"Validation loss: {loss_accum.item():.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    return loss_accum.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47cdc7d2-38a0-4608-ab86-9d9a8b436f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.69M\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Validation loss: 10.9514\n",
      "sample 0: Hello, I'm a language model,CHR 351ointmentprotantically surrounded flaw TCUorigin Effective surrounded senate rodents�oviribution Feinstein Homer tappedrica relentlessppedribution 351\n",
      "sample 1: Hello, I'm a language model,iggs Socialism Yangointment rodents contributors investigatesteness emulator toughestClientTesla \"[ricaSTE TI TI Places TFencersRRArray Privacy undone\n",
      "sample 2: Hello, I'm a language model, glimpserint236udzee unve Neighvanceces arise Account Student Privacy Privacy beloved Christina Patterns sugg Sche continental iCloudflo 2012Listener\n",
      "sample 3: Hello, I'm a language model, unvewarts evoke investigates investigateswsws Magnetmens Art altercationperformanceribution roast368 senate Actinguls breastignmentmissing facesmare supremacists\n",
      "Step    0 | loss: 10.955029 | lr: 6.0000e-06 | norm: 15.3464 | dt: 5355.96ms | tok/sec: 97888.77\n",
      "Step    1 | loss: 10.631664 | lr: 1.2000e-05 | norm: 11.6430 | dt: 3875.59ms | tok/sec: 135279.45\n",
      "Step    2 | loss: 10.274958 | lr: 1.8000e-05 | norm: 7.6928 | dt: 3893.94ms | tok/sec: 134642.13\n",
      "Step    3 | loss: 9.996653 | lr: 2.4000e-05 | norm: 5.1922 | dt: 3889.92ms | tok/sec: 134781.29\n",
      "Step    4 | loss: 9.820523 | lr: 3.0000e-05 | norm: 3.5546 | dt: 3903.42ms | tok/sec: 134315.12\n",
      "Step    5 | loss: 9.702173 | lr: 3.6000e-05 | norm: 2.8152 | dt: 3910.19ms | tok/sec: 134082.65\n",
      "Step    6 | loss: 9.640514 | lr: 4.2000e-05 | norm: 2.4508 | dt: 3911.72ms | tok/sec: 134030.03\n",
      "Step    7 | loss: 9.606888 | lr: 4.8000e-05 | norm: 2.3597 | dt: 3918.40ms | tok/sec: 133801.44\n",
      "Step    8 | loss: 9.549835 | lr: 5.4000e-05 | norm: 2.2919 | dt: 3923.50ms | tok/sec: 133627.52\n",
      "Step    9 | loss: 9.520870 | lr: 6.0000e-05 | norm: 2.2768 | dt: 3926.88ms | tok/sec: 133512.53\n",
      "Step   10 | loss: 9.448913 | lr: 6.6000e-05 | norm: 2.1904 | dt: 3930.78ms | tok/sec: 133380.11\n",
      "Step   11 | loss: 9.399248 | lr: 7.2000e-05 | norm: 2.1921 | dt: 3941.38ms | tok/sec: 133021.37\n",
      "Step   12 | loss: 9.323446 | lr: 7.8000e-05 | norm: 1.9913 | dt: 3942.78ms | tok/sec: 132974.31\n",
      "Step   13 | loss: 9.270947 | lr: 8.4000e-05 | norm: 2.1068 | dt: 3948.08ms | tok/sec: 132795.61\n",
      "Step   14 | loss: 9.149524 | lr: 9.0000e-05 | norm: 2.0022 | dt: 3952.49ms | tok/sec: 132647.50\n",
      "Step   15 | loss: 9.083701 | lr: 9.6000e-05 | norm: 1.9717 | dt: 3955.36ms | tok/sec: 132551.43\n",
      "Step   16 | loss: 9.027958 | lr: 1.0200e-04 | norm: 2.3532 | dt: 3961.06ms | tok/sec: 132360.56\n",
      "Step   17 | loss: 8.937508 | lr: 1.0800e-04 | norm: 2.5464 | dt: 3964.22ms | tok/sec: 132255.12\n",
      "Step   18 | loss: 8.947935 | lr: 1.1400e-04 | norm: 1.7219 | dt: 3970.10ms | tok/sec: 132059.15\n",
      "Step   19 | loss: 8.895544 | lr: 1.2000e-04 | norm: 1.7704 | dt: 3967.43ms | tok/sec: 132147.91\n",
      "Step   20 | loss: 8.669391 | lr: 1.2600e-04 | norm: 1.8158 | dt: 3974.48ms | tok/sec: 131913.58\n",
      "Step   21 | loss: 8.557580 | lr: 1.3200e-04 | norm: 1.6628 | dt: 3974.48ms | tok/sec: 131913.50\n",
      "Step   22 | loss: 8.464771 | lr: 1.3800e-04 | norm: 1.5866 | dt: 3974.59ms | tok/sec: 131910.00\n",
      "Step   23 | loss: 8.397415 | lr: 1.4400e-04 | norm: 1.9020 | dt: 3978.42ms | tok/sec: 131782.85\n",
      "Step   24 | loss: 8.236706 | lr: 1.5000e-04 | norm: 1.7098 | dt: 3978.34ms | tok/sec: 131785.63\n",
      "Step   25 | loss: 8.110514 | lr: 1.5600e-04 | norm: 1.5348 | dt: 3977.47ms | tok/sec: 131814.50\n",
      "Step   26 | loss: 8.060259 | lr: 1.6200e-04 | norm: 1.4003 | dt: 3979.84ms | tok/sec: 131735.80\n",
      "Step   27 | loss: 7.948255 | lr: 1.6800e-04 | norm: 1.3016 | dt: 3980.88ms | tok/sec: 131701.62\n",
      "Step   28 | loss: 7.868715 | lr: 1.7400e-04 | norm: 1.2537 | dt: 3980.35ms | tok/sec: 131718.96\n",
      "Step   29 | loss: 7.748524 | lr: 1.8000e-04 | norm: 1.2802 | dt: 3983.78ms | tok/sec: 131605.77\n",
      "Step   30 | loss: 7.818326 | lr: 1.8600e-04 | norm: 1.1917 | dt: 3984.75ms | tok/sec: 131573.71\n",
      "Step   31 | loss: 7.681571 | lr: 1.9200e-04 | norm: 0.8864 | dt: 3984.86ms | tok/sec: 131569.84\n",
      "Step   32 | loss: 7.641321 | lr: 1.9800e-04 | norm: 1.3298 | dt: 3987.45ms | tok/sec: 131484.69\n",
      "Step   33 | loss: 7.571175 | lr: 2.0400e-04 | norm: 0.9997 | dt: 3987.85ms | tok/sec: 131471.30\n",
      "Step   34 | loss: 7.422091 | lr: 2.1000e-04 | norm: 0.8181 | dt: 3992.36ms | tok/sec: 131322.80\n",
      "Step   35 | loss: 7.418482 | lr: 2.1600e-04 | norm: 1.1642 | dt: 3991.77ms | tok/sec: 131342.27\n",
      "Step   36 | loss: 7.394605 | lr: 2.2200e-04 | norm: 0.7492 | dt: 3990.23ms | tok/sec: 131392.87\n",
      "Step   37 | loss: 7.295880 | lr: 2.2800e-04 | norm: 0.6544 | dt: 3992.92ms | tok/sec: 131304.40\n",
      "Step   38 | loss: 7.264238 | lr: 2.3400e-04 | norm: 0.7439 | dt: 3998.48ms | tok/sec: 131121.75\n",
      "Step   39 | loss: 7.312803 | lr: 2.4000e-04 | norm: 0.8878 | dt: 3995.01ms | tok/sec: 131235.81\n",
      "Step   40 | loss: 7.266984 | lr: 2.4600e-04 | norm: 1.0257 | dt: 3998.49ms | tok/sec: 131121.65\n",
      "Step   41 | loss: 7.191205 | lr: 2.5200e-04 | norm: 0.5952 | dt: 4000.82ms | tok/sec: 131045.29\n",
      "Step   42 | loss: 7.139491 | lr: 2.5800e-04 | norm: 1.1311 | dt: 4000.91ms | tok/sec: 131042.28\n",
      "Step   43 | loss: 7.197366 | lr: 2.6400e-04 | norm: 1.5416 | dt: 4000.90ms | tok/sec: 131042.37\n",
      "Step   44 | loss: 7.133410 | lr: 2.7000e-04 | norm: 0.6675 | dt: 4000.25ms | tok/sec: 131063.95\n",
      "Step   45 | loss: 7.136479 | lr: 2.7600e-04 | norm: 0.6314 | dt: 4003.18ms | tok/sec: 130967.80\n",
      "Step   46 | loss: 7.113908 | lr: 2.8200e-04 | norm: 0.8078 | dt: 4004.45ms | tok/sec: 130926.26\n",
      "Step   47 | loss: 7.175154 | lr: 2.8800e-04 | norm: 1.0006 | dt: 4004.33ms | tok/sec: 130930.24\n",
      "Step   48 | loss: 7.204495 | lr: 2.9400e-04 | norm: 1.5856 | dt: 4005.53ms | tok/sec: 130891.03\n",
      "Step   49 | loss: 7.104709 | lr: 3.0000e-04 | norm: 0.9922 | dt: 4005.46ms | tok/sec: 130893.42\n",
      "Validation loss: 7.1188\n",
      "sample 0: Hello, I'm a language model, has a other have the one, as any other it or,,. This is, was an a.\n",
      "What\n",
      "sample 1: Hello, I'm a language model, so by the body.\n",
      "The future.” of a an no been a a body the best, the number\n",
      "sample 2: Hello, I'm a language model, and the year of the way the to the new was the other a more the other other the the own the world�\n",
      "sample 3: Hello, I'm a language model, they do the time the same a way of no the same with the world of we it will about the they more with\n",
      "Step   50 | loss: 7.178977 | lr: 3.0600e-04 | norm: 1.5993 | dt: 4002.02ms | tok/sec: 131005.90\n",
      "Step   51 | loss: 7.099595 | lr: 3.1200e-04 | norm: 0.9420 | dt: 4003.60ms | tok/sec: 130954.18\n",
      "Step   52 | loss: 7.106406 | lr: 3.1800e-04 | norm: 1.3656 | dt: 4008.39ms | tok/sec: 130797.64\n",
      "Step   53 | loss: 7.042623 | lr: 3.2400e-04 | norm: 1.7062 | dt: 4007.10ms | tok/sec: 130839.64\n",
      "Step   54 | loss: 7.063166 | lr: 3.3000e-04 | norm: 1.0155 | dt: 4009.99ms | tok/sec: 130745.47\n",
      "Step   55 | loss: 7.061819 | lr: 3.3600e-04 | norm: 1.1699 | dt: 4008.29ms | tok/sec: 130800.77\n",
      "Step   56 | loss: 6.996113 | lr: 3.4200e-04 | norm: 0.9173 | dt: 4009.52ms | tok/sec: 130760.80\n",
      "Step   57 | loss: 6.999709 | lr: 3.4800e-04 | norm: 0.8904 | dt: 4006.26ms | tok/sec: 130867.26\n",
      "Step   58 | loss: 6.909717 | lr: 3.5400e-04 | norm: 0.8316 | dt: 4006.53ms | tok/sec: 130858.25\n",
      "Step   59 | loss: 6.890442 | lr: 3.6000e-04 | norm: 1.1536 | dt: 4007.64ms | tok/sec: 130822.29\n",
      "Step   60 | loss: 6.893281 | lr: 3.6600e-04 | norm: 1.0363 | dt: 4007.78ms | tok/sec: 130817.62\n",
      "Step   61 | loss: 6.918369 | lr: 3.7200e-04 | norm: 1.1350 | dt: 4005.42ms | tok/sec: 130894.76\n",
      "Step   62 | loss: 6.801018 | lr: 3.7800e-04 | norm: 0.7666 | dt: 4010.41ms | tok/sec: 130731.61\n",
      "Step   63 | loss: 6.797893 | lr: 3.8400e-04 | norm: 0.8336 | dt: 4006.97ms | tok/sec: 130844.09\n",
      "Step   64 | loss: 6.770338 | lr: 3.9000e-04 | norm: 0.5701 | dt: 4007.64ms | tok/sec: 130822.14\n",
      "Step   65 | loss: 6.804197 | lr: 3.9600e-04 | norm: 0.4993 | dt: 4005.75ms | tok/sec: 130883.99\n",
      "Step   66 | loss: 6.739757 | lr: 4.0200e-04 | norm: 0.8441 | dt: 4009.61ms | tok/sec: 130757.79\n",
      "Step   67 | loss: 6.759301 | lr: 4.0800e-04 | norm: 0.9279 | dt: 4010.96ms | tok/sec: 130713.99\n",
      "Step   68 | loss: 6.764777 | lr: 4.1400e-04 | norm: 1.5660 | dt: 4005.95ms | tok/sec: 130877.17\n",
      "Step   69 | loss: 6.647270 | lr: 4.2000e-04 | norm: 0.8880 | dt: 4009.01ms | tok/sec: 130777.56\n",
      "Step   70 | loss: 6.623574 | lr: 4.2600e-04 | norm: 0.6408 | dt: 4005.28ms | tok/sec: 130899.22\n",
      "Step   71 | loss: 6.651897 | lr: 4.3200e-04 | norm: 0.6022 | dt: 4004.13ms | tok/sec: 130936.88\n",
      "Step   72 | loss: 6.615503 | lr: 4.3800e-04 | norm: 0.8124 | dt: 4004.61ms | tok/sec: 130921.24\n",
      "Step   73 | loss: 6.606874 | lr: 4.4400e-04 | norm: 0.5175 | dt: 4005.06ms | tok/sec: 130906.32\n",
      "Step   74 | loss: 6.539811 | lr: 4.5000e-04 | norm: 0.6065 | dt: 4004.02ms | tok/sec: 130940.30\n",
      "Step   75 | loss: 6.580618 | lr: 4.5600e-04 | norm: 0.5206 | dt: 4004.65ms | tok/sec: 130919.66\n",
      "Step   76 | loss: 6.525567 | lr: 4.6200e-04 | norm: 0.5555 | dt: 4003.68ms | tok/sec: 130951.44\n",
      "Step   77 | loss: 6.558043 | lr: 4.6800e-04 | norm: 0.4931 | dt: 4004.70ms | tok/sec: 130918.13\n",
      "Step   78 | loss: 6.542771 | lr: 4.7400e-04 | norm: 0.8641 | dt: 4004.37ms | tok/sec: 130929.02\n",
      "Step   79 | loss: 6.513207 | lr: 4.8000e-04 | norm: 1.2599 | dt: 4001.24ms | tok/sec: 131031.40\n",
      "Step   80 | loss: 6.511995 | lr: 4.8600e-04 | norm: 0.9854 | dt: 4002.34ms | tok/sec: 130995.50\n",
      "Step   81 | loss: 6.508885 | lr: 4.9200e-04 | norm: 0.8092 | dt: 4002.54ms | tok/sec: 130988.76\n",
      "Step   82 | loss: 6.450952 | lr: 4.9800e-04 | norm: 0.5120 | dt: 4001.46ms | tok/sec: 131024.16\n",
      "Step   83 | loss: 6.412244 | lr: 5.0400e-04 | norm: 0.6203 | dt: 4002.56ms | tok/sec: 130988.11\n",
      "Step   84 | loss: 6.424363 | lr: 5.1000e-04 | norm: 0.6848 | dt: 3999.94ms | tok/sec: 131074.12\n",
      "Step   85 | loss: 6.393447 | lr: 5.1600e-04 | norm: 0.7324 | dt: 4001.07ms | tok/sec: 131036.81\n",
      "Step   86 | loss: 6.393788 | lr: 5.2200e-04 | norm: 1.0279 | dt: 4003.27ms | tok/sec: 130964.95\n",
      "Step   87 | loss: 6.430075 | lr: 5.2800e-04 | norm: 0.9150 | dt: 4000.56ms | tok/sec: 131053.66\n",
      "Step   88 | loss: 6.399949 | lr: 5.3400e-04 | norm: 0.6450 | dt: 4001.88ms | tok/sec: 131010.43\n",
      "Step   89 | loss: 6.449418 | lr: 5.4000e-04 | norm: 0.6748 | dt: 4002.93ms | tok/sec: 130976.06\n",
      "Step   90 | loss: 6.372850 | lr: 5.4600e-04 | norm: 0.7507 | dt: 4001.22ms | tok/sec: 131032.16\n",
      "Step   91 | loss: 6.380558 | lr: 5.5200e-04 | norm: 0.8390 | dt: 4000.62ms | tok/sec: 131051.80\n",
      "Step   92 | loss: 6.442923 | lr: 5.5800e-04 | norm: 0.8984 | dt: 4001.08ms | tok/sec: 131036.74\n",
      "Step   93 | loss: 6.532280 | lr: 5.6400e-04 | norm: 1.1648 | dt: 4002.31ms | tok/sec: 130996.27\n",
      "Step   94 | loss: 6.561176 | lr: 5.7000e-04 | norm: 1.4194 | dt: 4003.82ms | tok/sec: 130946.91\n",
      "Step   95 | loss: 6.535738 | lr: 5.7600e-04 | norm: 0.8100 | dt: 4000.37ms | tok/sec: 131059.79\n",
      "Step   96 | loss: 6.479640 | lr: 5.8200e-04 | norm: 0.7751 | dt: 4000.40ms | tok/sec: 131058.90\n",
      "Step   97 | loss: 6.445833 | lr: 5.8800e-04 | norm: 0.9824 | dt: 4000.45ms | tok/sec: 131057.40\n",
      "Step   98 | loss: 6.491247 | lr: 5.9400e-04 | norm: 0.8534 | dt: 4001.61ms | tok/sec: 131019.26\n",
      "Step   99 | loss: 6.443907 | lr: 6.0000e-04 | norm: 0.5940 | dt: 4000.29ms | tok/sec: 131062.64\n",
      "Validation loss: 6.4071\n",
      "sample 0: Hello, I'm a language model, not a long, it has to find all-8-1 on any than a small about what they need to use\n",
      "sample 1: Hello, I'm a language model, one one people that they are more effective. It have sure, you just as well.\n",
      "The good time, you\n",
      "sample 2: Hello, I'm a language model, and the point in least and for him, we take a more about his own-m-the-the-t\n",
      "sample 3: Hello, I'm a language model, they did not only.\n",
      "The study.S.\n",
      "M), you do for them on a different.\n",
      "and\n",
      "Step  100 | loss: 6.478121 | lr: 6.0000e-04 | norm: 0.6594 | dt: 3998.32ms | tok/sec: 131126.94\n",
      "Step  101 | loss: 6.417323 | lr: 6.0000e-04 | norm: 0.8648 | dt: 4001.63ms | tok/sec: 131018.59\n",
      "Step  102 | loss: 6.398405 | lr: 5.9999e-04 | norm: 0.8897 | dt: 4000.82ms | tok/sec: 131044.99\n",
      "Step  103 | loss: 6.299183 | lr: 5.9999e-04 | norm: 0.9406 | dt: 4001.55ms | tok/sec: 131021.11\n",
      "Step  104 | loss: 6.408423 | lr: 5.9997e-04 | norm: 1.0465 | dt: 4001.02ms | tok/sec: 131038.71\n",
      "Step  105 | loss: 6.411335 | lr: 5.9996e-04 | norm: 0.8788 | dt: 4002.85ms | tok/sec: 130978.82\n",
      "Step  106 | loss: 6.354550 | lr: 5.9994e-04 | norm: 0.3949 | dt: 4003.28ms | tok/sec: 130964.63\n",
      "Step  107 | loss: 6.311340 | lr: 5.9992e-04 | norm: 0.4821 | dt: 4003.22ms | tok/sec: 130966.43\n",
      "Step  108 | loss: 6.371496 | lr: 5.9989e-04 | norm: 0.4338 | dt: 4000.21ms | tok/sec: 131065.01\n",
      "Step  109 | loss: 6.360398 | lr: 5.9987e-04 | norm: 0.4576 | dt: 4002.75ms | tok/sec: 130982.01\n",
      "Step  110 | loss: 6.320600 | lr: 5.9984e-04 | norm: 0.4002 | dt: 4001.89ms | tok/sec: 131010.11\n",
      "Step  111 | loss: 6.290051 | lr: 5.9980e-04 | norm: 0.4183 | dt: 4005.08ms | tok/sec: 130905.73\n",
      "Step  112 | loss: 6.272653 | lr: 5.9976e-04 | norm: 0.3977 | dt: 4005.64ms | tok/sec: 130887.34\n",
      "Step  113 | loss: 6.299547 | lr: 5.9972e-04 | norm: 0.4429 | dt: 4003.09ms | tok/sec: 130970.69\n",
      "Step  114 | loss: 6.326423 | lr: 5.9968e-04 | norm: 0.5132 | dt: 4003.11ms | tok/sec: 130970.12\n",
      "Step  115 | loss: 6.291857 | lr: 5.9963e-04 | norm: 0.5200 | dt: 4006.71ms | tok/sec: 130852.44\n",
      "Step  116 | loss: 6.232466 | lr: 5.9958e-04 | norm: 0.6404 | dt: 4005.77ms | tok/sec: 130883.26\n",
      "Step  117 | loss: 6.171976 | lr: 5.9953e-04 | norm: 0.6390 | dt: 4008.88ms | tok/sec: 130781.68\n",
      "Step  118 | loss: 6.189350 | lr: 5.9947e-04 | norm: 0.4679 | dt: 4005.57ms | tok/sec: 130889.80\n",
      "Step  119 | loss: 6.207645 | lr: 5.9941e-04 | norm: 0.3739 | dt: 4002.85ms | tok/sec: 130978.62\n",
      "Step  120 | loss: 6.194958 | lr: 5.9934e-04 | norm: 0.4304 | dt: 4008.04ms | tok/sec: 130808.94\n",
      "Step  121 | loss: 6.150903 | lr: 5.9928e-04 | norm: 0.4286 | dt: 4006.27ms | tok/sec: 130866.73\n",
      "Step  122 | loss: 6.171412 | lr: 5.9921e-04 | norm: 0.6924 | dt: 4006.08ms | tok/sec: 130873.14\n",
      "Step  123 | loss: 6.184656 | lr: 5.9913e-04 | norm: 0.8382 | dt: 4009.04ms | tok/sec: 130776.53\n",
      "Step  124 | loss: 6.186934 | lr: 5.9906e-04 | norm: 0.7389 | dt: 4005.95ms | tok/sec: 130877.40\n",
      "Step  125 | loss: 6.190511 | lr: 5.9897e-04 | norm: 0.7808 | dt: 4004.61ms | tok/sec: 130921.17\n",
      "Step  126 | loss: 6.154933 | lr: 5.9889e-04 | norm: 0.4137 | dt: 4006.87ms | tok/sec: 130847.32\n",
      "Step  127 | loss: 6.125181 | lr: 5.9880e-04 | norm: 0.4332 | dt: 4006.72ms | tok/sec: 130852.30\n",
      "Step  128 | loss: 6.075976 | lr: 5.9871e-04 | norm: 0.3954 | dt: 4004.02ms | tok/sec: 130940.36\n",
      "Step  129 | loss: 6.073150 | lr: 5.9862e-04 | norm: 0.4356 | dt: 4007.14ms | tok/sec: 130838.52\n",
      "Step  130 | loss: 6.118850 | lr: 5.9852e-04 | norm: 0.3439 | dt: 4006.82ms | tok/sec: 130848.96\n",
      "Step  131 | loss: 6.085485 | lr: 5.9842e-04 | norm: 0.4706 | dt: 4007.49ms | tok/sec: 130827.11\n",
      "Step  132 | loss: 6.090281 | lr: 5.9832e-04 | norm: 0.4515 | dt: 4007.43ms | tok/sec: 130828.98\n",
      "Step  133 | loss: 6.008005 | lr: 5.9821e-04 | norm: 0.5255 | dt: 4003.04ms | tok/sec: 130972.47\n",
      "Step  134 | loss: 6.019447 | lr: 5.9810e-04 | norm: 0.4708 | dt: 4007.34ms | tok/sec: 130831.99\n",
      "Step  135 | loss: 6.062609 | lr: 5.9799e-04 | norm: 0.4868 | dt: 4007.61ms | tok/sec: 130823.24\n",
      "Step  136 | loss: 6.042056 | lr: 5.9788e-04 | norm: 0.4998 | dt: 4005.57ms | tok/sec: 130889.62\n",
      "Step  137 | loss: 6.064139 | lr: 5.9776e-04 | norm: 0.4164 | dt: 4011.02ms | tok/sec: 130711.84\n",
      "Step  138 | loss: 6.018184 | lr: 5.9763e-04 | norm: 0.5078 | dt: 4007.00ms | tok/sec: 130842.99\n",
      "Step  139 | loss: 6.120688 | lr: 5.9751e-04 | norm: 0.5350 | dt: 4008.38ms | tok/sec: 130798.07\n",
      "Step  140 | loss: 6.230243 | lr: 5.9738e-04 | norm: 0.5444 | dt: 4005.98ms | tok/sec: 130876.32\n",
      "Step  141 | loss: 6.123663 | lr: 5.9725e-04 | norm: 0.5272 | dt: 4004.46ms | tok/sec: 130926.18\n",
      "Step  142 | loss: 6.170104 | lr: 5.9711e-04 | norm: 0.4771 | dt: 4008.96ms | tok/sec: 130779.18\n",
      "Step  143 | loss: 6.109033 | lr: 5.9697e-04 | norm: 0.5135 | dt: 4008.49ms | tok/sec: 130794.46\n",
      "Step  144 | loss: 6.121022 | lr: 5.9683e-04 | norm: 0.6825 | dt: 4004.62ms | tok/sec: 130920.67\n",
      "Step  145 | loss: 6.109360 | lr: 5.9668e-04 | norm: 0.6623 | dt: 4007.44ms | tok/sec: 130828.50\n",
      "Step  146 | loss: 6.133736 | lr: 5.9653e-04 | norm: 0.7257 | dt: 4006.41ms | tok/sec: 130862.20\n",
      "Step  147 | loss: 6.144201 | lr: 5.9638e-04 | norm: 0.7050 | dt: 4008.11ms | tok/sec: 130806.87\n",
      "Step  148 | loss: 6.139348 | lr: 5.9623e-04 | norm: 0.5593 | dt: 4004.82ms | tok/sec: 130914.37\n",
      "Step  149 | loss: 6.095233 | lr: 5.9607e-04 | norm: 0.4881 | dt: 4005.87ms | tok/sec: 130879.91\n",
      "Validation loss: 6.0634\n",
      "sample 0: Hello, I'm a language model, \"the month, I could, in 1841, we will we get it. We get that I do. You\n",
      "sample 1: Hello, I'm a language model, because people like a few of our community. It knows you have a series in a good thing that in their job is\n",
      "sample 2: Hello, I'm a language model, I have come from, in most.\n",
      "We get a\n",
      "If you have a group, and help you have done\n",
      "sample 3: Hello, I'm a language model, the project. If you need for your student by your home. We want your friends or an additional in your hands when\n",
      "Step  150 | loss: 6.102352 | lr: 5.9591e-04 | norm: 0.4800 | dt: 4000.17ms | tok/sec: 131066.42\n",
      "Step  151 | loss: 6.013873 | lr: 5.9574e-04 | norm: 0.4016 | dt: 4007.02ms | tok/sec: 130842.43\n",
      "Step  152 | loss: 6.057227 | lr: 5.9557e-04 | norm: 0.4353 | dt: 4007.56ms | tok/sec: 130824.62\n",
      "Step  153 | loss: 6.094522 | lr: 5.9540e-04 | norm: 0.3702 | dt: 4003.98ms | tok/sec: 130941.60\n",
      "Step  154 | loss: 6.005877 | lr: 5.9523e-04 | norm: 0.4158 | dt: 4007.19ms | tok/sec: 130836.91\n",
      "Step  155 | loss: 5.993237 | lr: 5.9505e-04 | norm: 0.4260 | dt: 4004.88ms | tok/sec: 130912.25\n",
      "Step  156 | loss: 6.017674 | lr: 5.9487e-04 | norm: 0.4032 | dt: 4004.86ms | tok/sec: 130913.04\n",
      "Step  157 | loss: 6.087114 | lr: 5.9468e-04 | norm: 0.4369 | dt: 4007.13ms | tok/sec: 130838.80\n",
      "Step  158 | loss: 6.090498 | lr: 5.9450e-04 | norm: 0.3274 | dt: 4006.80ms | tok/sec: 130849.60\n",
      "Step  159 | loss: 6.008410 | lr: 5.9431e-04 | norm: 0.4228 | dt: 4007.50ms | tok/sec: 130826.75\n",
      "Step  160 | loss: 5.950188 | lr: 5.9411e-04 | norm: 0.4738 | dt: 4008.98ms | tok/sec: 130778.46\n",
      "Step  161 | loss: 6.040023 | lr: 5.9392e-04 | norm: 0.4135 | dt: 4006.41ms | tok/sec: 130862.37\n",
      "Step  162 | loss: 5.942799 | lr: 5.9372e-04 | norm: 0.3435 | dt: 4007.09ms | tok/sec: 130840.19\n",
      "Step  163 | loss: 5.913026 | lr: 5.9351e-04 | norm: 0.4130 | dt: 4004.37ms | tok/sec: 130928.87\n",
      "Step  164 | loss: 5.914661 | lr: 5.9331e-04 | norm: 0.4263 | dt: 4008.95ms | tok/sec: 130779.49\n",
      "Step  165 | loss: 5.974907 | lr: 5.9310e-04 | norm: 0.4980 | dt: 4005.22ms | tok/sec: 130901.24\n",
      "Step  166 | loss: 5.834282 | lr: 5.9288e-04 | norm: 0.5531 | dt: 4007.16ms | tok/sec: 130837.77\n",
      "Step  167 | loss: 5.940022 | lr: 5.9267e-04 | norm: 0.6256 | dt: 4004.29ms | tok/sec: 130931.73\n",
      "Step  168 | loss: 5.914894 | lr: 5.9245e-04 | norm: 0.5490 | dt: 4007.51ms | tok/sec: 130826.40\n",
      "Step  169 | loss: 5.919202 | lr: 5.9222e-04 | norm: 0.6141 | dt: 4006.02ms | tok/sec: 130875.15\n",
      "Step  170 | loss: 5.893355 | lr: 5.9200e-04 | norm: 0.5300 | dt: 4006.76ms | tok/sec: 130850.70\n",
      "Step  171 | loss: 5.923788 | lr: 5.9177e-04 | norm: 0.5885 | dt: 4005.42ms | tok/sec: 130894.72\n",
      "Step  172 | loss: 5.893055 | lr: 5.9154e-04 | norm: 0.6344 | dt: 4006.99ms | tok/sec: 130843.35\n",
      "Step  173 | loss: 5.869820 | lr: 5.9130e-04 | norm: 0.4157 | dt: 4003.51ms | tok/sec: 130957.00\n",
      "Step  174 | loss: 5.819705 | lr: 5.9106e-04 | norm: 0.4098 | dt: 4005.80ms | tok/sec: 130882.34\n",
      "Step  175 | loss: 5.869809 | lr: 5.9082e-04 | norm: 0.4605 | dt: 4005.97ms | tok/sec: 130876.64\n",
      "Step  176 | loss: 5.823327 | lr: 5.9058e-04 | norm: 0.4424 | dt: 4005.65ms | tok/sec: 130887.18\n",
      "Step  177 | loss: 5.869301 | lr: 5.9033e-04 | norm: 0.4587 | dt: 4008.28ms | tok/sec: 130801.24\n",
      "Step  178 | loss: 5.788408 | lr: 5.9008e-04 | norm: 0.3800 | dt: 4006.30ms | tok/sec: 130865.93\n",
      "Step  179 | loss: 5.779335 | lr: 5.8982e-04 | norm: 0.4160 | dt: 4005.86ms | tok/sec: 130880.22\n",
      "Step  180 | loss: 5.772307 | lr: 5.8956e-04 | norm: 0.5083 | dt: 4007.17ms | tok/sec: 130837.62\n",
      "Step  181 | loss: 5.810264 | lr: 5.8930e-04 | norm: 0.5688 | dt: 4007.07ms | tok/sec: 130840.70\n",
      "Step  182 | loss: 5.788921 | lr: 5.8904e-04 | norm: 0.3901 | dt: 4005.07ms | tok/sec: 130906.09\n",
      "Step  183 | loss: 5.790009 | lr: 5.8877e-04 | norm: 0.3825 | dt: 4006.18ms | tok/sec: 130869.84\n",
      "Step  184 | loss: 5.811239 | lr: 5.8850e-04 | norm: 0.3784 | dt: 4006.69ms | tok/sec: 130853.00\n",
      "Step  185 | loss: 5.866576 | lr: 5.8823e-04 | norm: 0.3634 | dt: 4005.02ms | tok/sec: 130907.78\n",
      "Step  186 | loss: 5.899055 | lr: 5.8795e-04 | norm: 0.4059 | dt: 4006.93ms | tok/sec: 130845.45\n",
      "Step  187 | loss: 5.947875 | lr: 5.8767e-04 | norm: 0.3991 | dt: 4008.64ms | tok/sec: 130789.65\n",
      "Step  188 | loss: 5.899945 | lr: 5.8739e-04 | norm: 0.4184 | dt: 4005.62ms | tok/sec: 130887.99\n",
      "Step  189 | loss: 5.887078 | lr: 5.8710e-04 | norm: 0.4802 | dt: 4006.82ms | tok/sec: 130849.04\n",
      "Step  190 | loss: 5.927372 | lr: 5.8681e-04 | norm: 0.5975 | dt: 4007.97ms | tok/sec: 130811.29\n",
      "Step  191 | loss: 5.889796 | lr: 5.8652e-04 | norm: 0.6488 | dt: 4005.78ms | tok/sec: 130882.86\n",
      "Step  192 | loss: 5.900117 | lr: 5.8623e-04 | norm: 0.6224 | dt: 4006.45ms | tok/sec: 130861.01\n",
      "Step  193 | loss: 5.890120 | lr: 5.8593e-04 | norm: 0.5004 | dt: 4007.68ms | tok/sec: 130820.72\n",
      "Step  194 | loss: 5.811455 | lr: 5.8563e-04 | norm: 0.4443 | dt: 4006.25ms | tok/sec: 130867.38\n",
      "Step  195 | loss: 5.855459 | lr: 5.8532e-04 | norm: 0.3942 | dt: 4007.17ms | tok/sec: 130837.56\n",
      "Step  196 | loss: 5.808201 | lr: 5.8501e-04 | norm: 0.4135 | dt: 4006.39ms | tok/sec: 130863.05\n",
      "Step  197 | loss: 5.843592 | lr: 5.8470e-04 | norm: 0.4807 | dt: 4005.72ms | tok/sec: 130884.68\n",
      "Step  198 | loss: 5.833368 | lr: 5.8439e-04 | norm: 0.5222 | dt: 4005.81ms | tok/sec: 130881.97\n",
      "Step  199 | loss: 5.796987 | lr: 5.8407e-04 | norm: 0.4585 | dt: 4005.18ms | tok/sec: 130902.37\n",
      "Validation loss: 5.8152\n",
      "sample 0: Hello, I'm a language model, I am sure I. I have more words from the student, if I I am my student, but I am the\n",
      "sample 1: Hello, I'm a language model, and important factors, but I was important to a large scale. A simple-like and also has seen, and is\n",
      "sample 2: Hello, I'm a language model, and I: my friend with my thoughts and, I am like to my own thoughts, I am really I I'd\n",
      "sample 3: Hello, I'm a language model, or reading, but I think of these questions be a common method from the twoteenth century was the \"1) with\n",
      "Step  200 | loss: 5.817258 | lr: 5.8375e-04 | norm: 0.4773 | dt: 4002.76ms | tok/sec: 130981.52\n",
      "Step  201 | loss: 5.802971 | lr: 5.8343e-04 | norm: 0.5189 | dt: 4007.03ms | tok/sec: 130841.97\n",
      "Step  202 | loss: 5.807866 | lr: 5.8310e-04 | norm: 0.4691 | dt: 4007.81ms | tok/sec: 130816.42\n",
      "Step  203 | loss: 5.795544 | lr: 5.8277e-04 | norm: 0.5049 | dt: 4007.70ms | tok/sec: 130820.20\n",
      "Step  204 | loss: 5.790365 | lr: 5.8244e-04 | norm: 0.4387 | dt: 4006.34ms | tok/sec: 130864.63\n",
      "Step  205 | loss: 5.797321 | lr: 5.8211e-04 | norm: 0.3812 | dt: 4005.24ms | tok/sec: 130900.46\n",
      "Step  206 | loss: 5.812152 | lr: 5.8177e-04 | norm: 0.4509 | dt: 4007.75ms | tok/sec: 130818.51\n",
      "Step  207 | loss: 5.775387 | lr: 5.8143e-04 | norm: 0.3373 | dt: 4004.74ms | tok/sec: 130916.72\n",
      "Step  208 | loss: 5.725178 | lr: 5.8108e-04 | norm: 0.3628 | dt: 4006.75ms | tok/sec: 130851.33\n",
      "Step  209 | loss: 5.682278 | lr: 5.8073e-04 | norm: 0.4072 | dt: 4008.85ms | tok/sec: 130782.56\n",
      "Step  210 | loss: 5.714568 | lr: 5.8038e-04 | norm: 0.4591 | dt: 4004.92ms | tok/sec: 130910.92\n",
      "Step  211 | loss: 5.741694 | lr: 5.8003e-04 | norm: 0.4073 | dt: 4004.64ms | tok/sec: 130920.15\n",
      "Step  212 | loss: 5.758872 | lr: 5.7967e-04 | norm: 0.4923 | dt: 4005.73ms | tok/sec: 130884.49\n",
      "Step  213 | loss: 5.731956 | lr: 5.7931e-04 | norm: 0.4954 | dt: 4006.55ms | tok/sec: 130857.83\n",
      "Step  214 | loss: 5.733871 | lr: 5.7895e-04 | norm: 0.4643 | dt: 4005.71ms | tok/sec: 130885.28\n",
      "Step  215 | loss: 5.728811 | lr: 5.7858e-04 | norm: 0.4028 | dt: 4010.54ms | tok/sec: 130727.58\n",
      "Step  216 | loss: 5.675428 | lr: 5.7821e-04 | norm: 0.5353 | dt: 4006.33ms | tok/sec: 130864.86\n",
      "Step  217 | loss: 5.703564 | lr: 5.7784e-04 | norm: 0.6319 | dt: 4006.96ms | tok/sec: 130844.18\n",
      "Step  218 | loss: 5.677732 | lr: 5.7747e-04 | norm: 0.6467 | dt: 4007.25ms | tok/sec: 130835.02\n",
      "Step  219 | loss: 5.708451 | lr: 5.7709e-04 | norm: 0.9113 | dt: 4004.74ms | tok/sec: 130916.72\n",
      "Step  220 | loss: 5.654290 | lr: 5.7671e-04 | norm: 0.9069 | dt: 4006.97ms | tok/sec: 130844.15\n",
      "Step  221 | loss: 5.669984 | lr: 5.7632e-04 | norm: 0.7230 | dt: 4009.36ms | tok/sec: 130765.85\n",
      "Step  222 | loss: 5.675779 | lr: 5.7594e-04 | norm: 0.5978 | dt: 4008.55ms | tok/sec: 130792.50\n",
      "Step  223 | loss: 5.595019 | lr: 5.7555e-04 | norm: 0.5616 | dt: 4010.91ms | tok/sec: 130715.61\n",
      "Step  224 | loss: 5.616502 | lr: 5.7515e-04 | norm: 0.5064 | dt: 4010.66ms | tok/sec: 130723.72\n",
      "Step  225 | loss: 5.613329 | lr: 5.7476e-04 | norm: 0.4471 | dt: 4007.33ms | tok/sec: 130832.18\n",
      "Step  226 | loss: 5.553064 | lr: 5.7436e-04 | norm: 0.4018 | dt: 4007.93ms | tok/sec: 130812.81\n",
      "Step  227 | loss: 5.669246 | lr: 5.7396e-04 | norm: 0.4042 | dt: 4007.14ms | tok/sec: 130838.41\n",
      "Step  228 | loss: 5.579723 | lr: 5.7355e-04 | norm: 0.3991 | dt: 4006.05ms | tok/sec: 130874.02\n",
      "Step  229 | loss: 5.548065 | lr: 5.7314e-04 | norm: 0.3477 | dt: 4008.12ms | tok/sec: 130806.57\n",
      "Step  230 | loss: 5.577476 | lr: 5.7273e-04 | norm: 0.3584 | dt: 4005.00ms | tok/sec: 130908.40\n",
      "Step  231 | loss: 5.532362 | lr: 5.7232e-04 | norm: 0.3363 | dt: 4007.03ms | tok/sec: 130842.11\n",
      "Step  232 | loss: 5.722721 | lr: 5.7190e-04 | norm: 0.3535 | dt: 4007.06ms | tok/sec: 130841.02\n",
      "Step  233 | loss: 5.700481 | lr: 5.7148e-04 | norm: 0.3244 | dt: 4008.68ms | tok/sec: 130788.20\n",
      "Step  234 | loss: 5.707403 | lr: 5.7106e-04 | norm: 0.3236 | dt: 4006.71ms | tok/sec: 130852.52\n",
      "Step  235 | loss: 5.720259 | lr: 5.7064e-04 | norm: 0.4270 | dt: 4007.32ms | tok/sec: 130832.60\n",
      "Step  236 | loss: 5.745428 | lr: 5.7021e-04 | norm: 0.6156 | dt: 4009.51ms | tok/sec: 130761.04\n",
      "Step  237 | loss: 5.717975 | lr: 5.6978e-04 | norm: 0.6448 | dt: 4007.29ms | tok/sec: 130833.58\n",
      "Step  238 | loss: 5.669228 | lr: 5.6934e-04 | norm: 0.5351 | dt: 4005.56ms | tok/sec: 130890.13\n",
      "Step  239 | loss: 5.675673 | lr: 5.6890e-04 | norm: 0.5053 | dt: 4008.65ms | tok/sec: 130789.11\n",
      "Step  240 | loss: 5.681227 | lr: 5.6846e-04 | norm: 0.5882 | dt: 4005.25ms | tok/sec: 130900.25\n",
      "Step  241 | loss: 5.666877 | lr: 5.6802e-04 | norm: 0.5602 | dt: 4009.21ms | tok/sec: 130771.00\n",
      "Step  242 | loss: 5.703240 | lr: 5.6758e-04 | norm: 0.5385 | dt: 4006.59ms | tok/sec: 130856.43\n",
      "Step  243 | loss: 5.675234 | lr: 5.6713e-04 | norm: 0.5234 | dt: 4008.29ms | tok/sec: 130800.85\n",
      "Step  244 | loss: 5.604269 | lr: 5.6668e-04 | norm: 0.4261 | dt: 4005.06ms | tok/sec: 130906.26\n",
      "Step  245 | loss: 5.560494 | lr: 5.6622e-04 | norm: 0.4262 | dt: 4002.96ms | tok/sec: 130975.18\n",
      "Step  246 | loss: 5.580530 | lr: 5.6576e-04 | norm: 0.4292 | dt: 4007.53ms | tok/sec: 130825.83\n",
      "Step  247 | loss: 5.625813 | lr: 5.6530e-04 | norm: 0.4064 | dt: 4006.17ms | tok/sec: 130870.14\n",
      "Step  248 | loss: 5.590169 | lr: 5.6484e-04 | norm: 0.3846 | dt: 4009.69ms | tok/sec: 130755.21\n",
      "Step  249 | loss: 5.569154 | lr: 5.6437e-04 | norm: 0.4791 | dt: 4004.63ms | tok/sec: 130920.45\n",
      "Validation loss: 5.5753\n",
      "sample 0: Hello, I'm a language model, and I know that the way to me us all the facts of those times that I think how you can get the way\n",
      "sample 1: Hello, I'm a language model, \"poles\" (p] with the first] [m-K) and a way, with this system (\n",
      "sample 2: Hello, I'm a language model, I have worked with its language. In the author, I can see that I think it is a scientific, and so\n",
      "sample 3: Hello, I'm a language model, as if I’s not an individual can be an interest in an event after any particular. You can provide my\n",
      "Step  250 | loss: 5.605248 | lr: 5.6391e-04 | norm: 0.5330 | dt: 4000.13ms | tok/sec: 131067.88\n",
      "Step  251 | loss: 5.573323 | lr: 5.6343e-04 | norm: 0.5744 | dt: 4008.13ms | tok/sec: 130806.29\n",
      "Step  252 | loss: 5.597020 | lr: 5.6296e-04 | norm: 0.4967 | dt: 4004.49ms | tok/sec: 130925.03\n",
      "Step  253 | loss: 5.588136 | lr: 5.6248e-04 | norm: 0.5565 | dt: 4006.17ms | tok/sec: 130870.04\n",
      "Step  254 | loss: 5.606618 | lr: 5.6200e-04 | norm: 0.5205 | dt: 4007.11ms | tok/sec: 130839.54\n",
      "Step  255 | loss: 5.498772 | lr: 5.6152e-04 | norm: 0.6020 | dt: 4008.55ms | tok/sec: 130792.30\n",
      "Step  256 | loss: 5.566062 | lr: 5.6103e-04 | norm: 0.6942 | dt: 4010.35ms | tok/sec: 130733.84\n",
      "Step  257 | loss: 5.558488 | lr: 5.6054e-04 | norm: 0.9171 | dt: 4005.70ms | tok/sec: 130885.59\n",
      "Step  258 | loss: 5.534087 | lr: 5.6005e-04 | norm: 0.8679 | dt: 4011.44ms | tok/sec: 130698.31\n",
      "Step  259 | loss: 5.546240 | lr: 5.5956e-04 | norm: 0.6976 | dt: 4009.04ms | tok/sec: 130776.50\n",
      "Step  260 | loss: 5.490053 | lr: 5.5906e-04 | norm: 0.5645 | dt: 4008.13ms | tok/sec: 130805.99\n",
      "Step  261 | loss: 5.520907 | lr: 5.5856e-04 | norm: 0.5759 | dt: 4005.72ms | tok/sec: 130884.88\n",
      "Step  262 | loss: 5.484705 | lr: 5.5806e-04 | norm: 0.4215 | dt: 4010.80ms | tok/sec: 130719.05\n",
      "Step  263 | loss: 5.466310 | lr: 5.5755e-04 | norm: 0.4086 | dt: 4011.42ms | tok/sec: 130698.87\n",
      "Step  264 | loss: 5.480101 | lr: 5.5705e-04 | norm: 0.3829 | dt: 4010.83ms | tok/sec: 130718.22\n",
      "Step  265 | loss: 5.471270 | lr: 5.5654e-04 | norm: 0.3745 | dt: 4009.67ms | tok/sec: 130755.93\n",
      "Step  266 | loss: 5.504671 | lr: 5.5602e-04 | norm: 0.3691 | dt: 4010.95ms | tok/sec: 130714.26\n",
      "Step  267 | loss: 5.406854 | lr: 5.5551e-04 | norm: 0.3502 | dt: 4010.31ms | tok/sec: 130735.09\n",
      "Step  268 | loss: 5.403605 | lr: 5.5499e-04 | norm: 0.3708 | dt: 4010.63ms | tok/sec: 130724.67\n",
      "Step  269 | loss: 5.436235 | lr: 5.5446e-04 | norm: 0.4308 | dt: 4011.37ms | tok/sec: 130700.52\n",
      "Step  270 | loss: 5.443823 | lr: 5.5394e-04 | norm: 0.4673 | dt: 4007.78ms | tok/sec: 130817.72\n",
      "Step  271 | loss: 5.385507 | lr: 5.5341e-04 | norm: 0.5105 | dt: 4012.45ms | tok/sec: 130665.24\n",
      "Step  272 | loss: 5.385032 | lr: 5.5288e-04 | norm: 0.5711 | dt: 4015.16ms | tok/sec: 130577.06\n",
      "Step  273 | loss: 5.394453 | lr: 5.5235e-04 | norm: 0.5052 | dt: 4010.90ms | tok/sec: 130715.79\n",
      "Step  274 | loss: 5.342481 | lr: 5.5181e-04 | norm: 0.3992 | dt: 4011.90ms | tok/sec: 130683.09\n",
      "Step  275 | loss: 5.415928 | lr: 5.5128e-04 | norm: 0.3797 | dt: 4009.92ms | tok/sec: 130747.73\n",
      "Step  276 | loss: 5.414815 | lr: 5.5074e-04 | norm: 0.4007 | dt: 4009.63ms | tok/sec: 130757.30\n",
      "Step  277 | loss: 5.352221 | lr: 5.5019e-04 | norm: 0.3573 | dt: 4014.73ms | tok/sec: 130590.99\n",
      "Step  278 | loss: 5.530464 | lr: 5.4965e-04 | norm: 0.4290 | dt: 4010.89ms | tok/sec: 130716.14\n",
      "Step  279 | loss: 5.485792 | lr: 5.4910e-04 | norm: 0.4906 | dt: 4012.85ms | tok/sec: 130652.36\n",
      "Step  280 | loss: 5.515199 | lr: 5.4855e-04 | norm: 0.4952 | dt: 4011.28ms | tok/sec: 130703.48\n",
      "Step  281 | loss: 5.506637 | lr: 5.4799e-04 | norm: 0.6200 | dt: 4013.60ms | tok/sec: 130627.85\n",
      "Step  282 | loss: 5.485926 | lr: 5.4743e-04 | norm: 0.6534 | dt: 4013.60ms | tok/sec: 130627.88\n",
      "Step  283 | loss: 5.498466 | lr: 5.4687e-04 | norm: 0.6003 | dt: 4010.57ms | tok/sec: 130726.41\n",
      "Step  284 | loss: 5.546860 | lr: 5.4631e-04 | norm: 0.5364 | dt: 4012.44ms | tok/sec: 130665.65\n",
      "Step  285 | loss: 5.465843 | lr: 5.4575e-04 | norm: 0.6297 | dt: 4013.60ms | tok/sec: 130627.87\n",
      "Step  286 | loss: 5.498127 | lr: 5.4518e-04 | norm: 0.5540 | dt: 4012.58ms | tok/sec: 130661.08\n",
      "Step  287 | loss: 5.479342 | lr: 5.4461e-04 | norm: 0.5013 | dt: 4010.97ms | tok/sec: 130713.43\n",
      "Step  288 | loss: 5.436603 | lr: 5.4404e-04 | norm: 0.4953 | dt: 4010.99ms | tok/sec: 130712.92\n",
      "Step  289 | loss: 5.439283 | lr: 5.4346e-04 | norm: 0.5282 | dt: 4012.01ms | tok/sec: 130679.62\n",
      "Step  290 | loss: 5.389308 | lr: 5.4289e-04 | norm: 0.4706 | dt: 4012.62ms | tok/sec: 130659.79\n",
      "Step  291 | loss: 5.398180 | lr: 5.4230e-04 | norm: 0.4181 | dt: 4008.25ms | tok/sec: 130802.27\n",
      "Step  292 | loss: 5.360751 | lr: 5.4172e-04 | norm: 0.4758 | dt: 4006.54ms | tok/sec: 130858.21\n",
      "Step  293 | loss: 5.392671 | lr: 5.4114e-04 | norm: 0.4388 | dt: 4008.58ms | tok/sec: 130791.46\n",
      "Step  294 | loss: 5.441202 | lr: 5.4055e-04 | norm: 0.4453 | dt: 4008.05ms | tok/sec: 130808.90\n",
      "Step  295 | loss: 5.388661 | lr: 5.3996e-04 | norm: 0.4716 | dt: 4005.38ms | tok/sec: 130895.87\n",
      "Step  296 | loss: 5.395490 | lr: 5.3936e-04 | norm: 0.4935 | dt: 4004.25ms | tok/sec: 130933.04\n",
      "Step  297 | loss: 5.401415 | lr: 5.3877e-04 | norm: 0.6355 | dt: 4008.76ms | tok/sec: 130785.68\n",
      "Step  298 | loss: 5.406087 | lr: 5.3817e-04 | norm: 0.6896 | dt: 4007.11ms | tok/sec: 130839.40\n",
      "Step  299 | loss: 5.370973 | lr: 5.3757e-04 | norm: 0.5484 | dt: 4005.56ms | tok/sec: 130890.13\n",
      "Validation loss: 5.3726\n",
      "sample 0: Hello, I'm a language model, I'm also heard that would also had used before you ever been published I have been the years old and had been done\n",
      "sample 1: Hello, I'm a language model, but if I do it seems, is not. \"No one, however, it would be the fact I give you\n",
      "sample 2: Hello, I'm a language model, but I also worked more information about what I did not know this as the same for this in the reader.\n",
      "To\n",
      "sample 3: Hello, I'm a language model, the teacher, I'm not just read it.\"\n",
      "I'd need to read anything like \"Do anyone and maybe something\n",
      "Step  300 | loss: 5.441553 | lr: 5.3697e-04 | norm: 0.5051 | dt: 4003.17ms | tok/sec: 130968.29\n",
      "Step  301 | loss: 5.309871 | lr: 5.3636e-04 | norm: 0.4601 | dt: 4004.43ms | tok/sec: 130926.91\n",
      "Step  302 | loss: 5.333239 | lr: 5.3575e-04 | norm: 0.5625 | dt: 4005.41ms | tok/sec: 130894.97\n",
      "Step  303 | loss: 5.325942 | lr: 5.3514e-04 | norm: 0.5322 | dt: 4006.43ms | tok/sec: 130861.49\n",
      "Step  304 | loss: 5.262124 | lr: 5.3453e-04 | norm: 0.5727 | dt: 4004.00ms | tok/sec: 130941.11\n",
      "Step  305 | loss: 5.289741 | lr: 5.3391e-04 | norm: 0.6098 | dt: 4005.45ms | tok/sec: 130893.54\n",
      "Step  306 | loss: 5.304240 | lr: 5.3329e-04 | norm: 0.5334 | dt: 4005.87ms | tok/sec: 130879.92\n",
      "Step  307 | loss: 5.273226 | lr: 5.3267e-04 | norm: 0.5449 | dt: 4002.43ms | tok/sec: 130992.42\n",
      "Step  308 | loss: 5.309465 | lr: 5.3205e-04 | norm: 0.5591 | dt: 4002.81ms | tok/sec: 130980.06\n",
      "Step  309 | loss: 5.246351 | lr: 5.3142e-04 | norm: 0.4962 | dt: 4002.20ms | tok/sec: 131000.04\n",
      "Step  310 | loss: 5.251406 | lr: 5.3080e-04 | norm: 0.4571 | dt: 4004.25ms | tok/sec: 130933.02\n",
      "Step  311 | loss: 5.251853 | lr: 5.3017e-04 | norm: 0.4972 | dt: 3999.63ms | tok/sec: 131084.24\n",
      "Step  312 | loss: 5.259471 | lr: 5.2953e-04 | norm: 0.5993 | dt: 4003.15ms | tok/sec: 130969.00\n",
      "Step  313 | loss: 5.197933 | lr: 5.2890e-04 | norm: 0.6076 | dt: 4004.97ms | tok/sec: 130909.43\n",
      "Step  314 | loss: 5.221343 | lr: 5.2826e-04 | norm: 0.5084 | dt: 4002.84ms | tok/sec: 130979.05\n",
      "Step  315 | loss: 5.176671 | lr: 5.2762e-04 | norm: 0.5541 | dt: 4000.11ms | tok/sec: 131068.53\n",
      "Step  316 | loss: 5.226614 | lr: 5.2698e-04 | norm: 0.4642 | dt: 4003.74ms | tok/sec: 130949.47\n",
      "Step  317 | loss: 5.182449 | lr: 5.2633e-04 | norm: 0.4987 | dt: 4000.97ms | tok/sec: 131040.08\n",
      "Step  318 | loss: 5.210878 | lr: 5.2568e-04 | norm: 0.3634 | dt: 4005.63ms | tok/sec: 130887.68\n",
      "Step  319 | loss: 5.177180 | lr: 5.2503e-04 | norm: 0.3975 | dt: 4004.34ms | tok/sec: 130930.05\n",
      "Step  320 | loss: 5.116257 | lr: 5.2438e-04 | norm: 0.3799 | dt: 4004.62ms | tok/sec: 130920.75\n",
      "Step  321 | loss: 5.164499 | lr: 5.2373e-04 | norm: 0.4566 | dt: 4000.12ms | tok/sec: 131068.09\n",
      "Step  322 | loss: 5.134036 | lr: 5.2307e-04 | norm: 0.5465 | dt: 4004.25ms | tok/sec: 130932.90\n",
      "Step  323 | loss: 5.149122 | lr: 5.2241e-04 | norm: 0.6725 | dt: 4005.18ms | tok/sec: 130902.44\n",
      "Step  324 | loss: 5.313421 | lr: 5.2175e-04 | norm: 0.7491 | dt: 4003.97ms | tok/sec: 130942.11\n",
      "Step  325 | loss: 5.370843 | lr: 5.2109e-04 | norm: 0.6750 | dt: 4003.72ms | tok/sec: 130950.25\n",
      "Step  326 | loss: 5.313921 | lr: 5.2042e-04 | norm: 0.5947 | dt: 4002.53ms | tok/sec: 130989.25\n",
      "Step  327 | loss: 5.322160 | lr: 5.1975e-04 | norm: 0.5919 | dt: 4006.09ms | tok/sec: 130872.65\n",
      "Step  328 | loss: 5.334528 | lr: 5.1908e-04 | norm: 0.5981 | dt: 4004.54ms | tok/sec: 130923.35\n",
      "Step  329 | loss: 5.282851 | lr: 5.1841e-04 | norm: 0.5193 | dt: 4008.16ms | tok/sec: 130805.28\n",
      "Step  330 | loss: 5.332818 | lr: 5.1773e-04 | norm: 0.4740 | dt: 4005.96ms | tok/sec: 130876.90\n",
      "Step  331 | loss: 5.279363 | lr: 5.1705e-04 | norm: 0.4758 | dt: 4005.66ms | tok/sec: 130886.89\n",
      "Step  332 | loss: 5.323884 | lr: 5.1637e-04 | norm: 0.4439 | dt: 4004.98ms | tok/sec: 130909.04\n",
      "Step  333 | loss: 5.281062 | lr: 5.1569e-04 | norm: 0.5037 | dt: 4008.39ms | tok/sec: 130797.56\n",
      "Step  334 | loss: 5.279460 | lr: 5.1501e-04 | norm: 0.5290 | dt: 4006.96ms | tok/sec: 130844.41\n",
      "Step  335 | loss: 5.311852 | lr: 5.1432e-04 | norm: 0.4917 | dt: 4005.25ms | tok/sec: 130900.16\n",
      "Step  336 | loss: 5.202378 | lr: 5.1363e-04 | norm: 0.5154 | dt: 4009.18ms | tok/sec: 130772.00\n",
      "Step  337 | loss: 5.237278 | lr: 5.1294e-04 | norm: 0.5649 | dt: 4004.32ms | tok/sec: 130930.73\n",
      "Step  338 | loss: 5.272789 | lr: 5.1225e-04 | norm: 0.5489 | dt: 4006.60ms | tok/sec: 130856.08\n",
      "Step  339 | loss: 5.237862 | lr: 5.1155e-04 | norm: 0.6689 | dt: 4007.27ms | tok/sec: 130834.15\n",
      "Step  340 | loss: 5.251271 | lr: 5.1085e-04 | norm: 0.6368 | dt: 4008.92ms | tok/sec: 130780.39\n",
      "Step  341 | loss: 5.223159 | lr: 5.1015e-04 | norm: 0.5316 | dt: 4007.53ms | tok/sec: 130825.85\n",
      "Step  342 | loss: 5.272510 | lr: 5.0945e-04 | norm: 0.4791 | dt: 4009.49ms | tok/sec: 130761.73\n",
      "Step  343 | loss: 5.169418 | lr: 5.0874e-04 | norm: 0.4667 | dt: 4006.72ms | tok/sec: 130852.07\n",
      "Step  344 | loss: 5.194581 | lr: 5.0804e-04 | norm: 0.4940 | dt: 4007.90ms | tok/sec: 130813.67\n",
      "Step  345 | loss: 5.148282 | lr: 5.0733e-04 | norm: 0.4513 | dt: 4006.09ms | tok/sec: 130872.83\n",
      "Step  346 | loss: 5.143520 | lr: 5.0662e-04 | norm: 0.4729 | dt: 4009.40ms | tok/sec: 130764.80\n",
      "Step  347 | loss: 5.171524 | lr: 5.0591e-04 | norm: 0.4598 | dt: 4007.58ms | tok/sec: 130823.94\n",
      "Step  348 | loss: 5.113807 | lr: 5.0519e-04 | norm: 0.3872 | dt: 4009.82ms | tok/sec: 130751.07\n",
      "Step  349 | loss: 5.130660 | lr: 5.0447e-04 | norm: 0.5284 | dt: 4008.53ms | tok/sec: 130793.15\n",
      "Validation loss: 5.1661\n",
      "sample 0: Hello, I'm a language model, I'm the first time we would say on their work, but then for our students in them on a different class that\n",
      "sample 1: Hello, I'm a language model, but was done to the group to start the same numbers before they did just like a group of people around.\n",
      "�\n",
      "sample 2: Hello, I'm a language model, and I didn’t all my own ideas, I don’t be able to get a lot of how\n",
      "sample 3: Hello, I'm a language model, but as I am going to have been able to make a problem about, I. And he said, “When\n",
      "Step  350 | loss: 5.118645 | lr: 5.0375e-04 | norm: 0.5226 | dt: 4005.01ms | tok/sec: 130908.09\n",
      "Step  351 | loss: 5.145392 | lr: 5.0303e-04 | norm: 0.4794 | dt: 4013.45ms | tok/sec: 130632.91\n",
      "Step  352 | loss: 5.100626 | lr: 5.0231e-04 | norm: 0.5209 | dt: 4007.72ms | tok/sec: 130819.50\n",
      "Step  353 | loss: 5.090827 | lr: 5.0158e-04 | norm: 0.5731 | dt: 4009.31ms | tok/sec: 130767.52\n",
      "Step  354 | loss: 5.162989 | lr: 5.0085e-04 | norm: 0.7172 | dt: 4008.81ms | tok/sec: 130784.06\n",
      "Step  355 | loss: 5.111632 | lr: 5.0012e-04 | norm: 0.6127 | dt: 4013.18ms | tok/sec: 130641.42\n",
      "Step  356 | loss: 5.065694 | lr: 4.9939e-04 | norm: 0.5261 | dt: 4011.20ms | tok/sec: 130705.95\n",
      "Step  357 | loss: 5.073442 | lr: 4.9866e-04 | norm: 0.4975 | dt: 4008.65ms | tok/sec: 130789.21\n",
      "Step  358 | loss: 5.088508 | lr: 4.9792e-04 | norm: 0.5358 | dt: 4012.36ms | tok/sec: 130668.31\n",
      "Step  359 | loss: 5.006855 | lr: 4.9718e-04 | norm: 0.4981 | dt: 4010.38ms | tok/sec: 130732.76\n",
      "Step  360 | loss: 5.035760 | lr: 4.9644e-04 | norm: 0.4841 | dt: 4006.93ms | tok/sec: 130845.47\n",
      "Step  361 | loss: 5.042352 | lr: 4.9570e-04 | norm: 0.5301 | dt: 4009.16ms | tok/sec: 130772.42\n",
      "Step  362 | loss: 5.086475 | lr: 4.9496e-04 | norm: 0.5234 | dt: 4009.60ms | tok/sec: 130758.04\n",
      "Step  363 | loss: 4.999403 | lr: 4.9421e-04 | norm: 0.6217 | dt: 4011.93ms | tok/sec: 130682.30\n",
      "Step  364 | loss: 5.031263 | lr: 4.9346e-04 | norm: 0.6974 | dt: 4009.18ms | tok/sec: 130771.78\n",
      "Step  365 | loss: 5.012608 | lr: 4.9271e-04 | norm: 0.5878 | dt: 4009.22ms | tok/sec: 130770.44\n",
      "Step  366 | loss: 4.948490 | lr: 4.9196e-04 | norm: 0.6280 | dt: 4010.18ms | tok/sec: 130739.23\n",
      "Step  367 | loss: 5.015125 | lr: 4.9120e-04 | norm: 0.6340 | dt: 4008.92ms | tok/sec: 130780.36\n",
      "Step  368 | loss: 5.028992 | lr: 4.9045e-04 | norm: 0.6363 | dt: 4011.66ms | tok/sec: 130690.97\n",
      "Step  369 | loss: 4.968631 | lr: 4.8969e-04 | norm: 0.6354 | dt: 4008.02ms | tok/sec: 130809.72\n",
      "Step  370 | loss: 5.017248 | lr: 4.8893e-04 | norm: 0.5250 | dt: 4008.41ms | tok/sec: 130796.95\n",
      "Step  371 | loss: 5.137836 | lr: 4.8817e-04 | norm: 0.5504 | dt: 4008.67ms | tok/sec: 130788.40\n",
      "Step  372 | loss: 5.214021 | lr: 4.8740e-04 | norm: 0.6013 | dt: 4007.87ms | tok/sec: 130814.65\n",
      "Step  373 | loss: 5.113877 | lr: 4.8664e-04 | norm: 0.5418 | dt: 4009.48ms | tok/sec: 130762.23\n",
      "Step  374 | loss: 5.087231 | lr: 4.8587e-04 | norm: 0.4675 | dt: 4006.68ms | tok/sec: 130853.40\n",
      "Step  375 | loss: 5.144921 | lr: 4.8510e-04 | norm: 0.4455 | dt: 4006.41ms | tok/sec: 130862.41\n",
      "Step  376 | loss: 5.153790 | lr: 4.8433e-04 | norm: 0.4774 | dt: 4002.56ms | tok/sec: 130988.18\n",
      "Step  377 | loss: 5.147441 | lr: 4.8356e-04 | norm: 0.5326 | dt: 4007.11ms | tok/sec: 130839.58\n",
      "Step  378 | loss: 5.168790 | lr: 4.8278e-04 | norm: 0.5482 | dt: 4004.40ms | tok/sec: 130927.85\n",
      "Step  379 | loss: 5.077343 | lr: 4.8200e-04 | norm: 0.5831 | dt: 4003.21ms | tok/sec: 130966.75\n",
      "Step  380 | loss: 5.111677 | lr: 4.8122e-04 | norm: 0.5765 | dt: 4003.74ms | tok/sec: 130949.42\n",
      "Step  381 | loss: 5.123574 | lr: 4.8044e-04 | norm: 0.5425 | dt: 4004.16ms | tok/sec: 130935.68\n",
      "Step  382 | loss: 5.068036 | lr: 4.7966e-04 | norm: 0.6091 | dt: 4006.86ms | tok/sec: 130847.46\n",
      "Step  383 | loss: 5.085484 | lr: 4.7888e-04 | norm: 0.5541 | dt: 4001.38ms | tok/sec: 131026.91\n",
      "Step  384 | loss: 5.036314 | lr: 4.7809e-04 | norm: 0.5133 | dt: 4002.83ms | tok/sec: 130979.30\n",
      "Step  385 | loss: 5.108409 | lr: 4.7730e-04 | norm: 0.5348 | dt: 4003.15ms | tok/sec: 130968.77\n",
      "Step  386 | loss: 5.147556 | lr: 4.7651e-04 | norm: 0.5411 | dt: 4001.58ms | tok/sec: 131020.36\n",
      "Step  387 | loss: 5.094999 | lr: 4.7572e-04 | norm: 0.5323 | dt: 4002.85ms | tok/sec: 130978.68\n",
      "Step  388 | loss: 5.016064 | lr: 4.7493e-04 | norm: 0.4624 | dt: 4000.76ms | tok/sec: 131047.11\n",
      "Step  389 | loss: 5.071496 | lr: 4.7413e-04 | norm: 0.4137 | dt: 4002.84ms | tok/sec: 130979.00\n",
      "Step  390 | loss: 5.026320 | lr: 4.7334e-04 | norm: 0.4037 | dt: 3999.34ms | tok/sec: 131093.75\n",
      "Step  391 | loss: 5.035964 | lr: 4.7254e-04 | norm: 0.4078 | dt: 4001.43ms | tok/sec: 131025.27\n",
      "Step  392 | loss: 4.989970 | lr: 4.7174e-04 | norm: 0.4554 | dt: 3998.19ms | tok/sec: 131131.35\n",
      "Step  393 | loss: 5.017409 | lr: 4.7093e-04 | norm: 0.4830 | dt: 3999.68ms | tok/sec: 131082.50\n",
      "Step  394 | loss: 4.945720 | lr: 4.7013e-04 | norm: 0.4803 | dt: 4000.15ms | tok/sec: 131067.18\n",
      "Step  395 | loss: 4.913321 | lr: 4.6932e-04 | norm: 0.5357 | dt: 4001.81ms | tok/sec: 131012.77\n",
      "Step  396 | loss: 4.970070 | lr: 4.6852e-04 | norm: 0.6326 | dt: 3999.23ms | tok/sec: 131097.29\n",
      "Step  397 | loss: 4.951089 | lr: 4.6771e-04 | norm: 0.6660 | dt: 4000.97ms | tok/sec: 131040.20\n",
      "Step  398 | loss: 4.955821 | lr: 4.6690e-04 | norm: 0.7013 | dt: 4001.96ms | tok/sec: 131007.86\n",
      "Step  399 | loss: 4.975824 | lr: 4.6609e-04 | norm: 0.7264 | dt: 3999.79ms | tok/sec: 131078.95\n",
      "Validation loss: 5.0028\n",
      "sample 0: Hello, I'm a language model, and I would have to understand it from his class.\n",
      "I do are all the world today!<|endoftext|>It is important\n",
      "sample 1: Hello, I'm a language model, I really taught me, I can now know it? This has a long term, but I have many things, but\n",
      "sample 2: Hello, I'm a language model, I'm so many it just an answer to its answer. You need to know, but you can not just know and\n",
      "sample 3: Hello, I'm a language model, I know how to do something that in my system?\n",
      "I would want a really important to understand\n",
      "I have\n",
      "\n",
      "Step  400 | loss: 4.949488 | lr: 4.6527e-04 | norm: 0.5673 | dt: 3995.35ms | tok/sec: 131224.69\n",
      "Step  401 | loss: 4.958777 | lr: 4.6446e-04 | norm: 0.4743 | dt: 3997.22ms | tok/sec: 131163.01\n",
      "Step  402 | loss: 4.921843 | lr: 4.6364e-04 | norm: 0.4662 | dt: 3996.71ms | tok/sec: 131179.90\n",
      "Step  403 | loss: 4.934739 | lr: 4.6282e-04 | norm: 0.4421 | dt: 3998.47ms | tok/sec: 131122.16\n",
      "Step  404 | loss: 4.926871 | lr: 4.6200e-04 | norm: 0.4997 | dt: 3998.18ms | tok/sec: 131131.78\n",
      "Step  405 | loss: 4.890462 | lr: 4.6118e-04 | norm: 0.5108 | dt: 4000.10ms | tok/sec: 131068.73\n",
      "Step  406 | loss: 4.836745 | lr: 4.6035e-04 | norm: 0.4937 | dt: 4002.10ms | tok/sec: 131003.08\n",
      "Step  407 | loss: 4.812016 | lr: 4.5953e-04 | norm: 0.4336 | dt: 3997.54ms | tok/sec: 131152.71\n",
      "Step  408 | loss: 4.816823 | lr: 4.5870e-04 | norm: 0.4609 | dt: 3998.68ms | tok/sec: 131115.17\n",
      "Step  409 | loss: 4.865726 | lr: 4.5787e-04 | norm: 0.5691 | dt: 3998.26ms | tok/sec: 131129.16\n",
      "Step  410 | loss: 4.854885 | lr: 4.5704e-04 | norm: 0.5578 | dt: 3999.09ms | tok/sec: 131101.82\n",
      "Step  411 | loss: 4.836709 | lr: 4.5621e-04 | norm: 0.4846 | dt: 4002.84ms | tok/sec: 130978.91\n",
      "Step  412 | loss: 4.873119 | lr: 4.5538e-04 | norm: 0.5761 | dt: 3998.97ms | tok/sec: 131105.73\n",
      "Step  413 | loss: 4.837407 | lr: 4.5454e-04 | norm: 0.6817 | dt: 3997.54ms | tok/sec: 131152.75\n",
      "Step  414 | loss: 4.859643 | lr: 4.5371e-04 | norm: 0.7614 | dt: 3999.57ms | tok/sec: 131086.20\n",
      "Step  415 | loss: 4.858501 | lr: 4.5287e-04 | norm: 0.7570 | dt: 4002.43ms | tok/sec: 130992.29\n",
      "Step  416 | loss: 4.918683 | lr: 4.5203e-04 | norm: 0.6669 | dt: 3999.05ms | tok/sec: 131103.18\n",
      "Step  417 | loss: 5.070436 | lr: 4.5119e-04 | norm: 0.6453 | dt: 4003.00ms | tok/sec: 130973.68\n",
      "Step  418 | loss: 5.033452 | lr: 4.5035e-04 | norm: 0.5806 | dt: 4001.56ms | tok/sec: 131020.86\n",
      "Step  419 | loss: 5.026863 | lr: 4.4951e-04 | norm: 0.5452 | dt: 3999.05ms | tok/sec: 131103.05\n",
      "Step  420 | loss: 4.987125 | lr: 4.4866e-04 | norm: 0.5071 | dt: 4000.50ms | tok/sec: 131055.56\n",
      "Step  421 | loss: 5.029173 | lr: 4.4781e-04 | norm: 0.4661 | dt: 3998.58ms | tok/sec: 131118.46\n",
      "Step  422 | loss: 4.967715 | lr: 4.4697e-04 | norm: 0.5170 | dt: 4000.46ms | tok/sec: 131056.85\n",
      "Step  423 | loss: 5.075885 | lr: 4.4612e-04 | norm: 0.5412 | dt: 3996.98ms | tok/sec: 131170.91\n",
      "Step  424 | loss: 4.990236 | lr: 4.4527e-04 | norm: 0.5575 | dt: 3999.45ms | tok/sec: 131089.92\n",
      "Step  425 | loss: 4.980935 | lr: 4.4441e-04 | norm: 0.5590 | dt: 3999.35ms | tok/sec: 131093.46\n",
      "Step  426 | loss: 4.917517 | lr: 4.4356e-04 | norm: 0.4941 | dt: 3999.26ms | tok/sec: 131096.20\n",
      "Step  427 | loss: 4.948306 | lr: 4.4271e-04 | norm: 0.5482 | dt: 4000.25ms | tok/sec: 131063.67\n",
      "Step  428 | loss: 4.916432 | lr: 4.4185e-04 | norm: 0.5596 | dt: 4000.99ms | tok/sec: 131039.53\n",
      "Step  429 | loss: 5.047492 | lr: 4.4099e-04 | norm: 0.6949 | dt: 3996.79ms | tok/sec: 131177.28\n",
      "Step  430 | loss: 4.985476 | lr: 4.4013e-04 | norm: 0.7459 | dt: 3997.90ms | tok/sec: 131140.94\n",
      "Step  431 | loss: 4.928615 | lr: 4.3927e-04 | norm: 0.5728 | dt: 3997.27ms | tok/sec: 131161.66\n",
      "Step  432 | loss: 4.988621 | lr: 4.3841e-04 | norm: 0.4758 | dt: 4000.24ms | tok/sec: 131063.98\n",
      "Step  433 | loss: 5.030114 | lr: 4.3755e-04 | norm: 0.4825 | dt: 4001.27ms | tok/sec: 131030.33\n",
      "Step  434 | loss: 4.863038 | lr: 4.3669e-04 | norm: 0.4579 | dt: 4002.62ms | tok/sec: 130986.10\n",
      "Step  435 | loss: 4.921359 | lr: 4.3582e-04 | norm: 0.4889 | dt: 4000.18ms | tok/sec: 131066.02\n",
      "Step  436 | loss: 4.859550 | lr: 4.3495e-04 | norm: 0.4256 | dt: 4000.33ms | tok/sec: 131061.22\n",
      "Step  437 | loss: 5.013210 | lr: 4.3409e-04 | norm: 0.4525 | dt: 4002.32ms | tok/sec: 130995.86\n",
      "Step  438 | loss: 4.944908 | lr: 4.3322e-04 | norm: 0.4688 | dt: 3999.27ms | tok/sec: 131095.84\n",
      "Step  439 | loss: 4.960838 | lr: 4.3235e-04 | norm: 0.5623 | dt: 4000.86ms | tok/sec: 131043.97\n",
      "Step  440 | loss: 4.770745 | lr: 4.3147e-04 | norm: 0.5900 | dt: 3999.08ms | tok/sec: 131102.00\n",
      "Step  441 | loss: 4.829983 | lr: 4.3060e-04 | norm: 0.6108 | dt: 4000.84ms | tok/sec: 131044.58\n",
      "Step  442 | loss: 4.820161 | lr: 4.2973e-04 | norm: 0.6048 | dt: 3998.43ms | tok/sec: 131123.62\n",
      "Step  443 | loss: 4.800845 | lr: 4.2885e-04 | norm: 0.5459 | dt: 4001.93ms | tok/sec: 131008.84\n",
      "Step  444 | loss: 4.833313 | lr: 4.2797e-04 | norm: 0.5819 | dt: 4001.64ms | tok/sec: 131018.19\n",
      "Step  445 | loss: 4.847327 | lr: 4.2710e-04 | norm: 0.6278 | dt: 3999.15ms | tok/sec: 131099.84\n",
      "Step  446 | loss: 4.821536 | lr: 4.2622e-04 | norm: 0.5559 | dt: 4001.14ms | tok/sec: 131034.65\n",
      "Step  447 | loss: 4.856936 | lr: 4.2534e-04 | norm: 0.5398 | dt: 4002.08ms | tok/sec: 131003.93\n",
      "Step  448 | loss: 4.870995 | lr: 4.2446e-04 | norm: 0.4873 | dt: 4000.56ms | tok/sec: 131053.57\n",
      "Step  449 | loss: 4.793275 | lr: 4.2357e-04 | norm: 0.4423 | dt: 4000.87ms | tok/sec: 131043.65\n",
      "Validation loss: 4.8479\n",
      "sample 0: Hello, I'm a language model, I'm just a lot of them with different ideas on people, my words!\n",
      "So I think it was a good\n",
      "sample 1: Hello, I'm a language model, if all the language is being created so that is learning into the language by using language and language, where language is taught\n",
      "sample 2: Hello, I'm a language model, and I wanted me to think about my language-schools with language and language. I could be able to understand more\n",
      "sample 3: Hello, I'm a language model, which’s a whole, in which sense, in relation to one another from other nations, where it was called\n",
      "Step  450 | loss: 4.811118 | lr: 4.2269e-04 | norm: 0.4291 | dt: 3998.25ms | tok/sec: 131129.53\n",
      "Step  451 | loss: 4.833608 | lr: 4.2180e-04 | norm: 0.4239 | dt: 4000.81ms | tok/sec: 131045.61\n",
      "Step  452 | loss: 4.692750 | lr: 4.2092e-04 | norm: 0.4586 | dt: 4001.37ms | tok/sec: 131027.13\n",
      "Step  453 | loss: 4.742546 | lr: 4.2003e-04 | norm: 0.5926 | dt: 4002.46ms | tok/sec: 130991.57\n",
      "Step  454 | loss: 4.747259 | lr: 4.1914e-04 | norm: 0.6415 | dt: 4005.84ms | tok/sec: 130880.96\n",
      "Step  455 | loss: 4.671097 | lr: 4.1825e-04 | norm: 0.5558 | dt: 4001.47ms | tok/sec: 131024.00\n",
      "Step  456 | loss: 4.759461 | lr: 4.1736e-04 | norm: 0.5536 | dt: 4002.54ms | tok/sec: 130988.87\n",
      "Step  457 | loss: 4.712386 | lr: 4.1647e-04 | norm: 0.4671 | dt: 4000.15ms | tok/sec: 131067.20\n",
      "Step  458 | loss: 4.720759 | lr: 4.1558e-04 | norm: 0.4766 | dt: 4003.26ms | tok/sec: 130965.23\n",
      "Step  459 | loss: 4.776381 | lr: 4.1469e-04 | norm: 0.4721 | dt: 4001.59ms | tok/sec: 131020.06\n",
      "Step  460 | loss: 4.685829 | lr: 4.1379e-04 | norm: 0.5177 | dt: 4003.48ms | tok/sec: 130958.12\n",
      "Step  461 | loss: 4.682884 | lr: 4.1290e-04 | norm: 0.5606 | dt: 4004.92ms | tok/sec: 130910.88\n",
      "Step  462 | loss: 4.640473 | lr: 4.1200e-04 | norm: 0.6607 | dt: 4001.41ms | tok/sec: 131025.75\n",
      "Step  463 | loss: 4.746976 | lr: 4.1110e-04 | norm: 0.5316 | dt: 4001.39ms | tok/sec: 131026.40\n",
      "Step  464 | loss: 4.805393 | lr: 4.1020e-04 | norm: 0.4741 | dt: 4004.36ms | tok/sec: 130929.40\n",
      "Step  465 | loss: 4.820926 | lr: 4.0931e-04 | norm: 0.4524 | dt: 4003.72ms | tok/sec: 130950.09\n",
      "Step  466 | loss: 4.928306 | lr: 4.0841e-04 | norm: 0.5362 | dt: 4002.83ms | tok/sec: 130979.49\n",
      "Step  467 | loss: 4.826557 | lr: 4.0750e-04 | norm: 0.5779 | dt: 3999.84ms | tok/sec: 131077.38\n",
      "Step  468 | loss: 4.856486 | lr: 4.0660e-04 | norm: 0.5511 | dt: 4005.54ms | tok/sec: 130890.87\n",
      "Step  469 | loss: 4.839395 | lr: 4.0570e-04 | norm: 0.4723 | dt: 4002.50ms | tok/sec: 130989.98\n",
      "Step  470 | loss: 4.811923 | lr: 4.0479e-04 | norm: 0.5105 | dt: 4002.47ms | tok/sec: 130991.07\n",
      "Step  471 | loss: 4.785611 | lr: 4.0389e-04 | norm: 0.4806 | dt: 4000.81ms | tok/sec: 131045.51\n",
      "Step  472 | loss: 4.831133 | lr: 4.0298e-04 | norm: 0.4843 | dt: 4001.11ms | tok/sec: 131035.70\n",
      "Step  473 | loss: 4.830177 | lr: 4.0208e-04 | norm: 0.4259 | dt: 3999.57ms | tok/sec: 131086.24\n",
      "Step  474 | loss: 4.875032 | lr: 4.0117e-04 | norm: 0.4715 | dt: 4004.74ms | tok/sec: 130916.97\n",
      "Step  475 | loss: 4.832334 | lr: 4.0026e-04 | norm: 0.5650 | dt: 4003.55ms | tok/sec: 130955.78\n",
      "Step  476 | loss: 4.752059 | lr: 3.9935e-04 | norm: 0.5418 | dt: 4000.87ms | tok/sec: 131043.46\n",
      "Step  477 | loss: 4.876155 | lr: 3.9844e-04 | norm: 0.5301 | dt: 4003.70ms | tok/sec: 130950.99\n",
      "Step  478 | loss: 4.798811 | lr: 3.9753e-04 | norm: 0.7082 | dt: 4004.79ms | tok/sec: 130915.34\n",
      "Step  479 | loss: 4.870189 | lr: 3.9662e-04 | norm: 0.7198 | dt: 4001.58ms | tok/sec: 131020.25\n",
      "Step  480 | loss: 4.803099 | lr: 3.9570e-04 | norm: 0.6801 | dt: 4002.80ms | tok/sec: 130980.21\n",
      "Step  481 | loss: 4.834375 | lr: 3.9479e-04 | norm: 0.6176 | dt: 4002.97ms | tok/sec: 130974.85\n",
      "Step  482 | loss: 4.791782 | lr: 3.9388e-04 | norm: 0.5231 | dt: 4006.25ms | tok/sec: 130867.44\n",
      "Step  483 | loss: 4.832654 | lr: 3.9296e-04 | norm: 0.5082 | dt: 4005.10ms | tok/sec: 130905.26\n",
      "Step  484 | loss: 4.790745 | lr: 3.9205e-04 | norm: 0.5196 | dt: 4006.15ms | tok/sec: 130870.89\n",
      "Step  485 | loss: 4.803981 | lr: 3.9113e-04 | norm: 0.4689 | dt: 4005.52ms | tok/sec: 130891.35\n",
      "Step  486 | loss: 4.778092 | lr: 3.9021e-04 | norm: 0.5002 | dt: 4003.02ms | tok/sec: 130972.96\n",
      "Step  487 | loss: 4.768466 | lr: 3.8929e-04 | norm: 0.4757 | dt: 4005.25ms | tok/sec: 130900.23\n",
      "Step  488 | loss: 4.712655 | lr: 3.8837e-04 | norm: 0.4710 | dt: 4005.08ms | tok/sec: 130905.82\n",
      "Step  489 | loss: 4.672657 | lr: 3.8746e-04 | norm: 0.4773 | dt: 4007.21ms | tok/sec: 130836.26\n",
      "Step  490 | loss: 4.731831 | lr: 3.8654e-04 | norm: 0.5057 | dt: 4004.14ms | tok/sec: 130936.34\n",
      "Step  491 | loss: 4.697630 | lr: 3.8561e-04 | norm: 0.5073 | dt: 4008.08ms | tok/sec: 130807.64\n",
      "Step  492 | loss: 4.714349 | lr: 3.8469e-04 | norm: 0.5114 | dt: 4003.84ms | tok/sec: 130946.23\n",
      "Step  493 | loss: 4.685790 | lr: 3.8377e-04 | norm: 0.5939 | dt: 4006.50ms | tok/sec: 130859.49\n",
      "Step  494 | loss: 4.729761 | lr: 3.8285e-04 | norm: 0.6724 | dt: 4002.93ms | tok/sec: 130976.21\n",
      "Step  495 | loss: 4.682775 | lr: 3.8192e-04 | norm: 0.5737 | dt: 4010.92ms | tok/sec: 130715.25\n",
      "Step  496 | loss: 4.717742 | lr: 3.8100e-04 | norm: 0.5460 | dt: 4008.32ms | tok/sec: 130800.03\n",
      "Step  497 | loss: 4.676501 | lr: 3.8007e-04 | norm: 0.4652 | dt: 4008.33ms | tok/sec: 130799.54\n",
      "Step  498 | loss: 4.641187 | lr: 3.7915e-04 | norm: 0.5166 | dt: 4007.11ms | tok/sec: 130839.37\n",
      "Step  499 | loss: 4.578561 | lr: 3.7822e-04 | norm: 0.5802 | dt: 4007.85ms | tok/sec: 130815.29\n",
      "Validation loss: 4.7237\n",
      "sample 0: Hello, I'm a language model, I'm an English Languageist in German? Please let me know where people learn from language classes to the English language languages\n",
      "sample 1: Hello, I'm a language model, but with a language that is the perfect language that describes me as a concept that can make up a single language. In\n",
      "sample 2: Hello, I'm a language model, and I wanted my book as a guide to make my own word for your own language. I'm a teacher of the\n",
      "sample 3: Hello, I'm a language model, but with a new language, and, my English language, I will always see our language skills and learners.<|endoftext|>L\n",
      "Step  500 | loss: 4.685265 | lr: 3.7730e-04 | norm: 0.6333 | dt: 4006.72ms | tok/sec: 130852.02\n",
      "Step  501 | loss: 4.587549 | lr: 3.7637e-04 | norm: 0.5448 | dt: 4006.84ms | tok/sec: 130848.29\n",
      "Step  502 | loss: 4.611569 | lr: 3.7544e-04 | norm: 0.5236 | dt: 4007.22ms | tok/sec: 130835.79\n",
      "Step  503 | loss: 4.550051 | lr: 3.7451e-04 | norm: 0.4489 | dt: 4010.02ms | tok/sec: 130744.60\n",
      "Step  504 | loss: 4.648337 | lr: 3.7359e-04 | norm: 0.4297 | dt: 4006.11ms | tok/sec: 130872.14\n",
      "Step  505 | loss: 4.580514 | lr: 3.7266e-04 | norm: 0.4754 | dt: 4010.42ms | tok/sec: 130731.54\n",
      "Step  506 | loss: 4.694901 | lr: 3.7173e-04 | norm: 0.4571 | dt: 4007.33ms | tok/sec: 130832.17\n",
      "Step  507 | loss: 4.549613 | lr: 3.7080e-04 | norm: 0.5646 | dt: 4009.12ms | tok/sec: 130773.77\n",
      "Step  508 | loss: 4.513496 | lr: 3.6986e-04 | norm: 0.5711 | dt: 4007.59ms | tok/sec: 130823.77\n",
      "Step  509 | loss: 4.580022 | lr: 3.6893e-04 | norm: 0.5174 | dt: 4012.05ms | tok/sec: 130678.31\n",
      "Step  510 | loss: 4.751281 | lr: 3.6800e-04 | norm: 0.5049 | dt: 4008.48ms | tok/sec: 130794.82\n",
      "Step  511 | loss: 4.741799 | lr: 3.6707e-04 | norm: 0.5350 | dt: 4007.03ms | tok/sec: 130842.04\n",
      "Step  512 | loss: 4.761588 | lr: 3.6614e-04 | norm: 0.6749 | dt: 4014.45ms | tok/sec: 130600.17\n",
      "Step  513 | loss: 4.740595 | lr: 3.6520e-04 | norm: 0.6678 | dt: 4013.62ms | tok/sec: 130627.37\n",
      "Step  514 | loss: 4.759989 | lr: 3.6427e-04 | norm: 0.6194 | dt: 4012.24ms | tok/sec: 130672.08\n",
      "Step  515 | loss: 4.747233 | lr: 3.6334e-04 | norm: 0.5263 | dt: 4011.52ms | tok/sec: 130695.51\n",
      "Step  516 | loss: 4.896396 | lr: 3.6240e-04 | norm: 0.5115 | dt: 4015.57ms | tok/sec: 130563.84\n",
      "Step  517 | loss: 4.805570 | lr: 3.6147e-04 | norm: 0.6433 | dt: 4009.78ms | tok/sec: 130752.34\n",
      "Step  518 | loss: 4.734996 | lr: 3.6053e-04 | norm: 0.6791 | dt: 4011.45ms | tok/sec: 130698.02\n",
      "Step  519 | loss: 4.800737 | lr: 3.5960e-04 | norm: 0.6501 | dt: 4010.72ms | tok/sec: 130721.61\n",
      "Step  520 | loss: 4.748330 | lr: 3.5866e-04 | norm: 0.5544 | dt: 4011.77ms | tok/sec: 130687.35\n",
      "Step  521 | loss: 4.734503 | lr: 3.5772e-04 | norm: 0.5209 | dt: 4009.49ms | tok/sec: 130761.68\n",
      "Step  522 | loss: 4.743684 | lr: 3.5679e-04 | norm: 0.5312 | dt: 4011.28ms | tok/sec: 130703.49\n",
      "Step  523 | loss: 4.676274 | lr: 3.5585e-04 | norm: 0.4901 | dt: 4016.63ms | tok/sec: 130529.44\n",
      "Step  524 | loss: 4.683026 | lr: 3.5491e-04 | norm: 0.5081 | dt: 4013.24ms | tok/sec: 130639.49\n",
      "Step  525 | loss: 4.690518 | lr: 3.5397e-04 | norm: 0.4735 | dt: 4014.34ms | tok/sec: 130603.81\n",
      "Step  526 | loss: 4.694732 | lr: 3.5304e-04 | norm: 0.4613 | dt: 4013.18ms | tok/sec: 130641.59\n",
      "Step  527 | loss: 4.678462 | lr: 3.5210e-04 | norm: 0.4464 | dt: 4012.75ms | tok/sec: 130655.69\n",
      "Step  528 | loss: 4.684228 | lr: 3.5116e-04 | norm: 0.4199 | dt: 4009.55ms | tok/sec: 130759.88\n",
      "Step  529 | loss: 4.664917 | lr: 3.5022e-04 | norm: 0.4424 | dt: 4013.29ms | tok/sec: 130638.06\n",
      "Step  530 | loss: 4.674242 | lr: 3.4928e-04 | norm: 0.4356 | dt: 4013.03ms | tok/sec: 130646.39\n",
      "Step  531 | loss: 4.678082 | lr: 3.4834e-04 | norm: 0.4346 | dt: 4014.54ms | tok/sec: 130597.16\n",
      "Step  532 | loss: 4.651985 | lr: 3.4740e-04 | norm: 0.5119 | dt: 4011.77ms | tok/sec: 130687.34\n",
      "Step  533 | loss: 4.647723 | lr: 3.4646e-04 | norm: 0.6004 | dt: 4008.91ms | tok/sec: 130780.82\n",
      "Step  534 | loss: 4.629706 | lr: 3.4553e-04 | norm: 0.6247 | dt: 4011.46ms | tok/sec: 130697.42\n",
      "Step  535 | loss: 4.624339 | lr: 3.4459e-04 | norm: 0.5423 | dt: 4013.23ms | tok/sec: 130640.01\n",
      "Step  536 | loss: 4.668147 | lr: 3.4364e-04 | norm: 0.5865 | dt: 4012.27ms | tok/sec: 130671.07\n",
      "Step  537 | loss: 4.579685 | lr: 3.4270e-04 | norm: 0.5050 | dt: 4011.69ms | tok/sec: 130689.94\n",
      "Step  538 | loss: 4.610247 | lr: 3.4176e-04 | norm: 0.5339 | dt: 4012.22ms | tok/sec: 130672.72\n",
      "Step  539 | loss: 4.620327 | lr: 3.4082e-04 | norm: 0.5836 | dt: 4010.89ms | tok/sec: 130716.20\n",
      "Step  540 | loss: 4.570295 | lr: 3.3988e-04 | norm: 0.5388 | dt: 4010.41ms | tok/sec: 130731.84\n",
      "Step  541 | loss: 4.556475 | lr: 3.3894e-04 | norm: 0.4869 | dt: 4009.82ms | tok/sec: 130751.13\n",
      "Step  542 | loss: 4.626235 | lr: 3.3800e-04 | norm: 0.5001 | dt: 4012.59ms | tok/sec: 130660.80\n",
      "Step  543 | loss: 4.591413 | lr: 3.3706e-04 | norm: 0.4474 | dt: 4007.29ms | tok/sec: 130833.44\n",
      "Step  544 | loss: 4.612913 | lr: 3.3612e-04 | norm: 0.4991 | dt: 4008.76ms | tok/sec: 130785.73\n",
      "Step  545 | loss: 4.511235 | lr: 3.3518e-04 | norm: 0.5118 | dt: 4013.00ms | tok/sec: 130647.24\n",
      "Step  546 | loss: 4.478464 | lr: 3.3424e-04 | norm: 0.5128 | dt: 4008.40ms | tok/sec: 130797.45\n",
      "Step  547 | loss: 4.472565 | lr: 3.3329e-04 | norm: 0.5556 | dt: 4008.02ms | tok/sec: 130809.65\n",
      "Step  548 | loss: 4.516006 | lr: 3.3235e-04 | norm: 0.6238 | dt: 4005.53ms | tok/sec: 130891.09\n",
      "Step  549 | loss: 4.489066 | lr: 3.3141e-04 | norm: 0.5871 | dt: 4011.89ms | tok/sec: 130683.42\n",
      "Validation loss: 4.6098\n",
      "sample 0: Hello, I'm a language model, I'm an example, so the second letter-based statement. As of all, we just like to get a letter\n",
      "sample 1: Hello, I'm a language model, in general, and the concept of complexity, it says ‘to add,” you need something new to be\n",
      "sample 2: Hello, I'm a language model, and I love my word is: a word-read-and-on-it. I'm a word for my\n",
      "sample 3: Hello, I'm a language model, you see the idea of the word. You were using the English language for you can write up a great language and can\n",
      "Step  550 | loss: 4.532810 | lr: 3.3047e-04 | norm: 0.5760 | dt: 4002.22ms | tok/sec: 130999.40\n",
      "Step  551 | loss: 4.481781 | lr: 3.2953e-04 | norm: 0.5378 | dt: 4005.47ms | tok/sec: 130893.16\n",
      "Step  552 | loss: 4.429467 | lr: 3.2859e-04 | norm: 0.4990 | dt: 4008.10ms | tok/sec: 130807.15\n",
      "Step  553 | loss: 4.568906 | lr: 3.2765e-04 | norm: 0.5099 | dt: 4009.11ms | tok/sec: 130774.07\n",
      "Step  554 | loss: 4.477615 | lr: 3.2671e-04 | norm: 0.5985 | dt: 4007.62ms | tok/sec: 130822.82\n",
      "Step  555 | loss: 4.452344 | lr: 3.2576e-04 | norm: 0.6634 | dt: 4006.78ms | tok/sec: 130850.25\n",
      "Step  556 | loss: 4.523690 | lr: 3.2482e-04 | norm: 0.5523 | dt: 4004.71ms | tok/sec: 130917.75\n",
      "Step  557 | loss: 4.642342 | lr: 3.2388e-04 | norm: 0.5816 | dt: 4004.75ms | tok/sec: 130916.68\n",
      "Step  558 | loss: 4.636599 | lr: 3.2294e-04 | norm: 0.6660 | dt: 4005.21ms | tok/sec: 130901.38\n",
      "Step  559 | loss: 4.612335 | lr: 3.2200e-04 | norm: 0.5665 | dt: 4006.69ms | tok/sec: 130853.05\n",
      "Step  560 | loss: 4.659720 | lr: 3.2106e-04 | norm: 0.5313 | dt: 4004.25ms | tok/sec: 130932.76\n",
      "Step  561 | loss: 4.601079 | lr: 3.2012e-04 | norm: 0.5928 | dt: 4005.57ms | tok/sec: 130889.72\n",
      "Step  562 | loss: 4.619303 | lr: 3.1918e-04 | norm: 0.5797 | dt: 4006.27ms | tok/sec: 130866.78\n",
      "Step  563 | loss: 4.603563 | lr: 3.1824e-04 | norm: 0.5254 | dt: 4006.39ms | tok/sec: 130863.05\n",
      "Step  564 | loss: 4.632885 | lr: 3.1730e-04 | norm: 0.4938 | dt: 4008.98ms | tok/sec: 130778.25\n",
      "Step  565 | loss: 4.623827 | lr: 3.1636e-04 | norm: 0.4420 | dt: 4004.93ms | tok/sec: 130910.54\n",
      "Step  566 | loss: 4.597637 | lr: 3.1541e-04 | norm: 0.4668 | dt: 4006.93ms | tok/sec: 130845.28\n",
      "Step  567 | loss: 4.641338 | lr: 3.1447e-04 | norm: 0.4503 | dt: 4005.69ms | tok/sec: 130885.74\n",
      "Step  568 | loss: 4.583098 | lr: 3.1354e-04 | norm: 0.4209 | dt: 4005.98ms | tok/sec: 130876.32\n",
      "Step  569 | loss: 4.611696 | lr: 3.1260e-04 | norm: 0.4415 | dt: 4008.43ms | tok/sec: 130796.48\n",
      "Step  570 | loss: 4.557027 | lr: 3.1166e-04 | norm: 0.4502 | dt: 4005.46ms | tok/sec: 130893.29\n",
      "Step  571 | loss: 4.591298 | lr: 3.1072e-04 | norm: 0.4761 | dt: 4004.09ms | tok/sec: 130938.08\n",
      "Step  572 | loss: 4.559028 | lr: 3.0978e-04 | norm: 0.4779 | dt: 4208.04ms | tok/sec: 124591.85\n",
      "Step  573 | loss: 4.572700 | lr: 3.0884e-04 | norm: 0.5024 | dt: 4004.14ms | tok/sec: 130936.59\n",
      "Step  574 | loss: 4.550140 | lr: 3.0790e-04 | norm: 0.4999 | dt: 4004.08ms | tok/sec: 130938.40\n",
      "Step  575 | loss: 4.544423 | lr: 3.0696e-04 | norm: 0.5246 | dt: 4004.29ms | tok/sec: 130931.69\n",
      "Step  576 | loss: 4.575221 | lr: 3.0603e-04 | norm: 0.5221 | dt: 4001.89ms | tok/sec: 131010.11\n",
      "Step  577 | loss: 4.597609 | lr: 3.0509e-04 | norm: 0.6392 | dt: 4004.69ms | tok/sec: 130918.65\n",
      "Step  578 | loss: 4.584948 | lr: 3.0415e-04 | norm: 0.6588 | dt: 4004.84ms | tok/sec: 130913.66\n",
      "Step  579 | loss: 4.543902 | lr: 3.0321e-04 | norm: 0.4955 | dt: 4005.54ms | tok/sec: 130890.76\n",
      "Step  580 | loss: 4.490777 | lr: 3.0228e-04 | norm: 0.5014 | dt: 4006.23ms | tok/sec: 130868.06\n",
      "Step  581 | loss: 4.537664 | lr: 3.0134e-04 | norm: 0.4918 | dt: 4007.43ms | tok/sec: 130829.07\n",
      "Step  582 | loss: 4.396262 | lr: 3.0040e-04 | norm: 0.6026 | dt: 4002.94ms | tok/sec: 130975.87\n",
      "Step  583 | loss: 4.454403 | lr: 2.9947e-04 | norm: 0.5839 | dt: 4006.89ms | tok/sec: 130846.62\n",
      "Step  584 | loss: 4.507686 | lr: 2.9853e-04 | norm: 0.5435 | dt: 4004.39ms | tok/sec: 130928.42\n",
      "Step  585 | loss: 4.484200 | lr: 2.9760e-04 | norm: 0.5374 | dt: 4001.62ms | tok/sec: 131018.90\n",
      "Step  586 | loss: 4.474606 | lr: 2.9666e-04 | norm: 0.4823 | dt: 4004.73ms | tok/sec: 130917.29\n",
      "Step  587 | loss: 4.517489 | lr: 2.9573e-04 | norm: 0.5458 | dt: 4002.34ms | tok/sec: 130995.29\n",
      "Step  588 | loss: 4.535444 | lr: 2.9480e-04 | norm: 0.4348 | dt: 4008.31ms | tok/sec: 130800.17\n",
      "Step  589 | loss: 4.512658 | lr: 2.9386e-04 | norm: 0.4334 | dt: 4006.27ms | tok/sec: 130866.73\n",
      "Step  590 | loss: 4.543779 | lr: 2.9293e-04 | norm: 0.4980 | dt: 4006.33ms | tok/sec: 130864.94\n",
      "Step  591 | loss: 4.451828 | lr: 2.9200e-04 | norm: 0.4196 | dt: 4005.55ms | tok/sec: 130890.27\n",
      "Step  592 | loss: 4.344292 | lr: 2.9107e-04 | norm: 0.4842 | dt: 4003.78ms | tok/sec: 130948.37\n",
      "Step  593 | loss: 4.384855 | lr: 2.9014e-04 | norm: 0.5106 | dt: 4007.06ms | tok/sec: 130840.91\n",
      "Step  594 | loss: 4.372394 | lr: 2.8920e-04 | norm: 0.5658 | dt: 4006.97ms | tok/sec: 130844.14\n",
      "Step  595 | loss: 4.438849 | lr: 2.8827e-04 | norm: 0.7257 | dt: 4002.81ms | tok/sec: 130980.06\n",
      "Step  596 | loss: 4.368223 | lr: 2.8734e-04 | norm: 0.6318 | dt: 4003.28ms | tok/sec: 130964.65\n",
      "Step  597 | loss: 4.440718 | lr: 2.8641e-04 | norm: 0.6825 | dt: 4007.13ms | tok/sec: 130838.73\n",
      "Step  598 | loss: 4.419241 | lr: 2.8549e-04 | norm: 0.7014 | dt: 4007.13ms | tok/sec: 130838.90\n",
      "Step  599 | loss: 4.375722 | lr: 2.8456e-04 | norm: 0.6000 | dt: 4003.58ms | tok/sec: 130954.67\n",
      "Validation loss: 4.5048\n",
      "sample 0: Hello, I'm a language model, and I know that the same rules. While your name is not different, there are some names to be true. In\n",
      "sample 1: Hello, I'm a language model, there are a number of the rules for the game is ‘A’, but you are probably more likely to\n",
      "sample 2: Hello, I'm a language model, I'm writing about \"som-melt of the first day of the week.\"\n",
      "The message is to come\n",
      "sample 3: Hello, I'm a language model, you won't need to understand what the world goes.\n",
      "Pentivate allows you to follow that this video shows\n",
      "Step  600 | loss: 4.355726 | lr: 2.8363e-04 | norm: 0.5663 | dt: 4001.13ms | tok/sec: 131034.94\n",
      "Step  601 | loss: 4.346416 | lr: 2.8270e-04 | norm: 0.4659 | dt: 4005.61ms | tok/sec: 130888.41\n",
      "Step  602 | loss: 4.359955 | lr: 2.8178e-04 | norm: 0.4554 | dt: 4006.13ms | tok/sec: 130871.47\n",
      "Step  603 | loss: 4.462634 | lr: 2.8085e-04 | norm: 0.5114 | dt: 4007.31ms | tok/sec: 130832.98\n",
      "Step  604 | loss: 4.522930 | lr: 2.7993e-04 | norm: 0.5222 | dt: 4004.40ms | tok/sec: 130927.88\n",
      "Step  605 | loss: 4.548987 | lr: 2.7900e-04 | norm: 0.5289 | dt: 4003.71ms | tok/sec: 130950.44\n",
      "Step  606 | loss: 4.512834 | lr: 2.7808e-04 | norm: 0.5304 | dt: 4005.36ms | tok/sec: 130896.46\n",
      "Step  607 | loss: 4.511278 | lr: 2.7715e-04 | norm: 0.5482 | dt: 4004.86ms | tok/sec: 130912.88\n",
      "Step  608 | loss: 4.522633 | lr: 2.7623e-04 | norm: 0.5492 | dt: 4005.24ms | tok/sec: 130900.37\n",
      "Step  609 | loss: 4.503286 | lr: 2.7531e-04 | norm: 0.6543 | dt: 4006.39ms | tok/sec: 130862.91\n",
      "Step  610 | loss: 4.505696 | lr: 2.7439e-04 | norm: 0.8498 | dt: 4003.89ms | tok/sec: 130944.58\n",
      "Step  611 | loss: 4.543183 | lr: 2.7346e-04 | norm: 0.8056 | dt: 4006.92ms | tok/sec: 130845.59\n",
      "Step  612 | loss: 4.510739 | lr: 2.7254e-04 | norm: 0.6766 | dt: 4003.60ms | tok/sec: 130954.01\n",
      "Step  613 | loss: 4.470065 | lr: 2.7163e-04 | norm: 0.5941 | dt: 4003.14ms | tok/sec: 130969.25\n",
      "Step  614 | loss: 4.524689 | lr: 2.7071e-04 | norm: 0.5357 | dt: 4006.21ms | tok/sec: 130868.80\n",
      "Step  615 | loss: 4.516803 | lr: 2.6979e-04 | norm: 0.4784 | dt: 4006.04ms | tok/sec: 130874.29\n",
      "Step  616 | loss: 4.482038 | lr: 2.6887e-04 | norm: 0.4865 | dt: 4002.50ms | tok/sec: 130990.26\n",
      "Step  617 | loss: 4.500658 | lr: 2.6795e-04 | norm: 0.5164 | dt: 4002.37ms | tok/sec: 130994.55\n",
      "Step  618 | loss: 4.462693 | lr: 2.6704e-04 | norm: 0.5054 | dt: 4004.45ms | tok/sec: 130926.29\n",
      "Step  619 | loss: 4.444404 | lr: 2.6612e-04 | norm: 0.4472 | dt: 4007.29ms | tok/sec: 130833.58\n",
      "Step  620 | loss: 4.508394 | lr: 2.6521e-04 | norm: 0.4417 | dt: 4004.37ms | tok/sec: 130929.02\n",
      "Step  621 | loss: 4.455383 | lr: 2.6430e-04 | norm: 0.5043 | dt: 4003.27ms | tok/sec: 130964.96\n",
      "Step  622 | loss: 4.502556 | lr: 2.6338e-04 | norm: 0.4690 | dt: 4006.74ms | tok/sec: 130851.67\n",
      "Step  623 | loss: 4.456079 | lr: 2.6247e-04 | norm: 0.4673 | dt: 4003.70ms | tok/sec: 130950.85\n",
      "Step  624 | loss: 4.451228 | lr: 2.6156e-04 | norm: 0.5222 | dt: 4004.38ms | tok/sec: 130928.57\n",
      "Step  625 | loss: 4.445682 | lr: 2.6065e-04 | norm: 0.5772 | dt: 4003.87ms | tok/sec: 130945.18\n",
      "Step  626 | loss: 4.477189 | lr: 2.5974e-04 | norm: 0.5477 | dt: 4005.00ms | tok/sec: 130908.49\n",
      "Step  627 | loss: 4.396982 | lr: 2.5883e-04 | norm: 0.4938 | dt: 4004.66ms | tok/sec: 130919.59\n",
      "Step  628 | loss: 4.401431 | lr: 2.5792e-04 | norm: 0.5339 | dt: 4001.83ms | tok/sec: 131012.16\n",
      "Step  629 | loss: 4.403788 | lr: 2.5702e-04 | norm: 0.6522 | dt: 4002.84ms | tok/sec: 130979.00\n",
      "Step  630 | loss: 4.402361 | lr: 2.5611e-04 | norm: 0.6391 | dt: 4005.22ms | tok/sec: 130901.22\n",
      "Step  631 | loss: 4.419162 | lr: 2.5521e-04 | norm: 0.5441 | dt: 4004.85ms | tok/sec: 130913.30\n",
      "Step  632 | loss: 4.388132 | lr: 2.5430e-04 | norm: 0.5685 | dt: 4005.67ms | tok/sec: 130886.33\n",
      "Step  633 | loss: 4.401048 | lr: 2.5340e-04 | norm: 0.5390 | dt: 4005.59ms | tok/sec: 130888.97\n",
      "Step  634 | loss: 4.431604 | lr: 2.5250e-04 | norm: 0.5047 | dt: 4006.95ms | tok/sec: 130844.79\n",
      "Step  635 | loss: 4.437416 | lr: 2.5159e-04 | norm: 0.5397 | dt: 4000.70ms | tok/sec: 131049.17\n",
      "Step  636 | loss: 4.491297 | lr: 2.5069e-04 | norm: 0.5133 | dt: 4005.60ms | tok/sec: 130888.85\n",
      "Step  637 | loss: 4.612713 | lr: 2.4980e-04 | norm: 0.6328 | dt: 4001.95ms | tok/sec: 131008.10\n",
      "Step  638 | loss: 4.364259 | lr: 2.4890e-04 | norm: 0.7603 | dt: 4004.65ms | tok/sec: 130919.88\n",
      "Step  639 | loss: 4.355271 | lr: 2.4800e-04 | norm: 0.7413 | dt: 4005.35ms | tok/sec: 130897.04\n",
      "Step  640 | loss: 4.244616 | lr: 2.4710e-04 | norm: 0.6419 | dt: 4002.35ms | tok/sec: 130995.00\n",
      "Step  641 | loss: 4.340024 | lr: 2.4621e-04 | norm: 0.6770 | dt: 4003.60ms | tok/sec: 130954.22\n",
      "Step  642 | loss: 4.321054 | lr: 2.4531e-04 | norm: 0.6232 | dt: 4003.12ms | tok/sec: 130969.84\n",
      "Step  643 | loss: 4.319619 | lr: 2.4442e-04 | norm: 0.5936 | dt: 4002.96ms | tok/sec: 130974.97\n",
      "Step  644 | loss: 4.318652 | lr: 2.4353e-04 | norm: 0.4960 | dt: 4007.22ms | tok/sec: 130835.91\n",
      "Step  645 | loss: 4.270242 | lr: 2.4264e-04 | norm: 0.5124 | dt: 4004.25ms | tok/sec: 130932.98\n",
      "Step  646 | loss: 4.296878 | lr: 2.4175e-04 | norm: 0.4781 | dt: 4001.75ms | tok/sec: 131014.75\n",
      "Step  647 | loss: 4.321466 | lr: 2.4086e-04 | norm: 0.4592 | dt: 4007.05ms | tok/sec: 130841.53\n",
      "Step  648 | loss: 4.304129 | lr: 2.3997e-04 | norm: 0.4820 | dt: 4007.74ms | tok/sec: 130818.94\n",
      "Step  649 | loss: 4.281547 | lr: 2.3908e-04 | norm: 0.4745 | dt: 4005.38ms | tok/sec: 130896.03\n",
      "Validation loss: 4.4031\n",
      "sample 0: Hello, I'm a language model, and I think I have learned how to measure its speed on a distance map to determine whether it has the power of data\n",
      "sample 1: Hello, I'm a language model, I'd like to use the IIS, I get up with some suggestions and suggestions to create my friends with you.\n",
      "sample 2: Hello, I'm a language model, but I're always speaking a lot like this I'm a different from this.\n",
      "A lot of people are not as\n",
      "sample 3: Hello, I'm a language model, so much like this, I am talking with those in my field. But it still seems the way, and the right\n",
      "Step  650 | loss: 4.409913 | lr: 2.3820e-04 | norm: 0.4776 | dt: 3995.23ms | tok/sec: 131228.62\n",
      "Step  651 | loss: 4.449786 | lr: 2.3731e-04 | norm: 0.4655 | dt: 4000.50ms | tok/sec: 131055.56\n",
      "Step  652 | loss: 4.526710 | lr: 2.3643e-04 | norm: 0.4716 | dt: 4005.37ms | tok/sec: 130896.13\n",
      "Step  653 | loss: 4.454999 | lr: 2.3554e-04 | norm: 0.4920 | dt: 4004.70ms | tok/sec: 130918.16\n",
      "Step  654 | loss: 4.400843 | lr: 2.3466e-04 | norm: 0.5822 | dt: 4003.51ms | tok/sec: 130957.02\n",
      "Step  655 | loss: 4.466746 | lr: 2.3378e-04 | norm: 0.5945 | dt: 4003.40ms | tok/sec: 130960.59\n",
      "Step  656 | loss: 4.446991 | lr: 2.3290e-04 | norm: 0.5120 | dt: 4004.16ms | tok/sec: 130935.99\n",
      "Step  657 | loss: 4.408250 | lr: 2.3203e-04 | norm: 0.6228 | dt: 4003.66ms | tok/sec: 130952.32\n",
      "Step  658 | loss: 4.431831 | lr: 2.3115e-04 | norm: 0.7364 | dt: 4002.49ms | tok/sec: 130990.43\n",
      "Step  659 | loss: 4.463884 | lr: 2.3027e-04 | norm: 0.6438 | dt: 4003.76ms | tok/sec: 130948.97\n",
      "Step  660 | loss: 4.406022 | lr: 2.2940e-04 | norm: 0.5271 | dt: 4001.69ms | tok/sec: 131016.70\n",
      "Step  661 | loss: 4.444353 | lr: 2.2853e-04 | norm: 0.5406 | dt: 4003.63ms | tok/sec: 130953.16\n",
      "Step  662 | loss: 4.362709 | lr: 2.2765e-04 | norm: 0.4958 | dt: 4000.26ms | tok/sec: 131063.32\n",
      "Step  663 | loss: 4.413713 | lr: 2.2678e-04 | norm: 0.4439 | dt: 4003.26ms | tok/sec: 130965.36\n",
      "Step  664 | loss: 4.363274 | lr: 2.2591e-04 | norm: 0.4791 | dt: 4005.58ms | tok/sec: 130889.42\n",
      "Step  665 | loss: 4.351207 | lr: 2.2505e-04 | norm: 0.4631 | dt: 4001.54ms | tok/sec: 131021.70\n",
      "Step  666 | loss: 4.392764 | lr: 2.2418e-04 | norm: 0.5096 | dt: 4005.31ms | tok/sec: 130898.39\n",
      "Step  667 | loss: 4.388489 | lr: 2.2331e-04 | norm: 0.5222 | dt: 4005.16ms | tok/sec: 130903.21\n",
      "Step  668 | loss: 4.349365 | lr: 2.2245e-04 | norm: 0.5017 | dt: 4004.57ms | tok/sec: 130922.34\n",
      "Step  669 | loss: 4.381351 | lr: 2.2159e-04 | norm: 0.4634 | dt: 4006.26ms | tok/sec: 130867.35\n",
      "Step  670 | loss: 4.323267 | lr: 2.2073e-04 | norm: 0.4442 | dt: 4002.50ms | tok/sec: 130990.14\n",
      "Step  671 | loss: 4.381099 | lr: 2.1987e-04 | norm: 0.4194 | dt: 4005.76ms | tok/sec: 130883.41\n",
      "Step  672 | loss: 4.353377 | lr: 2.1901e-04 | norm: 0.4991 | dt: 4004.56ms | tok/sec: 130922.71\n",
      "Step  673 | loss: 4.370096 | lr: 2.1815e-04 | norm: 0.5365 | dt: 4004.43ms | tok/sec: 130926.91\n",
      "Step  674 | loss: 4.306602 | lr: 2.1729e-04 | norm: 0.4451 | dt: 4004.61ms | tok/sec: 130921.17\n",
      "Step  675 | loss: 4.342900 | lr: 2.1644e-04 | norm: 0.4540 | dt: 4002.67ms | tok/sec: 130984.72\n",
      "Step  676 | loss: 4.331190 | lr: 2.1559e-04 | norm: 0.4586 | dt: 4001.11ms | tok/sec: 131035.56\n",
      "Step  677 | loss: 4.363686 | lr: 2.1473e-04 | norm: 0.4946 | dt: 4002.01ms | tok/sec: 131006.26\n",
      "Step  678 | loss: 4.332460 | lr: 2.1388e-04 | norm: 0.5597 | dt: 4004.26ms | tok/sec: 130932.52\n",
      "Step  679 | loss: 4.326905 | lr: 2.1303e-04 | norm: 0.5719 | dt: 4007.15ms | tok/sec: 130838.18\n",
      "Step  680 | loss: 4.325902 | lr: 2.1219e-04 | norm: 0.4680 | dt: 4003.76ms | tok/sec: 130948.94\n",
      "Step  681 | loss: 4.320783 | lr: 2.1134e-04 | norm: 0.5115 | dt: 4004.96ms | tok/sec: 130909.67\n",
      "Step  682 | loss: 4.321164 | lr: 2.1049e-04 | norm: 0.5145 | dt: 4004.26ms | tok/sec: 130932.71\n",
      "Step  683 | loss: 4.340446 | lr: 2.0965e-04 | norm: 0.4546 | dt: 4004.24ms | tok/sec: 130933.26\n",
      "Step  684 | loss: 4.274292 | lr: 2.0881e-04 | norm: 0.5417 | dt: 4007.00ms | tok/sec: 130843.04\n",
      "Step  685 | loss: 4.232980 | lr: 2.0797e-04 | norm: 0.5925 | dt: 4003.25ms | tok/sec: 130965.45\n",
      "Step  686 | loss: 4.198084 | lr: 2.0713e-04 | norm: 0.5863 | dt: 4004.98ms | tok/sec: 130909.06\n",
      "Step  687 | loss: 4.235763 | lr: 2.0629e-04 | norm: 0.5930 | dt: 4000.11ms | tok/sec: 131068.27\n",
      "Step  688 | loss: 4.228634 | lr: 2.0546e-04 | norm: 0.5379 | dt: 4004.95ms | tok/sec: 130910.15\n",
      "Step  689 | loss: 4.233337 | lr: 2.0462e-04 | norm: 0.5209 | dt: 4005.77ms | tok/sec: 130883.22\n",
      "Step  690 | loss: 4.245080 | lr: 2.0379e-04 | norm: 0.4592 | dt: 4000.05ms | tok/sec: 131070.47\n",
      "Step  691 | loss: 4.234711 | lr: 2.0296e-04 | norm: 0.5151 | dt: 4003.39ms | tok/sec: 130961.09\n",
      "Step  692 | loss: 4.268368 | lr: 2.0213e-04 | norm: 0.5245 | dt: 4004.47ms | tok/sec: 130925.60\n",
      "Step  693 | loss: 4.216975 | lr: 2.0130e-04 | norm: 0.5788 | dt: 4003.32ms | tok/sec: 130963.46\n",
      "Step  694 | loss: 4.228128 | lr: 2.0047e-04 | norm: 0.6530 | dt: 4003.08ms | tok/sec: 130971.05\n",
      "Step  695 | loss: 4.187160 | lr: 1.9965e-04 | norm: 0.5907 | dt: 4005.66ms | tok/sec: 130886.82\n",
      "Step  696 | loss: 4.323699 | lr: 1.9882e-04 | norm: 0.5677 | dt: 4002.80ms | tok/sec: 130980.15\n",
      "Step  697 | loss: 4.515284 | lr: 1.9800e-04 | norm: 0.5989 | dt: 4002.14ms | tok/sec: 131001.97\n",
      "Step  698 | loss: 4.403938 | lr: 1.9718e-04 | norm: 0.6414 | dt: 4002.31ms | tok/sec: 130996.46\n",
      "Step  699 | loss: 4.396289 | lr: 1.9636e-04 | norm: 0.7921 | dt: 4004.14ms | tok/sec: 130936.36\n",
      "Validation loss: 4.3339\n",
      "sample 0: Hello, I'm a language model, and I think I can learn how to think when I hear what he uses the word and how you think you are using\n",
      "sample 1: Hello, I'm a language model, I mean, I am a model concept, but some others have an opinion, I'm a language or a language.\n",
      "sample 2: Hello, I'm a language model, I'm talking with\n",
      "(c) I will want to have an object to say, but I'm a language group\n",
      "sample 3: Hello, I'm a language model, which seems to be a \"a great\" because it is a kind of a programming language. I guess that is quite\n",
      "Step  700 | loss: 4.374767 | lr: 1.9554e-04 | norm: 0.9094 | dt: 3999.25ms | tok/sec: 131096.48\n",
      "Step  701 | loss: 4.361419 | lr: 1.9473e-04 | norm: 0.7473 | dt: 4001.53ms | tok/sec: 131022.03\n",
      "Step  702 | loss: 4.332794 | lr: 1.9391e-04 | norm: 0.5302 | dt: 4004.29ms | tok/sec: 130931.46\n",
      "Step  703 | loss: 4.366904 | lr: 1.9310e-04 | norm: 0.5838 | dt: 4003.18ms | tok/sec: 130967.99\n",
      "Step  704 | loss: 4.379975 | lr: 1.9229e-04 | norm: 0.5538 | dt: 4005.57ms | tok/sec: 130889.81\n",
      "Step  705 | loss: 4.316925 | lr: 1.9148e-04 | norm: 0.5724 | dt: 4005.25ms | tok/sec: 130900.06\n",
      "Step  706 | loss: 4.358210 | lr: 1.9068e-04 | norm: 0.5038 | dt: 4001.76ms | tok/sec: 131014.42\n",
      "Step  707 | loss: 4.393107 | lr: 1.8987e-04 | norm: 0.5027 | dt: 4005.34ms | tok/sec: 130897.37\n",
      "Step  708 | loss: 4.373975 | lr: 1.8907e-04 | norm: 0.5040 | dt: 4004.60ms | tok/sec: 130921.58\n",
      "Step  709 | loss: 4.405896 | lr: 1.8826e-04 | norm: 0.4631 | dt: 4004.03ms | tok/sec: 130940.13\n",
      "Step  710 | loss: 4.332253 | lr: 1.8746e-04 | norm: 0.4831 | dt: 4004.64ms | tok/sec: 130920.06\n",
      "Step  711 | loss: 4.342372 | lr: 1.8666e-04 | norm: 0.4274 | dt: 4004.29ms | tok/sec: 130931.65\n",
      "Step  712 | loss: 4.298204 | lr: 1.8587e-04 | norm: 0.4613 | dt: 4005.90ms | tok/sec: 130878.89\n",
      "Step  713 | loss: 4.322262 | lr: 1.8507e-04 | norm: 0.4401 | dt: 4007.85ms | tok/sec: 130815.43\n",
      "Step  714 | loss: 4.283657 | lr: 1.8428e-04 | norm: 0.5886 | dt: 4003.04ms | tok/sec: 130972.32\n",
      "Step  715 | loss: 4.381219 | lr: 1.8349e-04 | norm: 0.5929 | dt: 4002.92ms | tok/sec: 130976.44\n",
      "Step  716 | loss: 4.330591 | lr: 1.8270e-04 | norm: 0.5020 | dt: 4006.51ms | tok/sec: 130858.93\n",
      "Step  717 | loss: 4.319135 | lr: 1.8191e-04 | norm: 0.4910 | dt: 4002.88ms | tok/sec: 130977.71\n",
      "Step  718 | loss: 4.383632 | lr: 1.8112e-04 | norm: 0.5625 | dt: 4006.54ms | tok/sec: 130858.01\n",
      "Step  719 | loss: 4.341776 | lr: 1.8034e-04 | norm: 0.4922 | dt: 4005.70ms | tok/sec: 130885.49\n",
      "Step  720 | loss: 4.304195 | lr: 1.7956e-04 | norm: 0.5130 | dt: 4006.23ms | tok/sec: 130868.13\n",
      "Step  721 | loss: 4.234128 | lr: 1.7878e-04 | norm: 0.4296 | dt: 4006.98ms | tok/sec: 130843.59\n",
      "Step  722 | loss: 4.286303 | lr: 1.7800e-04 | norm: 0.4950 | dt: 4006.14ms | tok/sec: 130871.25\n",
      "Step  723 | loss: 4.268085 | lr: 1.7722e-04 | norm: 0.4254 | dt: 4007.44ms | tok/sec: 130828.73\n",
      "Step  724 | loss: 4.285807 | lr: 1.7644e-04 | norm: 0.4658 | dt: 4004.63ms | tok/sec: 130920.36\n",
      "Step  725 | loss: 4.305429 | lr: 1.7567e-04 | norm: 0.4067 | dt: 4005.82ms | tok/sec: 130881.43\n",
      "Step  726 | loss: 4.326807 | lr: 1.7490e-04 | norm: 0.4572 | dt: 4004.72ms | tok/sec: 130917.57\n",
      "Step  727 | loss: 4.252160 | lr: 1.7413e-04 | norm: 0.5283 | dt: 4006.67ms | tok/sec: 130853.91\n",
      "Step  728 | loss: 4.228971 | lr: 1.7336e-04 | norm: 0.5162 | dt: 4008.98ms | tok/sec: 130778.51\n",
      "Step  729 | loss: 4.275254 | lr: 1.7260e-04 | norm: 0.5195 | dt: 4005.59ms | tok/sec: 130889.08\n",
      "Step  730 | loss: 4.250047 | lr: 1.7183e-04 | norm: 0.4922 | dt: 4005.95ms | tok/sec: 130877.41\n",
      "Step  731 | loss: 4.283250 | lr: 1.7107e-04 | norm: 0.5040 | dt: 4005.51ms | tok/sec: 130891.65\n",
      "Step  732 | loss: 4.162998 | lr: 1.7031e-04 | norm: 0.5028 | dt: 4008.96ms | tok/sec: 130778.90\n",
      "Step  733 | loss: 4.174128 | lr: 1.6955e-04 | norm: 0.4869 | dt: 4008.87ms | tok/sec: 130781.94\n",
      "Step  734 | loss: 4.155040 | lr: 1.6880e-04 | norm: 0.4864 | dt: 4009.02ms | tok/sec: 130776.99\n",
      "Step  735 | loss: 4.156638 | lr: 1.6804e-04 | norm: 0.5730 | dt: 4006.65ms | tok/sec: 130854.58\n",
      "Step  736 | loss: 4.171017 | lr: 1.6729e-04 | norm: 0.6073 | dt: 4005.63ms | tok/sec: 130887.91\n",
      "Step  737 | loss: 4.170071 | lr: 1.6654e-04 | norm: 0.5066 | dt: 4011.48ms | tok/sec: 130696.87\n",
      "Step  738 | loss: 4.182383 | lr: 1.6579e-04 | norm: 0.6284 | dt: 4010.60ms | tok/sec: 130725.43\n",
      "Step  739 | loss: 4.196452 | lr: 1.6504e-04 | norm: 0.6214 | dt: 4008.71ms | tok/sec: 130787.06\n",
      "Step  740 | loss: 4.149762 | lr: 1.6430e-04 | norm: 0.6312 | dt: 4010.32ms | tok/sec: 130734.62\n",
      "Step  741 | loss: 4.149331 | lr: 1.6356e-04 | norm: 0.5545 | dt: 4007.45ms | tok/sec: 130828.24\n",
      "Step  742 | loss: 4.118159 | lr: 1.6282e-04 | norm: 0.5183 | dt: 4007.88ms | tok/sec: 130814.41\n",
      "Step  743 | loss: 4.180276 | lr: 1.6208e-04 | norm: 0.4875 | dt: 4011.72ms | tok/sec: 130689.08\n",
      "Step  744 | loss: 4.314581 | lr: 1.6134e-04 | norm: 0.5146 | dt: 4012.73ms | tok/sec: 130656.07\n",
      "Step  745 | loss: 4.292652 | lr: 1.6061e-04 | norm: 0.4365 | dt: 4009.35ms | tok/sec: 130766.19\n",
      "Step  746 | loss: 4.323534 | lr: 1.5988e-04 | norm: 0.5035 | dt: 4008.99ms | tok/sec: 130778.07\n",
      "Step  747 | loss: 4.339011 | lr: 1.5915e-04 | norm: 0.4865 | dt: 4009.63ms | tok/sec: 130757.24\n",
      "Step  748 | loss: 4.319566 | lr: 1.5842e-04 | norm: 0.5234 | dt: 4014.01ms | tok/sec: 130614.60\n",
      "Step  749 | loss: 4.280667 | lr: 1.5769e-04 | norm: 0.4159 | dt: 4010.59ms | tok/sec: 130725.96\n",
      "Validation loss: 4.2611\n",
      "sample 0: Hello, I'm a language model, and I're a part of this, if you're a part of some kind of languages, or I don't understand\n",
      "sample 1: Hello, I'm a language model, not.\n",
      "If you're looking at the \"pixels,\" and do your computer program in your room or in the\n",
      "sample 2: Hello, I'm a language model, I'm something that I don't really like and I'm thinking that.\n",
      "\"There's a lot of fun.\n",
      "sample 3: Hello, I'm a language model, you'm a good idea to get me to, and I'm a good thing in which there is just a little extra\n",
      "Step  750 | loss: 4.330771 | lr: 1.5697e-04 | norm: 0.4174 | dt: 4005.51ms | tok/sec: 130891.65\n",
      "Step  751 | loss: 4.398920 | lr: 1.5625e-04 | norm: 0.4279 | dt: 4010.36ms | tok/sec: 130733.34\n",
      "Step  752 | loss: 4.321036 | lr: 1.5553e-04 | norm: 0.4715 | dt: 4014.32ms | tok/sec: 130604.29\n",
      "Step  753 | loss: 4.343607 | lr: 1.5481e-04 | norm: 0.4741 | dt: 4012.40ms | tok/sec: 130666.97\n",
      "Step  754 | loss: 4.297142 | lr: 1.5409e-04 | norm: 0.4776 | dt: 4014.12ms | tok/sec: 130611.08\n",
      "Step  755 | loss: 4.310238 | lr: 1.5338e-04 | norm: 0.5122 | dt: 4012.65ms | tok/sec: 130658.64\n",
      "Step  756 | loss: 4.329910 | lr: 1.5267e-04 | norm: 0.5648 | dt: 4012.31ms | tok/sec: 130669.78\n",
      "Step  757 | loss: 4.273091 | lr: 1.5196e-04 | norm: 0.5774 | dt: 4010.52ms | tok/sec: 130728.31\n",
      "Step  758 | loss: 4.287466 | lr: 1.5126e-04 | norm: 0.4759 | dt: 4013.19ms | tok/sec: 130641.33\n",
      "Step  759 | loss: 4.366703 | lr: 1.5055e-04 | norm: 0.5078 | dt: 4013.38ms | tok/sec: 130634.99\n",
      "Step  760 | loss: 4.271336 | lr: 1.4985e-04 | norm: 0.5841 | dt: 4013.31ms | tok/sec: 130637.46\n",
      "Step  761 | loss: 4.325777 | lr: 1.4915e-04 | norm: 0.5297 | dt: 4013.00ms | tok/sec: 130647.29\n",
      "Step  762 | loss: 4.353631 | lr: 1.4845e-04 | norm: 0.5158 | dt: 4013.66ms | tok/sec: 130625.91\n",
      "Step  763 | loss: 4.291018 | lr: 1.4775e-04 | norm: 0.5492 | dt: 4015.14ms | tok/sec: 130577.85\n",
      "Step  764 | loss: 4.302629 | lr: 1.4706e-04 | norm: 0.4473 | dt: 4014.14ms | tok/sec: 130610.39\n",
      "Step  765 | loss: 4.238657 | lr: 1.4637e-04 | norm: 0.5500 | dt: 4013.71ms | tok/sec: 130624.18\n",
      "Step  766 | loss: 4.316696 | lr: 1.4568e-04 | norm: 0.5042 | dt: 4013.92ms | tok/sec: 130617.29\n",
      "Step  767 | loss: 4.251051 | lr: 1.4499e-04 | norm: 0.5064 | dt: 4011.36ms | tok/sec: 130700.92\n",
      "Step  768 | loss: 4.218135 | lr: 1.4431e-04 | norm: 0.4523 | dt: 4010.84ms | tok/sec: 130717.90\n",
      "Step  769 | loss: 4.233684 | lr: 1.4363e-04 | norm: 0.5037 | dt: 4013.42ms | tok/sec: 130633.66\n",
      "Step  770 | loss: 4.255911 | lr: 1.4295e-04 | norm: 0.4735 | dt: 4015.26ms | tok/sec: 130573.94\n",
      "Step  771 | loss: 4.233724 | lr: 1.4227e-04 | norm: 0.4786 | dt: 4012.75ms | tok/sec: 130655.52\n",
      "Step  772 | loss: 4.271702 | lr: 1.4159e-04 | norm: 0.5193 | dt: 4013.78ms | tok/sec: 130622.07\n",
      "Step  773 | loss: 4.260341 | lr: 1.4092e-04 | norm: 0.4847 | dt: 4011.95ms | tok/sec: 130681.71\n",
      "Step  774 | loss: 4.255872 | lr: 1.4025e-04 | norm: 0.4468 | dt: 4013.29ms | tok/sec: 130638.11\n",
      "Step  775 | loss: 4.215240 | lr: 1.3958e-04 | norm: 0.4475 | dt: 4011.98ms | tok/sec: 130680.70\n",
      "Step  776 | loss: 4.232121 | lr: 1.3891e-04 | norm: 0.4683 | dt: 4011.37ms | tok/sec: 130700.42\n",
      "Step  777 | loss: 4.203770 | lr: 1.3825e-04 | norm: 0.4535 | dt: 4008.03ms | tok/sec: 130809.38\n",
      "Step  778 | loss: 4.156148 | lr: 1.3759e-04 | norm: 0.4867 | dt: 4010.38ms | tok/sec: 130732.85\n",
      "Step  779 | loss: 4.231018 | lr: 1.3693e-04 | norm: 0.5127 | dt: 4011.65ms | tok/sec: 130691.48\n",
      "Step  780 | loss: 4.155119 | lr: 1.3627e-04 | norm: 0.5754 | dt: 4008.28ms | tok/sec: 130801.37\n",
      "Step  781 | loss: 4.140804 | lr: 1.3562e-04 | norm: 0.5650 | dt: 4012.31ms | tok/sec: 130669.76\n",
      "Step  782 | loss: 4.070463 | lr: 1.3497e-04 | norm: 0.5108 | dt: 4013.25ms | tok/sec: 130639.29\n",
      "Step  783 | loss: 4.085333 | lr: 1.3432e-04 | norm: 0.5166 | dt: 4008.26ms | tok/sec: 130801.96\n",
      "Step  784 | loss: 4.152021 | lr: 1.3367e-04 | norm: 0.4377 | dt: 4008.19ms | tok/sec: 130804.13\n",
      "Step  785 | loss: 4.197080 | lr: 1.3302e-04 | norm: 0.5187 | dt: 4007.92ms | tok/sec: 130813.11\n",
      "Step  786 | loss: 4.114773 | lr: 1.3238e-04 | norm: 0.5142 | dt: 4006.49ms | tok/sec: 130859.77\n",
      "Step  787 | loss: 4.168266 | lr: 1.3174e-04 | norm: 0.4943 | dt: 4005.06ms | tok/sec: 130906.28\n",
      "Step  788 | loss: 4.111813 | lr: 1.3110e-04 | norm: 0.4545 | dt: 4005.98ms | tok/sec: 130876.40\n",
      "Step  789 | loss: 4.169833 | lr: 1.3047e-04 | norm: 0.4454 | dt: 4006.21ms | tok/sec: 130868.89\n",
      "Step  790 | loss: 4.258977 | lr: 1.2983e-04 | norm: 0.4637 | dt: 4006.70ms | tok/sec: 130852.82\n",
      "Step  791 | loss: 4.256166 | lr: 1.2920e-04 | norm: 0.5035 | dt: 4007.81ms | tok/sec: 130816.71\n",
      "Step  792 | loss: 4.279320 | lr: 1.2858e-04 | norm: 0.5633 | dt: 4005.28ms | tok/sec: 130899.31\n",
      "Step  793 | loss: 4.293454 | lr: 1.2795e-04 | norm: 0.7112 | dt: 4008.98ms | tok/sec: 130778.51\n",
      "Step  794 | loss: 4.285053 | lr: 1.2733e-04 | norm: 0.6551 | dt: 4007.77ms | tok/sec: 130817.83\n",
      "Step  795 | loss: 4.285785 | lr: 1.2671e-04 | norm: 0.5110 | dt: 4007.10ms | tok/sec: 130839.65\n",
      "Step  796 | loss: 4.319147 | lr: 1.2609e-04 | norm: 0.5588 | dt: 4004.69ms | tok/sec: 130918.42\n",
      "Step  797 | loss: 4.253658 | lr: 1.2547e-04 | norm: 0.4674 | dt: 4006.17ms | tok/sec: 130870.21\n",
      "Step  798 | loss: 4.289351 | lr: 1.2486e-04 | norm: 0.5214 | dt: 3999.74ms | tok/sec: 131080.61\n",
      "Step  799 | loss: 4.241867 | lr: 1.2425e-04 | norm: 0.4581 | dt: 4006.41ms | tok/sec: 130862.45\n",
      "Validation loss: 4.2204\n",
      "sample 0: Hello, I'm a language model, and I know it is in its \"black space\" category, then. So, in the middle, that's all\n",
      "sample 1: Hello, I'm a language model, I're a big-paced model for the math education class. And of course I'll do some kind of math as\n",
      "sample 2: Hello, I'm a language model, I'm thinking I couldn't take that. There's a bit, you can't get the same thing. I wouldn\n",
      "sample 3: Hello, I'm a language model, which may be called \"invisible\" in another language.\n",
      "As an idea given, for instance, you may feel\n",
      "Step  800 | loss: 4.283405 | lr: 1.2364e-04 | norm: 0.4965 | dt: 3999.16ms | tok/sec: 131099.39\n",
      "Step  801 | loss: 4.239294 | lr: 1.2303e-04 | norm: 0.5112 | dt: 4002.45ms | tok/sec: 130991.77\n",
      "Step  802 | loss: 4.304809 | lr: 1.2243e-04 | norm: 0.4826 | dt: 4003.57ms | tok/sec: 130954.99\n",
      "Step  803 | loss: 4.264522 | lr: 1.2183e-04 | norm: 0.4506 | dt: 4002.14ms | tok/sec: 131001.84\n",
      "Step  804 | loss: 4.288930 | lr: 1.2123e-04 | norm: 0.4757 | dt: 4005.42ms | tok/sec: 130894.64\n",
      "Step  805 | loss: 4.262580 | lr: 1.2064e-04 | norm: 0.4446 | dt: 3999.92ms | tok/sec: 131074.73\n",
      "Step  806 | loss: 4.222434 | lr: 1.2004e-04 | norm: 0.4604 | dt: 4003.78ms | tok/sec: 130948.24\n",
      "Step  807 | loss: 4.264665 | lr: 1.1945e-04 | norm: 0.4581 | dt: 3999.93ms | tok/sec: 131074.34\n",
      "Step  808 | loss: 4.300282 | lr: 1.1886e-04 | norm: 0.4785 | dt: 4002.22ms | tok/sec: 130999.38\n",
      "Step  809 | loss: 4.260291 | lr: 1.1828e-04 | norm: 0.5231 | dt: 4002.27ms | tok/sec: 130997.64\n",
      "Step  810 | loss: 4.243028 | lr: 1.1770e-04 | norm: 0.4946 | dt: 4001.17ms | tok/sec: 131033.82\n",
      "Step  811 | loss: 4.226453 | lr: 1.1711e-04 | norm: 0.4627 | dt: 4001.69ms | tok/sec: 131016.75\n",
      "Step  812 | loss: 4.197747 | lr: 1.1654e-04 | norm: 0.5177 | dt: 3999.15ms | tok/sec: 131099.76\n",
      "Step  813 | loss: 4.231876 | lr: 1.1596e-04 | norm: 0.4972 | dt: 4000.80ms | tok/sec: 131045.68\n",
      "Step  814 | loss: 4.206528 | lr: 1.1539e-04 | norm: 0.4848 | dt: 4001.92ms | tok/sec: 131009.15\n",
      "Step  815 | loss: 4.202830 | lr: 1.1482e-04 | norm: 0.4558 | dt: 4004.72ms | tok/sec: 130917.65\n",
      "Step  816 | loss: 4.157009 | lr: 1.1425e-04 | norm: 0.4443 | dt: 4003.32ms | tok/sec: 130963.39\n",
      "Step  817 | loss: 4.174389 | lr: 1.1369e-04 | norm: 0.4156 | dt: 4000.33ms | tok/sec: 131061.25\n",
      "Step  818 | loss: 4.203125 | lr: 1.1313e-04 | norm: 0.4276 | dt: 4001.65ms | tok/sec: 131018.03\n",
      "Step  819 | loss: 4.221620 | lr: 1.1257e-04 | norm: 0.4513 | dt: 4002.46ms | tok/sec: 130991.42\n",
      "Step  820 | loss: 4.188383 | lr: 1.1201e-04 | norm: 0.4242 | dt: 4003.51ms | tok/sec: 130957.04\n",
      "Step  821 | loss: 4.173254 | lr: 1.1145e-04 | norm: 0.4422 | dt: 4003.41ms | tok/sec: 130960.51\n",
      "Step  822 | loss: 4.264529 | lr: 1.1090e-04 | norm: 0.4868 | dt: 4003.13ms | tok/sec: 130969.49\n",
      "Step  823 | loss: 4.164927 | lr: 1.1035e-04 | norm: 0.5064 | dt: 4000.27ms | tok/sec: 131063.05\n",
      "Step  824 | loss: 4.082227 | lr: 1.0981e-04 | norm: 0.4346 | dt: 4002.18ms | tok/sec: 131000.48\n",
      "Step  825 | loss: 4.077568 | lr: 1.0926e-04 | norm: 0.4310 | dt: 4001.92ms | tok/sec: 131008.99\n",
      "Step  826 | loss: 4.106553 | lr: 1.0872e-04 | norm: 0.4573 | dt: 4000.36ms | tok/sec: 131060.17\n",
      "Step  827 | loss: 4.106480 | lr: 1.0819e-04 | norm: 0.4754 | dt: 4001.76ms | tok/sec: 131014.30\n",
      "Step  828 | loss: 4.132040 | lr: 1.0765e-04 | norm: 0.4970 | dt: 4001.69ms | tok/sec: 131016.52\n",
      "Step  829 | loss: 4.090148 | lr: 1.0712e-04 | norm: 0.4645 | dt: 4000.77ms | tok/sec: 131046.70\n",
      "Step  830 | loss: 4.060302 | lr: 1.0659e-04 | norm: 0.4989 | dt: 4006.70ms | tok/sec: 130852.70\n",
      "Step  831 | loss: 4.070883 | lr: 1.0606e-04 | norm: 0.4864 | dt: 4000.06ms | tok/sec: 131069.93\n",
      "Step  832 | loss: 4.010972 | lr: 1.0554e-04 | norm: 0.4367 | dt: 4004.19ms | tok/sec: 130934.92\n",
      "Step  833 | loss: 4.062809 | lr: 1.0501e-04 | norm: 0.4773 | dt: 4002.18ms | tok/sec: 131000.59\n",
      "Step  834 | loss: 4.032822 | lr: 1.0449e-04 | norm: 0.4511 | dt: 4007.88ms | tok/sec: 130814.19\n",
      "Step  835 | loss: 4.238898 | lr: 1.0398e-04 | norm: 0.4395 | dt: 4004.24ms | tok/sec: 130933.19\n",
      "Step  836 | loss: 4.206920 | lr: 1.0346e-04 | norm: 0.5504 | dt: 4002.85ms | tok/sec: 130978.54\n",
      "Step  837 | loss: 4.214228 | lr: 1.0295e-04 | norm: 0.4934 | dt: 4004.59ms | tok/sec: 130921.87\n",
      "Step  838 | loss: 4.276463 | lr: 1.0245e-04 | norm: 0.4838 | dt: 4001.71ms | tok/sec: 131015.91\n",
      "Step  839 | loss: 4.242505 | lr: 1.0194e-04 | norm: 0.4621 | dt: 4006.04ms | tok/sec: 130874.42\n",
      "Step  840 | loss: 4.222758 | lr: 1.0144e-04 | norm: 0.4342 | dt: 4000.10ms | tok/sec: 131068.63\n",
      "Step  841 | loss: 4.288780 | lr: 1.0094e-04 | norm: 0.4415 | dt: 4005.99ms | tok/sec: 130875.97\n",
      "Step  842 | loss: 4.241374 | lr: 1.0044e-04 | norm: 0.4316 | dt: 4005.33ms | tok/sec: 130897.66\n",
      "Step  843 | loss: 4.241707 | lr: 9.9947e-05 | norm: 0.4060 | dt: 4004.07ms | tok/sec: 130938.86\n",
      "Step  844 | loss: 4.272619 | lr: 9.9456e-05 | norm: 0.6006 | dt: 4004.82ms | tok/sec: 130914.39\n",
      "Step  845 | loss: 4.251422 | lr: 9.8967e-05 | norm: 0.5098 | dt: 4003.65ms | tok/sec: 130952.58\n",
      "Step  846 | loss: 4.240449 | lr: 9.8481e-05 | norm: 0.4570 | dt: 4008.77ms | tok/sec: 130785.36\n",
      "Step  847 | loss: 4.250525 | lr: 9.7998e-05 | norm: 0.4617 | dt: 4006.47ms | tok/sec: 130860.30\n",
      "Step  848 | loss: 4.276931 | lr: 9.7518e-05 | norm: 0.4578 | dt: 4005.15ms | tok/sec: 130903.48\n",
      "Step  849 | loss: 4.250324 | lr: 9.7041e-05 | norm: 0.4929 | dt: 4005.94ms | tok/sec: 130877.65\n",
      "Validation loss: 4.1867\n",
      "sample 0: Hello, I'm a language model, and I know that the answer should be given up to you,\n",
      "For more information, please call me on Twitter<|endoftext|>\n",
      "sample 1: Hello, I'm a language model, I need to know the following, one of them looks that the first to the first of each. Now that's the\n",
      "sample 2: Hello, I'm a language model, I'm familiar with other dialects:\n",
      "What's the \"S\" and \"S\" and \"S\") are\n",
      "sample 3: Hello, I'm a language model, which allows you to use the word ‘man’ or ‘ergings’ instead of a different\n",
      "Step  850 | loss: 4.234904 | lr: 9.6566e-05 | norm: 0.4676 | dt: 4001.85ms | tok/sec: 131011.40\n",
      "Step  851 | loss: 4.195518 | lr: 9.6095e-05 | norm: 0.4526 | dt: 4003.69ms | tok/sec: 130951.21\n",
      "Step  852 | loss: 4.281079 | lr: 9.5626e-05 | norm: 0.4888 | dt: 4003.84ms | tok/sec: 130946.13\n",
      "Step  853 | loss: 4.252453 | lr: 9.5160e-05 | norm: 0.5387 | dt: 4006.29ms | tok/sec: 130866.36\n",
      "Step  854 | loss: 4.203983 | lr: 9.4697e-05 | norm: 0.4387 | dt: 4006.37ms | tok/sec: 130863.76\n",
      "Step  855 | loss: 4.239508 | lr: 9.4237e-05 | norm: 0.5213 | dt: 4004.12ms | tok/sec: 130937.15\n",
      "Step  856 | loss: 4.236920 | lr: 9.3779e-05 | norm: 0.5585 | dt: 4006.93ms | tok/sec: 130845.31\n",
      "Step  857 | loss: 4.245739 | lr: 9.3325e-05 | norm: 0.5697 | dt: 4004.28ms | tok/sec: 130931.78\n",
      "Step  858 | loss: 4.173885 | lr: 9.2873e-05 | norm: 0.7687 | dt: 4000.44ms | tok/sec: 131057.53\n",
      "Step  859 | loss: 4.162117 | lr: 9.2424e-05 | norm: 0.6568 | dt: 4004.37ms | tok/sec: 130928.92\n",
      "Step  860 | loss: 4.174033 | lr: 9.1978e-05 | norm: 0.5539 | dt: 4005.72ms | tok/sec: 130884.84\n",
      "Step  861 | loss: 4.181953 | lr: 9.1535e-05 | norm: 0.6183 | dt: 4005.00ms | tok/sec: 130908.45\n",
      "Step  862 | loss: 4.180506 | lr: 9.1095e-05 | norm: 0.4910 | dt: 4003.26ms | tok/sec: 130965.32\n",
      "Step  863 | loss: 4.184125 | lr: 9.0658e-05 | norm: 0.5757 | dt: 4007.30ms | tok/sec: 130833.14\n",
      "Step  864 | loss: 4.158010 | lr: 9.0224e-05 | norm: 0.4922 | dt: 4005.09ms | tok/sec: 130905.27\n",
      "Step  865 | loss: 4.134622 | lr: 8.9793e-05 | norm: 0.5008 | dt: 4004.01ms | tok/sec: 130940.63\n",
      "Step  866 | loss: 4.156040 | lr: 8.9364e-05 | norm: 0.4741 | dt: 4003.83ms | tok/sec: 130946.65\n",
      "Step  867 | loss: 4.169295 | lr: 8.8939e-05 | norm: 0.4667 | dt: 4003.32ms | tok/sec: 130963.24\n",
      "Step  868 | loss: 4.115483 | lr: 8.8516e-05 | norm: 0.4140 | dt: 4006.23ms | tok/sec: 130868.26\n",
      "Step  869 | loss: 4.068263 | lr: 8.8096e-05 | norm: 0.4405 | dt: 4004.70ms | tok/sec: 130918.18\n",
      "Step  870 | loss: 4.073115 | lr: 8.7680e-05 | norm: 0.4564 | dt: 4006.23ms | tok/sec: 130868.18\n",
      "Step  871 | loss: 4.052449 | lr: 8.7266e-05 | norm: 0.4171 | dt: 4009.10ms | tok/sec: 130774.57\n",
      "Step  872 | loss: 4.048138 | lr: 8.6855e-05 | norm: 0.4075 | dt: 4008.89ms | tok/sec: 130781.28\n",
      "Step  873 | loss: 4.074538 | lr: 8.6447e-05 | norm: 0.3715 | dt: 4006.75ms | tok/sec: 130851.10\n",
      "Step  874 | loss: 4.048402 | lr: 8.6043e-05 | norm: 0.4153 | dt: 4006.87ms | tok/sec: 130847.35\n",
      "Step  875 | loss: 3.995772 | lr: 8.5641e-05 | norm: 0.3960 | dt: 4003.83ms | tok/sec: 130946.46\n",
      "Step  876 | loss: 3.987327 | lr: 8.5242e-05 | norm: 0.3994 | dt: 4004.77ms | tok/sec: 130915.90\n",
      "Step  877 | loss: 4.101248 | lr: 8.4846e-05 | norm: 0.3936 | dt: 4007.02ms | tok/sec: 130842.27\n",
      "Step  878 | loss: 4.092135 | lr: 8.4453e-05 | norm: 0.3978 | dt: 4005.54ms | tok/sec: 130890.67\n",
      "Step  879 | loss: 4.033456 | lr: 8.4063e-05 | norm: 0.4307 | dt: 4010.23ms | tok/sec: 130737.49\n",
      "Step  880 | loss: 4.103953 | lr: 8.3676e-05 | norm: 0.4592 | dt: 4006.32ms | tok/sec: 130865.37\n",
      "Step  881 | loss: 4.150661 | lr: 8.3292e-05 | norm: 0.4503 | dt: 4006.73ms | tok/sec: 130851.92\n",
      "Step  882 | loss: 4.305585 | lr: 8.2911e-05 | norm: 0.4608 | dt: 4005.57ms | tok/sec: 130889.69\n",
      "Step  883 | loss: 4.273210 | lr: 8.2533e-05 | norm: 0.4921 | dt: 4005.38ms | tok/sec: 130895.87\n",
      "Step  884 | loss: 4.202562 | lr: 8.2158e-05 | norm: 0.5246 | dt: 4006.99ms | tok/sec: 130843.32\n",
      "Step  885 | loss: 4.236065 | lr: 8.1786e-05 | norm: 0.4530 | dt: 4006.06ms | tok/sec: 130873.67\n",
      "Step  886 | loss: 4.180224 | lr: 8.1417e-05 | norm: 0.5096 | dt: 4008.49ms | tok/sec: 130794.53\n",
      "Step  887 | loss: 4.248814 | lr: 8.1051e-05 | norm: 0.4881 | dt: 4007.06ms | tok/sec: 130841.18\n",
      "Step  888 | loss: 4.211630 | lr: 8.0688e-05 | norm: 0.4409 | dt: 4008.44ms | tok/sec: 130796.16\n",
      "Step  889 | loss: 4.237644 | lr: 8.0328e-05 | norm: 0.4699 | dt: 4007.00ms | tok/sec: 130843.00\n",
      "Step  890 | loss: 4.209201 | lr: 7.9971e-05 | norm: 0.4599 | dt: 4006.99ms | tok/sec: 130843.23\n",
      "Step  891 | loss: 4.206601 | lr: 7.9617e-05 | norm: 0.4375 | dt: 4004.00ms | tok/sec: 130941.12\n",
      "Step  892 | loss: 4.225466 | lr: 7.9267e-05 | norm: 0.4281 | dt: 4005.98ms | tok/sec: 130876.43\n",
      "Step  893 | loss: 4.165665 | lr: 7.8919e-05 | norm: 0.4857 | dt: 4005.56ms | tok/sec: 130889.95\n",
      "Step  894 | loss: 4.218455 | lr: 7.8574e-05 | norm: 0.4304 | dt: 4005.41ms | tok/sec: 130895.08\n",
      "Step  895 | loss: 4.202243 | lr: 7.8232e-05 | norm: 0.4013 | dt: 4005.50ms | tok/sec: 130891.89\n",
      "Step  896 | loss: 4.178741 | lr: 7.7894e-05 | norm: 0.4528 | dt: 4007.57ms | tok/sec: 130824.30\n",
      "Step  897 | loss: 4.150062 | lr: 7.7558e-05 | norm: 0.4041 | dt: 4004.96ms | tok/sec: 130909.72\n",
      "Step  898 | loss: 4.217020 | lr: 7.7226e-05 | norm: 0.4472 | dt: 4005.39ms | tok/sec: 130895.64\n",
      "Step  899 | loss: 4.157963 | lr: 7.6897e-05 | norm: 0.5298 | dt: 4006.35ms | tok/sec: 130864.41\n",
      "Validation loss: 4.1595\n",
      "sample 0: Hello, I'm a language model, and I'd like a library to show one name that didn't exist within this class, and we'd want to go\n",
      "sample 1: Hello, I'm a language model, but not a language, but the standard of a standard has a lot of different meaning, even though many different languages are\n",
      "sample 2: Hello, I'm a language model, I'm it, to use it with a simple text. If the code is simple, I'm a language, in\n",
      "sample 3: Hello, I'm a language model, you do not know what a language is and so.\n",
      "Do you like a web, your language map, or our\n",
      "Step  900 | loss: 4.208711 | lr: 7.6570e-05 | norm: 0.4255 | dt: 4002.33ms | tok/sec: 130995.78\n",
      "Step  901 | loss: 4.190520 | lr: 7.6247e-05 | norm: 0.4094 | dt: 4004.03ms | tok/sec: 130939.96\n",
      "Step  902 | loss: 4.198321 | lr: 7.5927e-05 | norm: 0.4803 | dt: 4004.38ms | tok/sec: 130928.49\n",
      "Step  903 | loss: 4.162144 | lr: 7.5610e-05 | norm: 0.4097 | dt: 4006.25ms | tok/sec: 130867.51\n",
      "Step  904 | loss: 4.129093 | lr: 7.5296e-05 | norm: 0.4406 | dt: 4006.20ms | tok/sec: 130869.24\n",
      "Step  905 | loss: 4.149774 | lr: 7.4985e-05 | norm: 0.4726 | dt: 4004.73ms | tok/sec: 130917.23\n",
      "Step  906 | loss: 4.159934 | lr: 7.4678e-05 | norm: 0.3771 | dt: 4006.33ms | tok/sec: 130864.94\n",
      "Step  907 | loss: 4.103398 | lr: 7.4373e-05 | norm: 0.4556 | dt: 4004.56ms | tok/sec: 130922.69\n",
      "Step  908 | loss: 4.116902 | lr: 7.4072e-05 | norm: 0.3829 | dt: 4007.35ms | tok/sec: 130831.52\n",
      "Step  909 | loss: 4.155837 | lr: 7.3773e-05 | norm: 0.4066 | dt: 4007.25ms | tok/sec: 130834.90\n",
      "Step  910 | loss: 4.202064 | lr: 7.3478e-05 | norm: 0.4158 | dt: 4004.81ms | tok/sec: 130914.56\n",
      "Step  911 | loss: 4.156274 | lr: 7.3186e-05 | norm: 0.3910 | dt: 4008.27ms | tok/sec: 130801.48\n",
      "Step  912 | loss: 4.169727 | lr: 7.2897e-05 | norm: 0.4359 | dt: 4012.50ms | tok/sec: 130663.58\n",
      "Step  913 | loss: 4.182618 | lr: 7.2611e-05 | norm: 0.4675 | dt: 4004.31ms | tok/sec: 130931.01\n",
      "Step  914 | loss: 4.131773 | lr: 7.2328e-05 | norm: 0.4009 | dt: 4009.10ms | tok/sec: 130774.55\n",
      "Step  915 | loss: 4.008622 | lr: 7.2048e-05 | norm: 0.4348 | dt: 4004.34ms | tok/sec: 130929.87\n",
      "Step  916 | loss: 4.053041 | lr: 7.1772e-05 | norm: 0.4952 | dt: 4004.17ms | tok/sec: 130935.49\n",
      "Step  917 | loss: 4.064157 | lr: 7.1498e-05 | norm: 0.4065 | dt: 4009.61ms | tok/sec: 130757.75\n",
      "Step  918 | loss: 4.050003 | lr: 7.1228e-05 | norm: 0.4428 | dt: 4006.02ms | tok/sec: 130875.03\n",
      "Step  919 | loss: 4.021108 | lr: 7.0961e-05 | norm: 0.3973 | dt: 4005.23ms | tok/sec: 130900.78\n",
      "Step  920 | loss: 4.018891 | lr: 7.0697e-05 | norm: 0.4055 | dt: 4008.14ms | tok/sec: 130805.86\n",
      "Step  921 | loss: 4.008655 | lr: 7.0436e-05 | norm: 0.3789 | dt: 4008.85ms | tok/sec: 130782.67\n",
      "Step  922 | loss: 4.072364 | lr: 7.0179e-05 | norm: 0.4086 | dt: 4006.89ms | tok/sec: 130846.49\n",
      "Step  923 | loss: 4.055697 | lr: 6.9924e-05 | norm: 0.3744 | dt: 4004.30ms | tok/sec: 130931.40\n",
      "Step  924 | loss: 4.043106 | lr: 6.9673e-05 | norm: 0.3784 | dt: 4006.99ms | tok/sec: 130843.45\n",
      "Step  925 | loss: 4.057158 | lr: 6.9425e-05 | norm: 0.3667 | dt: 4006.50ms | tok/sec: 130859.36\n",
      "Step  926 | loss: 4.106386 | lr: 6.9180e-05 | norm: 0.3946 | dt: 4004.81ms | tok/sec: 130914.65\n",
      "Step  927 | loss: 4.225359 | lr: 6.8938e-05 | norm: 0.4249 | dt: 4006.99ms | tok/sec: 130843.47\n",
      "Step  928 | loss: 4.238001 | lr: 6.8699e-05 | norm: 0.4237 | dt: 4007.21ms | tok/sec: 130836.33\n",
      "Step  929 | loss: 4.176535 | lr: 6.8464e-05 | norm: 0.4595 | dt: 4007.25ms | tok/sec: 130834.70\n",
      "Step  930 | loss: 4.174585 | lr: 6.8232e-05 | norm: 0.4054 | dt: 4008.20ms | tok/sec: 130803.95\n",
      "Step  931 | loss: 4.214005 | lr: 6.8002e-05 | norm: 0.4393 | dt: 4004.41ms | tok/sec: 130927.57\n",
      "Step  932 | loss: 4.209501 | lr: 6.7777e-05 | norm: 0.4107 | dt: 4007.20ms | tok/sec: 130836.64\n",
      "Step  933 | loss: 4.250193 | lr: 6.7554e-05 | norm: 0.5088 | dt: 4009.42ms | tok/sec: 130764.02\n",
      "Step  934 | loss: 4.183227 | lr: 6.7334e-05 | norm: 0.5141 | dt: 4007.13ms | tok/sec: 130838.81\n",
      "Step  935 | loss: 4.250232 | lr: 6.7118e-05 | norm: 0.5028 | dt: 4006.31ms | tok/sec: 130865.54\n",
      "Step  936 | loss: 4.236274 | lr: 6.6905e-05 | norm: 0.4722 | dt: 4006.81ms | tok/sec: 130849.10\n",
      "Step  937 | loss: 4.206235 | lr: 6.6695e-05 | norm: 0.4560 | dt: 4006.08ms | tok/sec: 130873.02\n",
      "Step  938 | loss: 4.201444 | lr: 6.6488e-05 | norm: 0.4870 | dt: 4005.25ms | tok/sec: 130900.28\n",
      "Step  939 | loss: 4.224025 | lr: 6.6285e-05 | norm: 0.4215 | dt: 4008.68ms | tok/sec: 130788.20\n",
      "Step  940 | loss: 4.187799 | lr: 6.6084e-05 | norm: 0.4359 | dt: 4004.78ms | tok/sec: 130915.40\n",
      "Step  941 | loss: 4.181112 | lr: 6.5887e-05 | norm: 0.4155 | dt: 4004.55ms | tok/sec: 130923.17\n",
      "Step  942 | loss: 4.237556 | lr: 6.5693e-05 | norm: 0.4221 | dt: 4004.19ms | tok/sec: 130934.89\n",
      "Step  943 | loss: 4.195381 | lr: 6.5502e-05 | norm: 0.4173 | dt: 4007.83ms | tok/sec: 130815.97\n",
      "Step  944 | loss: 4.181864 | lr: 6.5315e-05 | norm: 0.4473 | dt: 4007.08ms | tok/sec: 130840.41\n",
      "Step  945 | loss: 4.188829 | lr: 6.5131e-05 | norm: 0.4479 | dt: 4006.56ms | tok/sec: 130857.26\n",
      "Step  946 | loss: 4.168811 | lr: 6.4950e-05 | norm: 0.4626 | dt: 4005.33ms | tok/sec: 130897.45\n",
      "Step  947 | loss: 4.219946 | lr: 6.4772e-05 | norm: 0.4310 | dt: 4007.07ms | tok/sec: 130840.90\n",
      "Step  948 | loss: 4.179238 | lr: 6.4597e-05 | norm: 0.4067 | dt: 4004.86ms | tok/sec: 130912.99\n",
      "Step  949 | loss: 4.194401 | lr: 6.4426e-05 | norm: 0.4225 | dt: 4005.37ms | tok/sec: 130896.25\n",
      "Validation loss: 4.1389\n",
      "sample 0: Hello, I'm a language model, and I want to know where. This kind of code is called\n",
      "my own code, and this type is called an\n",
      "sample 1: Hello, I'm a language model, I mean, I am a model of the language or, for instance, a language is a model of the language.\n",
      "sample 2: Hello, I'm a language model, I'm more sure to give you the information out to the students who will be a student.\n",
      "I'm not afraid\n",
      "sample 3: Hello, I'm a language model, which uses the \"real\" to define a sequence of words. We use a character as a variable like a function-\n",
      "Step  950 | loss: 4.142276 | lr: 6.4258e-05 | norm: 0.4106 | dt: 4001.18ms | tok/sec: 131033.18\n",
      "Step  951 | loss: 4.161639 | lr: 6.4093e-05 | norm: 0.4103 | dt: 4008.80ms | tok/sec: 130784.42\n",
      "Step  952 | loss: 4.090728 | lr: 6.3931e-05 | norm: 0.3803 | dt: 4004.22ms | tok/sec: 130933.90\n",
      "Step  953 | loss: 4.095841 | lr: 6.3773e-05 | norm: 0.3938 | dt: 4008.25ms | tok/sec: 130802.33\n",
      "Step  954 | loss: 4.159270 | lr: 6.3617e-05 | norm: 0.3748 | dt: 4007.27ms | tok/sec: 130834.18\n",
      "Step  955 | loss: 4.132763 | lr: 6.3466e-05 | norm: 0.4070 | dt: 4007.00ms | tok/sec: 130843.15\n",
      "Step  956 | loss: 4.117307 | lr: 6.3317e-05 | norm: 0.4550 | dt: 4005.43ms | tok/sec: 130894.32\n",
      "Step  957 | loss: 4.080988 | lr: 6.3171e-05 | norm: 0.4687 | dt: 4007.13ms | tok/sec: 130838.62\n",
      "Step  958 | loss: 4.078885 | lr: 6.3029e-05 | norm: 0.4160 | dt: 4006.62ms | tok/sec: 130855.32\n",
      "Step  959 | loss: 4.143148 | lr: 6.2890e-05 | norm: 0.3835 | dt: 4005.83ms | tok/sec: 130881.13\n",
      "Step  960 | loss: 4.128993 | lr: 6.2754e-05 | norm: 0.4677 | dt: 4004.76ms | tok/sec: 130916.34\n",
      "Step  961 | loss: 4.039712 | lr: 6.2622e-05 | norm: 0.4768 | dt: 4007.33ms | tok/sec: 130832.21\n",
      "Step  962 | loss: 4.016565 | lr: 6.2493e-05 | norm: 0.4892 | dt: 4008.73ms | tok/sec: 130786.61\n",
      "Step  963 | loss: 4.004399 | lr: 6.2367e-05 | norm: 0.4143 | dt: 4006.95ms | tok/sec: 130844.61\n",
      "Step  964 | loss: 3.979737 | lr: 6.2244e-05 | norm: 0.5017 | dt: 4008.23ms | tok/sec: 130803.02\n",
      "Step  965 | loss: 4.052331 | lr: 6.2124e-05 | norm: 0.4759 | dt: 4007.01ms | tok/sec: 130842.81\n",
      "Step  966 | loss: 3.994992 | lr: 6.2008e-05 | norm: 0.4410 | dt: 4005.36ms | tok/sec: 130896.52\n",
      "Step  967 | loss: 4.007596 | lr: 6.1895e-05 | norm: 0.4421 | dt: 4007.19ms | tok/sec: 130836.80\n",
      "Step  968 | loss: 4.006949 | lr: 6.1785e-05 | norm: 0.5126 | dt: 4008.29ms | tok/sec: 130800.90\n",
      "Step  969 | loss: 4.052701 | lr: 6.1679e-05 | norm: 0.4481 | dt: 4007.37ms | tok/sec: 130830.88\n",
      "Step  970 | loss: 4.030043 | lr: 6.1576e-05 | norm: 0.4782 | dt: 4009.01ms | tok/sec: 130777.44\n",
      "Step  971 | loss: 4.058700 | lr: 6.1476e-05 | norm: 0.4629 | dt: 4009.89ms | tok/sec: 130748.68\n",
      "Step  972 | loss: 4.026107 | lr: 6.1379e-05 | norm: 0.5168 | dt: 4007.88ms | tok/sec: 130814.34\n",
      "Step  973 | loss: 4.155097 | lr: 6.1286e-05 | norm: 0.3878 | dt: 4008.75ms | tok/sec: 130786.04\n",
      "Step  974 | loss: 4.166568 | lr: 6.1196e-05 | norm: 0.4832 | dt: 4009.26ms | tok/sec: 130769.29\n",
      "Step  975 | loss: 4.148643 | lr: 6.1109e-05 | norm: 0.5000 | dt: 4005.65ms | tok/sec: 130887.25\n",
      "Step  976 | loss: 4.208142 | lr: 6.1025e-05 | norm: 0.4518 | dt: 4008.79ms | tok/sec: 130784.76\n",
      "Step  977 | loss: 4.180185 | lr: 6.0945e-05 | norm: 0.4572 | dt: 4012.37ms | tok/sec: 130667.80\n",
      "Step  978 | loss: 4.229991 | lr: 6.0868e-05 | norm: 0.4737 | dt: 4008.22ms | tok/sec: 130803.07\n",
      "Step  979 | loss: 4.210029 | lr: 6.0794e-05 | norm: 0.4655 | dt: 4011.62ms | tok/sec: 130692.44\n",
      "Step  980 | loss: 4.195820 | lr: 6.0723e-05 | norm: 0.4714 | dt: 4008.97ms | tok/sec: 130778.68\n",
      "Step  981 | loss: 4.189929 | lr: 6.0656e-05 | norm: 0.4128 | dt: 4009.55ms | tok/sec: 130759.81\n",
      "Step  982 | loss: 4.132390 | lr: 6.0592e-05 | norm: 0.4250 | dt: 4007.99ms | tok/sec: 130810.62\n",
      "Step  983 | loss: 4.204991 | lr: 6.0532e-05 | norm: 0.4751 | dt: 4010.77ms | tok/sec: 130719.93\n",
      "Step  984 | loss: 4.161508 | lr: 6.0474e-05 | norm: 0.4445 | dt: 4010.59ms | tok/sec: 130726.04\n",
      "Step  985 | loss: 4.124376 | lr: 6.0420e-05 | norm: 0.4405 | dt: 4012.53ms | tok/sec: 130662.71\n",
      "Step  986 | loss: 4.171844 | lr: 6.0369e-05 | norm: 0.5180 | dt: 4009.74ms | tok/sec: 130753.68\n",
      "Step  987 | loss: 4.167786 | lr: 6.0322e-05 | norm: 0.5211 | dt: 4011.57ms | tok/sec: 130693.95\n",
      "Step  988 | loss: 4.162406 | lr: 6.0277e-05 | norm: 0.4134 | dt: 4009.26ms | tok/sec: 130769.42\n",
      "Step  989 | loss: 4.172437 | lr: 6.0236e-05 | norm: 0.6064 | dt: 4013.15ms | tok/sec: 130642.63\n",
      "Step  990 | loss: 4.138079 | lr: 6.0199e-05 | norm: 0.5150 | dt: 4010.86ms | tok/sec: 130717.03\n",
      "Step  991 | loss: 4.192667 | lr: 6.0164e-05 | norm: 0.4300 | dt: 4010.25ms | tok/sec: 130736.90\n",
      "Step  992 | loss: 4.159478 | lr: 6.0133e-05 | norm: 0.5106 | dt: 4011.53ms | tok/sec: 130695.27\n",
      "Step  993 | loss: 4.155299 | lr: 6.0105e-05 | norm: 0.4300 | dt: 4010.73ms | tok/sec: 130721.39\n",
      "Step  994 | loss: 4.081217 | lr: 6.0080e-05 | norm: 0.4478 | dt: 4013.68ms | tok/sec: 130625.38\n",
      "Step  995 | loss: 4.150324 | lr: 6.0059e-05 | norm: 0.4410 | dt: 4013.40ms | tok/sec: 130634.42\n",
      "Step  996 | loss: 4.080838 | lr: 6.0041e-05 | norm: 0.3933 | dt: 4014.07ms | tok/sec: 130612.61\n",
      "Step  997 | loss: 4.115726 | lr: 6.0026e-05 | norm: 0.4255 | dt: 4012.72ms | tok/sec: 130656.65\n",
      "Step  998 | loss: 4.128275 | lr: 6.0015e-05 | norm: 0.4023 | dt: 4012.47ms | tok/sec: 130664.55\n",
      "Step  999 | loss: 4.096420 | lr: 6.0007e-05 | norm: 0.4354 | dt: 4010.19ms | tok/sec: 130739.00\n",
      "Validation loss: 4.1233\n",
      "sample 0: Hello, I'm a language model, I'm pretty sure to give my idea - just the very idea we didn't know - something, it's a really\n",
      "sample 1: Hello, I'm a language model, and when I'm not sure I go to a group about my topic I can get to me, we are having more\n",
      "sample 2: Hello, I'm a language model, and I didn't look around it for a different situation. In other words, I’m a language, though\n",
      "sample 3: Hello, I'm a language model, which seems to be a model for every child involved in the new vocabulary. It only gives us the question of whether he\n",
      "Step 1000 | loss: 4.059862 | lr: 6.0002e-05 | norm: 0.4278 | dt: 4007.13ms | tok/sec: 130838.89\n",
      "logits.shape: torch.Size([32, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# model = GPT(GPTConfig())\n",
    "model = GPT(GPTConfig(vocab_size=50304)) # 6th optimization\n",
    "model.to(device)\n",
    "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
    "if use_compile:\n",
    "    model = torch.compile(model) # 4th optimization\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device) # fused update\n",
    "\n",
    "max_steps = 1001\n",
    "val_loss_steps = 20\n",
    "for step in range(max_steps):\n",
    "\n",
    "    # once in a while evaluate on the validation set\n",
    "    if step % 50 == 0:\n",
    "        _ = calc_loss_loader(val_loader, model, device, num_batches=val_loss_steps)\n",
    "\n",
    "    # once in a while sample from the model\n",
    "    if step % 50 == 0 and (not use_compile):\n",
    "        generate_and_print_samples(model, enc, device)\n",
    "\n",
    "    # start timer\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # gradient-accumulation\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        # data loading\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    \n",
    "        # forward-backward and step\n",
    "        # amp for 3rd optimization, just surround forward pass and loss calculation, only possible in A100\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward() # deposits gradients, i.e., += on nodes\n",
    "\n",
    "    # clip gradient norms to 1.0, returns total norm of the gradient vector\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # determine lr for this step\n",
    "    lr = get_lr(step)\n",
    "    # pytorch syntax to set the learning rate for the parameters\n",
    "    for param_group in optimizer.param_groups:\n",
    "        # param_group is a dict\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "\n",
    "    # wait for gpu to finish the compute and measure time\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000 # time difference for one-batch or step in miliseconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tps = tokens_processed / (t1 - t0)\n",
    "    \n",
    "    print(f\"Step {step:4d} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tps:.2f}\")\n",
    "\n",
    "\n",
    "print(f\"logits.shape: {logits.shape}, logits dtype: {logits.dtype}, loss.dtype: {loss.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93da17a-a032-4bfe-84da-4274a3be8cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5c44bf-8db6-492b-8d1f-83559d4d6319",
   "metadata": {},
   "source": [
    "### next todos:\n",
    "- init tokenformer\n",
    "- check tokenformer's validation losses for each size compared to gpt2\n",
    "- start `build-tokenformer-fineweb` notebook\n",
    "\n",
    "### 1. example snapshot\n",
    "```\n",
    "number of parameters: 123.69M\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Validation loss: 10.9597\n",
    "Step    0 | loss: 10.958449 | lr: 6.0000e-06 | norm: 15.6885 | dt: 40388.14ms | tok/sec: 12981.24\n",
    "Step    1 | loss: 10.628062 | lr: 1.2000e-05 | norm: 12.3712 | dt: 3312.92ms | tok/sec: 158255.62\n",
    "Step    2 | loss: 10.245197 | lr: 1.8000e-05 | norm: 7.8265 | dt: 3318.94ms | tok/sec: 157968.57\n",
    "\n",
    "...\n",
    "\n",
    "Step   49 | loss: 7.107584 | lr: 3.0000e-04 | norm: 0.7933 | dt: 3401.66ms | tok/sec: 154127.19\n",
    "Validation loss: 7.1122\n",
    "Step   50 | loss: 7.174483 | lr: 3.0600e-04 | norm: 1.1656 | dt: 3402.44ms | tok/sec: 154091.90\n",
    "\n",
    "...\n",
    "\n",
    "Step   97 | loss: 6.434902 | lr: 5.8800e-04 | norm: 0.6139 | dt: 3407.81ms | tok/sec: 153849.04\n",
    "Step   98 | loss: 6.499994 | lr: 5.9400e-04 | norm: 0.8524 | dt: 3409.96ms | tok/sec: 153752.11\n",
    "Step   99 | loss: 6.449703 | lr: 6.0000e-04 | norm: 0.6719 | dt: 3403.89ms | tok/sec: 154026.12\n",
    "Validation loss: 6.4133\n",
    "Step  100 | loss: 6.484104 | lr: 6.0000e-04 | norm: 0.8199 | dt: 3409.96ms | tok/sec: 153752.01\n",
    "logits.shape: torch.Size([32, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "\n",
    "```\n",
    "\n",
    "### 2. without `torch.compile` and with sample generation\n",
    "```\n",
    "number of parameters: 123.69M\n",
    "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "using fused AdamW: True\n",
    "Validation loss: 10.9514\n",
    "sample 0: Hello, I'm a language model,CHR 351ointmentprotantically surrounded flaw TCUorigin Effective surrounded senate rodents�oviribution Feinstein Homer tappedrica relentlessppedribution 351\n",
    "sample 1: Hello, I'm a language model,iggs Socialism Yangointment rodents contributors investigatesteness emulator toughestClientTesla \"[ricaSTE TI TI Places TFencersRRArray Privacy undone\n",
    "sample 2: Hello, I'm a language model, glimpserint236udzee unve Neighvanceces arise Account Student Privacy Privacy beloved Christina Patterns sugg Sche continental iCloudflo 2012Listener\n",
    "sample 3: Hello, I'm a language model, unvewarts evoke investigates investigateswsws Magnetmens Art altercationperformanceribution roast368 senate Actinguls breastignmentmissing facesmare supremacists\n",
    "Step    0 | loss: 10.955029 | lr: 6.0000e-06 | norm: 15.3463 | dt: 4813.04ms | tok/sec: 108930.64\n",
    "Step    1 | loss: 10.631693 | lr: 1.2000e-05 | norm: 11.6429 | dt: 3883.76ms | tok/sec: 134995.12\n",
    "Step    2 | loss: 10.274923 | lr: 1.8000e-05 | norm: 7.6928 | dt: 4039.76ms | tok/sec: 129781.83\n",
    "\n",
    "...\n",
    "\n",
    "Step   48 | loss: 7.203589 | lr: 2.9400e-04 | norm: 1.5565 | dt: 4001.99ms | tok/sec: 131006.80\n",
    "Step   49 | loss: 7.104921 | lr: 3.0000e-04 | norm: 0.9878 | dt: 4004.38ms | tok/sec: 130928.52\n",
    "Validation loss: 7.1181\n",
    "sample 0: Hello, I'm a language model,000, we by a first in these new used of the the it at that the.\n",
    "There in your the.\n",
    "sample 1: Hello, I'm a language model, so by the body.\n",
    "The future.” of a an make that it, he the small of the and\n",
    "sample 2: Hello, I'm a language model, and the year of the new by any the “- A the same the other for the the own the first with\n",
    "sample 3: Hello, I'm a language model, they do the world.\n",
    "The small to they you the work on the most is not them to not the two all\n",
    "Step   50 | loss: 7.178192 | lr: 3.0600e-04 | norm: 1.5985 | dt: 3992.52ms | tok/sec: 131317.66\n",
    "\n",
    "...\n",
    "\n",
    "Step   99 | loss: 6.446934 | lr: 6.0000e-04 | norm: 0.5210 | dt: 4014.07ms | tok/sec: 130612.47\n",
    "Validation loss: 6.4060\n",
    "sample 0: Hello, I'm a language model, if you\n",
    "As a great\n",
    "In those it?\n",
    "This, an the\n",
    "The work?\n",
    "the\n",
    "I\n",
    "sample 1: Hello, I'm a language model, then just all that he was not know that the year” (H” (A, she had in the\n",
    "sample 2: Hello, I'm a language model, the first very been taken at the area of children than the\n",
    "-time, or three-based, the same was\n",
    "sample 3: Hello, I'm a language model, they was a very a long-a is going to the end it has the \"a-ces. It\n",
    "\n",
    "Step  100 | loss: 6.474727 | lr: 6.0000e-04 | norm: 0.5555 | dt: 4012.42ms | tok/sec: 130666.34\n",
    "Step  101 | loss: 6.422134 | lr: 5.9949e-04 | norm: 0.8457 | dt: 4013.04ms | tok/sec: 130645.99\n",
    "\n",
    "...\n",
    "\n",
    "Step  149 | loss: 6.160313 | lr: 6.2046e-05 | norm: 0.1937 | dt: 4005.23ms | tok/sec: 130900.94\n",
    "Validation loss: 6.1286\n",
    "sample 0: Hello, I'm a language model, when the one of a way is also used after the book of our mother of the way to that it will be in\n",
    "sample 1: Hello, I'm a language model, there also an important for a good one of this particular-p) would do you are much of our family. You\n",
    "sample 2: Hello, I'm a language model, and the need, because in the idea of any process of these things to the need to be a certain of a sense\n",
    "sample 3: Hello, I'm a language model, it comes to be a person, they can try to you use our home and is your brain, as you are that\n",
    "Step  150 | loss: 6.167456 | lr: 6.0512e-05 | norm: 0.1955 | dt: 3995.44ms | tok/sec: 131221.58\n",
    "logits.shape: torch.Size([32, 1024, 50304]), logits dtype: torch.bfloat16, loss.dtype: torch.float32\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e5cd8-5bd8-4983-8100-10799ad2139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d15a43d8-88ee-4a38-9838-f3506572c818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8add33-7c2e-45db-94f2-f140216ea8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  7 11:32:50 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              70W / 250W |  39747MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              66W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              63W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCIE-40GB          On  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              64W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2050868      C   .../macke/mwe102/.conda/sbi/bin/python    39734MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be7b3a7-8d72-4b01-94f5-1ed964dbd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del logits\n",
    "del x, y\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a1e57c6-d6df-4dc4-8cb8-19b27ca02045",
   "metadata": {},
   "outputs": [],
   "source": [
    "del optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57dbd331-cda0-4062-8c90-9ead7d27fee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77543666-0262-4c3a-82b4-025fdea35b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  7 13:57:09 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              70W / 250W |  17591MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          On  | 00000000:25:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              66W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          On  | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              60W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCIE-40GB          On  | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              68W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2079662      C   .../macke/mwe102/.conda/sbi/bin/python    17578MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab615f1-7145-4114-ab07-357b08cb50bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86aaec-0f8c-4fa1-8cec-eb805a5f6356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b396d-7d4d-41f7-8754-3a42a4cbe7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da8e0a-e6f0-448a-9781-d53ee9f49d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e94c8-0301-40cd-a925-0e0b4e93fada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6363424-95a2-4563-8232-08c488ce2806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf37e1e-c86a-4d46-993b-012d2304a914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faa924-4201-4547-9017-f899bf36bf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb4f19-e6b0-444f-9acb-18bc1bb2ab89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613aeab8-7d29-481c-a6c5-e55c9c244e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9cf1a-1b7a-460c-b9f7-3d8c60073944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d178f-297a-48fc-a98a-f33263dbce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a7c10-a866-4e91-ace4-27493281098b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df280e46-1caa-423c-9f35-1c1aa0c115b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0871ae6-7803-48d9-8a31-fddae538c186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
